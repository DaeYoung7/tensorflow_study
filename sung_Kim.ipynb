{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7\n",
    "x_data=[1,2,3]\n",
    "y_data=[1,2,3]\n",
    "\n",
    "W=tf.Variable(tf.random_normal([1]),name='weight')\n",
    "X=tf.placeholder(tf.float32)\n",
    "Y=tf.placeholder(tf.float32)\n",
    "\n",
    "hypothesis=W*X\n",
    "\n",
    "cost=tf.reduce_mean(tf.square(hypothesis-Y))\n",
    "\n",
    "learning_rate=0.1\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train=optimizer.minimize(cost)\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(21):\n",
    "    sess.run(train, feed_dict={X:x_data,Y:y_data})\n",
    "    print(step,sess.run(cost,feed_dict={X:x_data,Y:y_data}),sess.run(W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9\n",
    "x_data=[[72.,80.,75.],[93.,88.,93.],[89.,91.,90.],[96.,98.,100.],[73.,66.,70.]]\n",
    "y_data=[[152.],[185.],[180.],[196.],[142.]]\n",
    "\n",
    "W=tf.Variable(tf.random_normal([3,1]),name='weight')\n",
    "b=tf.Variable(tf.random_normal([1]),name='bias')\n",
    "X=tf.placeholder(tf.float32,shape=[None,3])\n",
    "Y=tf.placeholder(tf.float32,shape=[None,1])\n",
    "#지금은 5개이지만 데이터 추가가 용이하도록 None으로 설정\n",
    "hypothesis=tf.matmul(X,W)+b\n",
    "\n",
    "cost=tf.reduce_mean(tf.square(hypothesis-Y))\n",
    "\n",
    "learning_rate=1e-5\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train=optimizer.minimize(cost)\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(2001):\n",
    "    cost_val,hy_val,_=sess.run([cost,hypothesis,train], feed_dict={X:x_data,Y:y_data})\n",
    "    if step%10==0:\n",
    "        print(step,cost_val, hy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#13 당뇨병, 로지스틱 분류 이용(시그모이드, 바이너리)\n",
    "xy=np.loadtxt(\"data-03-diabetes.csv.txt\",delimiter=',',dtype=np.float32)\n",
    "x_data=xy[:,0:-1]\n",
    "y_data=xy[:,[-1]]\n",
    "\n",
    "X=tf.placeholder(tf.float32,shape=[None,8])\n",
    "Y=tf.placeholder(tf.float32,shape=[None,1])\n",
    "W=tf.Variable(tf.random_normal([8,1]))\n",
    "B=tf.Variable(tf.random_normal([1]))\n",
    "\n",
    "hypothesis=tf.sigmoid(tf.matmul(X,W)+B)\n",
    "cost=-tf.reduce_mean(Y*tf.log(hypothesis)+(1-Y)*tf.log(1-hypothesis))\n",
    "\n",
    "train=tf.train.GradientDescentOptimizer(0.1).minimize(cost)\n",
    "\n",
    "predict=tf.cast(hypothesis>0.5,dtype=tf.float32)\n",
    "accuracy=tf.reduce_mean(tf.cast(tf.equal(predict,Y),dtype=tf.float32))\n",
    "\n",
    "with tf.Session()as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    feed={X:x_data,Y:y_data}\n",
    "    for step in range(10001):\n",
    "        sess.run(train,feed_dict=feed)\n",
    "        if step%200==0:\n",
    "            print(step, sess.run(cost,feed_dict=feed))\n",
    "    h,c,a=sess.run([hypothesis,cost,accuracy],feed_dict=feed)\n",
    "    print(\"\\nhypothesis=\",h,\"\\ncost=\",c,\"\\naccuracy=\",a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#17 softmax 이용\n",
    "xy=np.loadtxt(\"data-04-zoo.csv.txt\",delimiter=',',dtype=np.float32)\n",
    "x_data=xy[:,0:-1]\n",
    "y_data=xy[:,[-1]]\n",
    "\n",
    "nb_classes=7\n",
    "\n",
    "X=tf.placeholder(tf.float32,shape=[None,16])\n",
    "Y=tf.placeholder(tf.int32,shape=[None,1])\n",
    "W=tf.Variable(tf.random_normal([16,nb_classes]))\n",
    "B=tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "Y_one_hot=tf.one_hot(Y,nb_classes)\n",
    "Y_one_hot=tf.reshape(Y_one_hot,[-1,nb_classes])\n",
    "\n",
    "logits=tf.matmul(X,W)+B\n",
    "hypothesis=tf.nn.softmax(logits)\n",
    "\n",
    "cost_i=tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=Y_one_hot)\n",
    "cost=tf.reduce_mean(cost_i)\n",
    "train=tf.train.GradientDescentOptimizer(0.1).minimize(cost)\n",
    "\n",
    "prediction=tf.argmax(hypothesis,1)\n",
    "correct_prediction=tf.equal(prediction,tf.argmax(Y_one_hot,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    feed={X:x_data,Y:y_data}\n",
    "    for step in range(2001):\n",
    "        sess.run(train,feed_dict=feed)\n",
    "        if step%200==0:\n",
    "            loss,acc=sess.run([cost,accuracy],feed_dict=feed)\n",
    "            print('step={:5}\\tloss={:.3f}\\taccuracy={:.2%}'.format(step,loss,acc))\n",
    "    pred=sess.run(prediction,feed_dict=feed)\n",
    "    for p,y in zip(pred,y_data.flatten()):\n",
    "        print(\"[{}] prediction={} Y={}\".format(p==int(y),p,int(y)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#20 normalization \n",
    "#[1,2,10000,3,4]->[0.001,0.002,0.5,0.003,0.004]\n",
    "#input 중 outlier를 다른 데이터와 균일하게 해야함\n",
    "#xy=minmaxscaler(xy) 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import input_data\n",
    "\n",
    "mnist=input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n",
    "\n",
    "nb_classes=10\n",
    "\n",
    "X=tf.placeholder(tf.float32,[None,784])\n",
    "Y=tf.placeholder(tf.float32,[None,10])\n",
    "W=tf.Variable(tf.random_normal([784,10]))\n",
    "B=tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "\n",
    "logits=tf.matmul(X,W)+B\n",
    "hypothesis=tf.nn.softmax(logits)\n",
    "\n",
    "cost_i=tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=Y)\n",
    "cost=tf.reduce_mean(cost_i)\n",
    "\n",
    "train=tf.train.GradientDescentOptimizer(0.1).minimize(cost)\n",
    "\n",
    "is_correct=tf.equal(tf.argmax(hypothesis,1),tf.argmax(Y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(is_correct,tf.float32))\n",
    "\n",
    "training_epochs=15\n",
    "batch_size=100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost=0\n",
    "        total_batch=int(mnist.train.num_examples/batch_size) \n",
    "        for i in range(total_batch):\n",
    "            batch_xs,batch_ys=mnist.train.next_batch(batch_size)\n",
    "            c,_=sess.run([cost,train],feed_dict={X:batch_xs,Y:batch_ys})\n",
    "            avg_cost+=c/total_batch\n",
    "        print('Epoch:','%04d'%(epoch+1),'cost=','{:.9f}'.format(avg_cost))\n",
    "    print(\"Accuracy: \",accuracy.eval(session=sess,feed_dict={X:mnist.test.images,Y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#24 tf 함수실습\n",
    "sess=tf.Session()\n",
    "x=[1,2,3]\n",
    "y=[4,5,6]\n",
    "z=[7,8,9]\n",
    "tf.stack([x,y,z],axis=-1).eval(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#28 xor해결하기\n",
    "#2개의 깊이, 2개의 너비\n",
    "x_data=np.array([[0,0],[0,1],[1,0],[1,1]],dtype=np.float32)\n",
    "y_data=np.array([[0],[1],[1],[0]],dtype=np.float32)\n",
    "\n",
    "X=tf.placeholder(tf.float32,[4,2])\n",
    "Y=tf.placeholder(tf.float32,[4,1])\n",
    "\n",
    "#2,2에서 두번째 2가 해당 층의 너비\n",
    "#layer의 개수가 모형의 깊이\n",
    "W1=tf.Variable(tf.random_normal([2,2]))\n",
    "B1=tf.Variable(tf.random_normal([2]))\n",
    "layer1=tf.sigmoid(tf.matmul(X,W1)+B1)\n",
    "\n",
    "W2=tf.Variable(tf.random_normal([2,1]))\n",
    "B2=tf.Variable(tf.random_normal([1]))\n",
    "hypothesis=tf.sigmoid(tf.matmul(layer1,W2)+B2)\n",
    "\n",
    "cost=-tf.reduce_mean(Y*tf.log(hypothesis)+(1-Y)*tf.log(1-hypothesis))\n",
    "train=tf.train.GradientDescentOptimizer(0.1).minimize(cost)\n",
    "\n",
    "predicted=tf.cast(hypothesis>0.5,dtype=tf.float32)\n",
    "accuracy=tf.reduce_mean(tf.cast(tf.equal(predicted,Y),tf.float32))\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        sess.run(train,feed_dict={X:x_data,Y:y_data})\n",
    "        if step%1000==0:\n",
    "            print(step,sess.run(cost,feed_dict={X:x_data,Y:y_data}),sess.run([W1,W2])) \n",
    "    h,c,a=sess.run([hypothesis,cost,accuracy],feed_dict={X:x_data,Y:y_data})\n",
    "    print('\\nhypothesis=',h,'\\ncorrect=',c,'\\naccuracy=',a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#34 정확도 높이기 mnist\n",
    "#경사하강->89.53 / 아담->91.88 / 초기값 문제 해결&l.r 0.01로 변경\n",
    "#깊이 너비 확장(3,256)->96.33 / 깊이 너비 확장(5,512)->97.25\n",
    "#더 나아졌지만 강의대로 dropout 사용->96.03?\n",
    "import input_data\n",
    "\n",
    "mnist=input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n",
    "\n",
    "nb_classes=10\n",
    "\n",
    "X=tf.placeholder(tf.float32,[None,784])\n",
    "Y=tf.placeholder(tf.float32,[None,10])\n",
    "keep_prob=tf.placeholder(tf.float32)\n",
    "\n",
    "W1=tf.get_variable(\"W1\",shape=[784,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "W2=tf.get_variable(\"W2\",shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "W3=tf.get_variable(\"W3\",shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "W4=tf.get_variable(\"W4\",shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "W5=tf.get_variable(\"W5\",shape=[512,10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "B1=tf.Variable(tf.random_normal([512]))\n",
    "B2=tf.Variable(tf.random_normal([512]))\n",
    "B3=tf.Variable(tf.random_normal([512]))\n",
    "B4=tf.Variable(tf.random_normal([512]))\n",
    "B5=tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "L1=tf.nn.relu(tf.matmul(X,W1)+B1)\n",
    "L1=tf.nn.dropout(L1,keep_prob=keep_prob)\n",
    "L2=tf.nn.relu(tf.matmul(L1,W2)+B2)\n",
    "L2=tf.nn.dropout(L2,keep_prob=keep_prob)\n",
    "L3=tf.nn.relu(tf.matmul(L2,W3)+B3)\n",
    "L4=tf.nn.relu(tf.matmul(L3,W4)+B4)\n",
    "\n",
    "logits=tf.matmul(L4,W5)+B5\n",
    "hypothesis=tf.nn.softmax(logits)\n",
    "\n",
    "cost_i=tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=Y)\n",
    "cost=tf.reduce_mean(cost_i)\n",
    "\n",
    "train=tf.train.AdamOptimizer(0.01).minimize(cost)\n",
    "\n",
    "is_correct=tf.equal(tf.argmax(hypothesis,1),tf.argmax(Y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(is_correct,tf.float32))\n",
    "\n",
    "training_epochs=15\n",
    "batch_size=100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost=0\n",
    "        total_batch=int(mnist.train.num_examples/batch_size) \n",
    "        for i in range(total_batch):\n",
    "            batch_xs,batch_ys=mnist.train.next_batch(batch_size)\n",
    "            c,_=sess.run([cost,train],feed_dict={X:batch_xs,Y:batch_ys,keep_prob:0.7})\n",
    "            avg_cost+=c/total_batch\n",
    "        print('Epoch:','%04d'%(epoch+1),'cost=','{:.9f}'.format(avg_cost))\n",
    "    print(\"Accuracy: \",accuracy.eval(session=sess,feed_dict={X:mnist.test.images,Y:mnist.test.labels,keep_prob:1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#34 정확도 높이기 iris 혼자실습\n",
    "#iris-setosa 1 / iris-versicolor 2 / iris-virginica 3\n",
    "#정확도::경사하강->97.3 / 아담->98.6 / 깊이와 너비 증가->1.0\n",
    "#but this program does'nt seperate test and train data\n",
    "xy=np.loadtxt(\"iris.csv.txt\",delimiter=',',dtype=np.string_)\n",
    "for i in range(len(xy)):\n",
    "    for j in range(len(xy[i])):\n",
    "        if xy[i][j]==b'Iris-setosa':\n",
    "            xy[i][j]='0'\n",
    "        elif xy[i][j]==b'Iris-versicolor':\n",
    "            xy[i][j]='1'\n",
    "        elif xy[i][j]==b'Iris-virginica':\n",
    "            xy[i][j]='2'\n",
    "xy1=xy.astype(np.float32)\n",
    "x_data=xy1[:,0:-1]\n",
    "y_data=xy1[:,[-1]]\n",
    "X=tf.placeholder(tf.float32,shape=[None,4])\n",
    "Y=tf.placeholder(tf.int32,shape=[None,1])\n",
    "W1=tf.Variable(tf.random_normal([4,100]))\n",
    "W2=tf.Variable(tf.random_normal([100,100]))\n",
    "W3=tf.Variable(tf.random_normal([100,3]))\n",
    "B1=tf.Variable(tf.random_normal([100]))\n",
    "B2=tf.Variable(tf.random_normal([100]))\n",
    "B3=tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "Y_one_hot=tf.one_hot(Y,3)\n",
    "Y_one_hot=tf.reshape(Y_one_hot,[-1,3])\n",
    "\n",
    "layer1=tf.nn.relu(tf.matmul(X,W1)+B1)\n",
    "layer2=tf.nn.relu(tf.matmul(layer1,W2)+B2)\n",
    "logits=tf.matmul(layer2,W3)+B3\n",
    "hypothesis=tf.nn.softmax(tf.matmul(layer2,W3)+B3)\n",
    "cost_i=tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=Y_one_hot)\n",
    "cost=tf.reduce_mean(cost_i)\n",
    "train=tf.train.AdamOptimizer(0.1).minimize(cost)\n",
    "\n",
    "prediction=tf.argmax(hypothesis,1)\n",
    "correct_prediction=tf.equal(prediction,tf.argmax(Y_one_hot,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    feed={X:x_data,Y:y_data}\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(5000):\n",
    "        sess.run(train,feed_dict=feed)\n",
    "        if step%500==0:\n",
    "            loss,acc=sess.run([cost,accuracy],feed_dict=feed)\n",
    "            print('step=',step,'\\tloss=',loss,'\\taccuracy=',acc)\n",
    "    pred=sess.run(prediction,feed_dict=feed)\n",
    "    for p,y in zip(pred,y_data.flatten()):\n",
    "        print('[{}] Prediction: {} True Y: {}'.format(p==int(y),p,int(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#39 mnist cnn적용 97.98\n",
    "import input_data\n",
    "\n",
    "mnist=input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n",
    "\n",
    "X=tf.placeholder(tf.float32,[None,784])\n",
    "X_img=tf.reshape(X,[-1,28,28,1])\n",
    "Y=tf.placeholder(tf.float32,[None,10])\n",
    "\n",
    "W1=tf.Variable(tf.random_normal([3,3,1,32]),tf.float32)\n",
    "L1=tf.nn.conv2d(X_img,W1,strides=[1,1,1,1],padding='SAME')\n",
    "L1=tf.nn.relu(L1)\n",
    "#maxpool strides=2 => 14,14,1\n",
    "L1=tf.nn.max_pool(L1,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "\n",
    "W2=tf.Variable(tf.random_normal([3,3,32,64]),tf.float32)\n",
    "L2=tf.nn.conv2d(L1,W2,strides=[1,1,1,1],padding='SAME')\n",
    "L2=tf.nn.relu(L2)\n",
    "L2=tf.nn.max_pool(L2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "L2=tf.reshape(L2,[-1,7*7*64])\n",
    "\n",
    "W3=tf.get_variable(\"W3\",shape=[7*7*64,10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "B=tf.Variable(tf.random_uniform([10]))\n",
    "\n",
    "logits=tf.matmul(L2,W3)+B\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=Y))\n",
    "optimizer=tf.train.AdamOptimizer(0.01).minimize(cost)\n",
    "\n",
    "training_epoch=15\n",
    "batch_size=100\n",
    "\n",
    "correct_prediction=tf.equal(tf.argmax(logits,1),tf.argmax(Y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(training_epoch):\n",
    "        avg_cost=0\n",
    "        total_batch=int(mnist.train.num_examples/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_xs,batch_ys=mnist.train.next_batch(batch_size)\n",
    "            c,_=sess.run([cost,optimizer],feed_dict={X:batch_xs,Y:batch_ys})\n",
    "            avg_cost+=c/total_batch\n",
    "        print(\"epoch: %04d\" %(epoch+1),\"cost: {:.9f}\".format(avg_cost))\n",
    "    print(\"Accuracy:\",sess.run(accuracy,feed_dict={X:mnist.test.images,Y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#40 mnist cnn 적용2 class로 정리\n",
    "#깊이 너비 증가(cnn깊이 2->3,행렬계산깊이1->2 너비625)->99.28%\n",
    "#앙상블 포함->\n",
    "class Model():\n",
    "    def __init__(self,sess,name):\n",
    "        self.sess=sess\n",
    "        self.name=name\n",
    "        self._build_net()\n",
    "    \n",
    "    def _build_net(self):\n",
    "        with tf.variable_scope(self.name):\n",
    "            self.X=tf.placeholder(tf.float32,[None,784])\n",
    "            X_img=tf.reshape(self.X,[-1,28,28,1])\n",
    "            self.Y=tf.placeholder(tf.float32,[None,10])\n",
    "            \n",
    "            self.keep_prob=tf.placeholder(tf.float32)\n",
    "\n",
    "            W1=tf.Variable(tf.random_normal([3,3,1,32],stddev=0.01))\n",
    "            L1=tf.nn.conv2d(X_img,W1,strides=[1,1,1,1],padding='SAME')\n",
    "            L1=tf.nn.relu(L1)\n",
    "            #maxpool strides=2 => 14,14,1\n",
    "            L1=tf.nn.max_pool(L1,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "            L1=tf.nn.dropout(L1,keep_prob=self.keep_prob)\n",
    "\n",
    "            W2=tf.Variable(tf.random_normal([3,3,32,64],stddev=0.01))\n",
    "            L2=tf.nn.conv2d(L1,W2,strides=[1,1,1,1],padding='SAME')\n",
    "            L2=tf.nn.relu(L2)\n",
    "            L2=tf.nn.max_pool(L2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "            L2=tf.nn.dropout(L2,keep_prob=self.keep_prob)\n",
    "            \n",
    "            W3=tf.Variable(tf.random_normal([3,3,64,128],stddev=0.01))\n",
    "            L3=tf.nn.conv2d(L2,W3,strides=[1,1,1,1],padding='SAME')\n",
    "            L3=tf.nn.relu(L3)\n",
    "            L3=tf.nn.max_pool(L3,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "            L3=tf.nn.dropout(L3,keep_prob=self.keep_prob)\n",
    "            L3=tf.reshape(L3,[-1,4*4*128])\n",
    "\n",
    "            W4=tf.get_variable(\"W4\",shape=[4*4*128,625],initializer=tf.contrib.layers.xavier_initializer())\n",
    "            B1=tf.Variable(tf.random_uniform([625]))\n",
    "            L4=tf.nn.relu(tf.matmul(L3,W4)+B1)\n",
    "            L4=tf.nn.dropout(L4,keep_prob=self.keep_prob)\n",
    "            \n",
    "            W5=tf.get_variable(\"W5\",shape=[625,10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "            B2=tf.Variable(tf.random_normal([10]))\n",
    "            \n",
    "            self.logits=tf.matmul(L4,W5)+B2\n",
    "            self.cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.logits,labels=self.Y))\n",
    "            self.optimizer=tf.train.AdamOptimizer(0.001).minimize(self.cost)\n",
    "\n",
    "            correct_prediction=tf.equal(tf.argmax(self.logits,1),tf.argmax(self.Y,1))\n",
    "            self.accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "            \n",
    "    def predict(self,x_test,keep_prob=1.0):\n",
    "        return self.sess.run(self.logits,feed_dict={self.X:x_test,self.keep_prob:keep_prob})\n",
    "    \n",
    "    def get_accuracy(self,x_test,y_test,keep_prob=1.0):\n",
    "        return self.sess.run(self.accuracy,feed_dict={self.X:x_test,self.Y:y_test,self.keep_prob:keep_prob})\n",
    "    def train(self,x_data,y_data,keep_prob=0.7):\n",
    "        return self.sess.run([self.cost,self.optimizer],feed_dict={self.X:x_data,self.Y:y_data,self.keep_prob:keep_prob})\n",
    "\n",
    "    \n",
    "import input_data\n",
    "\n",
    "mnist=input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n",
    "\n",
    "sess=tf.Session()\n",
    "m=[]\n",
    "for i in range(7):\n",
    "    m.append(Model(sess,\"model\"+str(i)))\n",
    "\n",
    "training_epoch=15\n",
    "batch_size=100\n",
    "prediction=np.zeros(len(mnist.test.labels)*10).reshape(len(mnist.test.labels),10)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(training_epoch):\n",
    "    avg_cost_list=np.zeros(len(m))\n",
    "    total_batch=int(mnist.train.num_examples/batch_size)\n",
    "    for i in range(total_batch):\n",
    "        batch_xs,batch_ys=mnist.train.next_batch(batch_size)\n",
    "        for m_idx,mm in enumerate(m):\n",
    "            c,_=mm.train(batch_xs,batch_ys)\n",
    "            avg_cost_list[m_idx]+=c/total_batch\n",
    "    print(\"epoch: %04d\" %(epoch+1),\"cost: \",avg_cost_list)\n",
    "for m_idx,mm in enumerate(m):\n",
    "    print(\"Accuracy:\",mm.get_accuracy(mnist.test.images,mnist.test.labels))\n",
    "    p=mm.predict(mnist.test.images)\n",
    "    prediction+=p\n",
    "ensemble_correct_prediction=tf.equal(tf.argmax(prediction,1),tf.argmax(mnist.test.labels,1))\n",
    "ensemble_accuracy=tf.reduce_mean(tf.cast(ensemble_correct_prediction,tf.float32))\n",
    "print('ensemble accuracy:',sess.run(ensemble_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cifar10 실습\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "train_x_list=[]\n",
    "train_y_list=[]\n",
    "for i in range(1,6):\n",
    "    a=unpickle(\"cifar-10-batches-py/data_batch_\"+str(i))\n",
    "    train_x_list.append(np.array(a[b'data']))\n",
    "    train_y_list.append(np.array(a[b'labels']).reshape(-1,1))\n",
    "test=unpickle(\"cifar-10-batches-py/test_batch\")\n",
    "test_x=np.array(test[b'data'])\n",
    "#test_x=np.array(train_x_list).flatten('C').reshape(-1,3072).shape\n",
    "test_y=np.array(test[b'labels']).reshape(-1,1)\n",
    "\n",
    "keep_prob=tf.placeholder(tf.float32)\n",
    "\n",
    "X=tf.placeholder(tf.float32,[None,3072])\n",
    "X_img=tf.reshape(X,[-1,32,32,3])\n",
    "Y=tf.placeholder(tf.int32,[None,1])\n",
    "Y_one_hot=tf.one_hot(Y,10)\n",
    "Y_one_hot=tf.reshape(Y_one_hot,[-1,10])\n",
    "W1=tf.Variable(tf.random_normal([3,3,3,32],stddev=0.01))\n",
    "L1=tf.nn.conv2d(X_img,W1,strides=[1,1,1,1],padding='SAME')\n",
    "L1=tf.nn.relu(L1)\n",
    "L1=tf.nn.max_pool(L1,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "L1=tf.nn.dropout(L1,keep_prob=keep_prob)\n",
    "L1=tf.reshape(L1,[-1,16*16*32])\n",
    "W2=tf.get_variable(\"W2\",shape=[16*16*32,10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "B=tf.Variable(tf.random_normal([1]))\n",
    "L2=tf.matmul(L1,W2)+B\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=L2,labels=Y_one_hot))\n",
    "optimizer=tf.train.AdamOptimizer(0.00001).minimize(cost)\n",
    "\n",
    "is_correct=tf.equal(tf.argmax(L1,1),tf.argmax(Y_one_hot,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(is_correct,tf.float32))\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(15):\n",
    "        avg_cost=0\n",
    "        for m,n in zip(train_x_list,train_y_list):\n",
    "            c,_=sess.run([cost,optimizer],feed_dict={X:m,Y:n,keep_prob:0.7})\n",
    "            avg_cost+=c/5\n",
    "        print(\"cost: \",avg_cost)\n",
    "    a=sess.run(accuracy,feed_dict={X:test_x,Y:test_y,keep_prob:1})\n",
    "    print('accuracy=',a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cifar10 출처: http://solarisailab.com/archives/2325\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "CIFAR-10 Convolutional Neural Networks(CNN) Example\n",
    "next_batch function is copied from edo's answer\n",
    "https://stackoverflow.com/questions/40994583/how-to-implement-tensorflows-next-batch-for-own-data\n",
    "Author : solaris33\n",
    "Project URL : http://solarisailab.com/archives/2325\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# CIFAR-10 데이터를 다운로드 받기 위한 keras의 helper 함수인 load_data 함수를 임포트합니다.\n",
    "from tensorflow.keras.datasets.cifar10 import load_data\n",
    "\n",
    "# 다음 배치를 읽어오기 위한 next_batch 유틸리티 함수를 정의합니다.\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    `num` 개수 만큼의 랜덤한 샘플들과 레이블들을 리턴합니다.\n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n",
    "\n",
    "# CNN 모델을 정의합니다. \n",
    "def build_CNN_classifier(x):\n",
    "  # 입력 이미지\n",
    "    x_image = x\n",
    "\n",
    "    # 첫번째 convolutional layer - 하나의 grayscale 이미지를 64개의 특징들(feature)으로 맵핑(maping)합니다.\n",
    "    W_conv1 = tf.Variable(tf.truncated_normal(shape=[5, 5, 3, 64], stddev=5e-2))\n",
    "    b_conv1 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "    h_conv1 = tf.nn.relu(tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1)\n",
    "\n",
    "    # 첫번째 Pooling layer\n",
    "    h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    # 두번째 convolutional layer - 32개의 특징들(feature)을 64개의 특징들(feature)로 맵핑(maping)합니다.\n",
    "    W_conv2 = tf.Variable(tf.truncated_normal(shape=[5, 5, 64, 64], stddev=5e-2))\n",
    "    b_conv2 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "    h_conv2 = tf.nn.relu(tf.nn.conv2d(h_pool1, W_conv2, strides=[1, 1, 1, 1], padding='SAME') + b_conv2)\n",
    "\n",
    "    # 두번째 pooling layer.\n",
    "    h_pool2 = tf.nn.max_pool(h_conv2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    # 세번째 convolutional layer\n",
    "    W_conv3 = tf.Variable(tf.truncated_normal(shape=[3, 3, 64, 128], stddev=5e-2))\n",
    "    b_conv3 = tf.Variable(tf.constant(0.1, shape=[128]))\n",
    "    h_conv3 = tf.nn.relu(tf.nn.conv2d(h_pool2, W_conv3, strides=[1, 1, 1, 1], padding='SAME') + b_conv3)\n",
    "\n",
    "    # 네번째 convolutional layer\n",
    "    W_conv4 = tf.Variable(tf.truncated_normal(shape=[3, 3, 128, 128], stddev=5e-2))\n",
    "    b_conv4 = tf.Variable(tf.constant(0.1, shape=[128])) \n",
    "    h_conv4 = tf.nn.relu(tf.nn.conv2d(h_conv3, W_conv4, strides=[1, 1, 1, 1], padding='SAME') + b_conv4)\n",
    "\n",
    "    # 다섯번째 convolutional layer\n",
    "    W_conv5 = tf.Variable(tf.truncated_normal(shape=[3, 3, 128, 128], stddev=5e-2))\n",
    "    b_conv5 = tf.Variable(tf.constant(0.1, shape=[128]))\n",
    "    h_conv5 = tf.nn.relu(tf.nn.conv2d(h_conv4, W_conv5, strides=[1, 1, 1, 1], padding='SAME') + b_conv5)\n",
    "\n",
    "    # Fully Connected Layer 1 - 2번의 downsampling 이후에, 우리의 32x32 이미지는 8x8x128 특징맵(feature map)이 됩니다.\n",
    "    # 이를 384개의 특징들로 맵핑(maping)합니다.\n",
    "    W_fc1 = tf.Variable(tf.truncated_normal(shape=[8 * 8 * 128, 384], stddev=5e-2))\n",
    "    b_fc1 = tf.Variable(tf.constant(0.1, shape=[384]))\n",
    "\n",
    "    h_conv5_flat = tf.reshape(h_conv5, [-1, 8*8*128])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_conv5_flat, W_fc1) + b_fc1)\n",
    "\n",
    "    # Dropout - 모델의 복잡도를 컨트롤합니다. 특징들의 co-adaptation을 방지합니다.\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob) \n",
    "\n",
    "    # Fully Connected Layer 2 - 384개의 특징들(feature)을 10개의 클래스-airplane, automobile, bird...-로 맵핑(maping)합니다.\n",
    "    W_fc2 = tf.Variable(tf.truncated_normal(shape=[384, 10], stddev=5e-2))\n",
    "    b_fc2 = tf.Variable(tf.constant(0.1, shape=[10]))\n",
    "    logits = tf.matmul(h_fc1_drop,W_fc2) + b_fc2\n",
    "    y_pred = tf.nn.softmax(logits)\n",
    "\n",
    "    return y_pred, logits\n",
    "\n",
    "# 인풋 아웃풋 데이터, 드롭아웃 확률을 입력받기위한 플레이스홀더를 정의합니다.\n",
    "x = tf.placeholder(tf.float32, shape=[None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# CIFAR-10 데이터를 다운로드하고 데이터를 불러옵니다.\n",
    "(x_train, y_train), (x_test, y_test) = load_data()\n",
    "# scalar 형태의 레이블(0~9)을 One-hot Encoding 형태로 변환합니다.\n",
    "y_train_one_hot = tf.squeeze(tf.one_hot(y_train, 10),axis=1)\n",
    "y_test_one_hot = tf.squeeze(tf.one_hot(y_test, 10),axis=1)\n",
    "\n",
    "# Convolutional Neural Networks(CNN) 그래프를 생성합니다.\n",
    "y_pred, logits = build_CNN_classifier(x)\n",
    "\n",
    "# Cross Entropy를 비용함수(loss function)으로 정의하고, RMSPropOptimizer를 이용해서 비용 함수를 최소화합니다.\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "train_step = tf.train.RMSPropOptimizer(1e-3).minimize(loss)\n",
    "\n",
    "# 정확도를 계산하는 연산을 추가합니다.\n",
    "correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# 세션을 열어 실제 학습을 진행합니다.\n",
    "with tf.Session() as sess:\n",
    "  # 모든 변수들을 초기화한다. \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "  \n",
    "    # 10000 Step만큼 최적화를 수행합니다.\n",
    "    for i in range(10000):\n",
    "        batch = next_batch(128, x_train, y_train_one_hot.eval())\n",
    "\n",
    "    # 100 Step마다 training 데이터셋에 대한 정확도와 loss를 출력합니다.\n",
    "        if i % 100 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={x: batch[0], y: batch[1], keep_prob: 1.0})\n",
    "            loss_print = loss.eval(feed_dict={x: batch[0], y: batch[1], keep_prob: 1.0})\n",
    "\n",
    "            print(\"반복(Epoch): %d, 트레이닝 데이터 정확도: %f, 손실 함수(loss): %f\" % (i, train_accuracy, loss_print))\n",
    "    # 20% 확률의 Dropout을 이용해서 학습을 진행합니다.\n",
    "        sess.run(train_step, feed_dict={x: batch[0], y: batch[1], keep_prob: 0.8})\n",
    "\n",
    "  # 학습이 끝나면 테스트 데이터(10000개)에 대한 정확도를 출력합니다.  \n",
    "    test_accuracy = 0.0  \n",
    "    for i in range(10):\n",
    "        test_batch = next_batch(1000, x_test, y_test_one_hot.eval())\n",
    "        test_accuracy = test_accuracy + accuracy.eval(feed_dict={x: test_batch[0], y: test_batch[1], keep_prob: 1.0})\n",
    "    test_accuracy = test_accuracy / 10;\n",
    "    print(\"테스트 데이터 정확도: %f\" % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#43 RNN basic\n",
    "input_dim=5\n",
    "hidden_size=5\n",
    "sequence_length=6\n",
    "batch_size=1\n",
    "\n",
    "idx2char=['h','i','e','l','o']\n",
    "x_data=[[0,1,0,2,3,3]]\n",
    "x_one_hot=[[[1,0,0,0,0],[0,1,0,0,0],[1,0,0,0,0],[0,0,1,0,0],[0,0,0,1,0],[0,0,0,1,0]]]\n",
    "y_data=[[1,0,2,3,3,4]]\n",
    "\n",
    "X=tf.placeholder(tf.float32,[None,sequence_length,input_dim])\n",
    "Y=tf.placeholder(tf.int32,[None,sequence_length])\n",
    "\n",
    "cell=tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size,state_is_tuple=True)\n",
    "initial_state=cell.zero_state(batch_size,tf.float32)\n",
    "#단순화를 위해 rnn에서 나온 output을 바로 sequence_loss에 사용(잘못된 것임)\n",
    "outputs,_state=tf.nn.dynamic_rnn(cell,X,initial_state=initial_state,dtype=tf.float32)\n",
    "weights=tf.ones([batch_size,sequence_length])\n",
    "\n",
    "sequence_loss=tf.contrib.seq2seq.sequence_loss(logits=outputs,targets=Y,weights=weights)\n",
    "loss=tf.reduce_mean(sequence_loss)\n",
    "train=tf.train.AdamOptimizer(0.1).minimize(loss)\n",
    "\n",
    "prediction=tf.argmax(outputs,axis=2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(2000):\n",
    "        l,_=sess.run([loss,train],feed_dict={X:x_one_hot,Y:y_data})\n",
    "        result=sess.run(prediction,feed_dict={X:x_one_hot})\n",
    "        print(i,\"loss:\",l,\"prediction: \",result,\"true Y:\",y_data)\n",
    "        result_str=[idx2char[c] for c in np.squeeze(result)]\n",
    "        print(\"\\tPrediction str: \",''.join(result_str))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#44 rnn long sequence\n",
    "sample='if you want you'\n",
    "\n",
    "idx2char=list(set(sample))\n",
    "char2idx={c: i for i,c in enumerate(idx2char)}\n",
    "sample_idx=[char2idx[c] for c in sample]\n",
    "x_data=[sample_idx[:-1]]\n",
    "y_data=[sample_idx[1:]]\n",
    "\n",
    "sequence_length=len(sample)-1\n",
    "num_classes=len(idx2char)\n",
    "dic_size=len(char2idx)\n",
    "rnn_hidden_size=len(char2idx)\n",
    "batch_size=1\n",
    "\n",
    "X=tf.placeholder(tf.int32,[None,sequence_length])\n",
    "Y=tf.placeholder(tf.int32,[None, sequence_length])\n",
    "\n",
    "x_one_hot=tf.one_hot(X,num_classes)\n",
    "\n",
    "cell=tf.contrib.rnn.BasicLSTMCell(num_units=rnn_hidden_size,state_is_tuple=True)\n",
    "initial_state=cell.zero_state(batch_size,tf.float32)\n",
    "outputs,_state=tf.nn.dynamic_rnn(cell,x_one_hot,initial_state=initial_state,dtype=tf.float32)\n",
    "\n",
    "weights=tf.ones([batch_size,sequence_length])\n",
    "sequence_loss=tf.contrib.seq2seq.sequence_loss(logits=outputs,targets=Y,weights=weights)\n",
    "loss=tf.reduce_mean(sequence_loss)\n",
    "train=tf.train.AdamOptimizer(0.1).minimize(loss)\n",
    "\n",
    "prediction=tf.argmax(outputs,axis=2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(2000):\n",
    "        l,_=sess.run([loss,train],feed_dict={X:x_data,Y:y_data})\n",
    "        result=sess.run(prediction,feed_dict={X:x_data})\n",
    "        result_str=[idx2char[c] for c in np.squeeze(result)]\n",
    "        print(i,\"loss:\",l,\"prediction:\",''.join(result_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#45 rnn longlong sequence\n",
    "sentence=(\"if you want to build a ship, don't drum up people together to \"\n",
    "        \"collect wood and don't assign them tasks and work, but rether \"\n",
    "        \"teach them to long for the endless immensity of the sea.\")\n",
    "char_set=list(set(sentence))\n",
    "char_dic={w:i for i,w in enumerate(char_set)}\n",
    "\n",
    "data_dim=len(char_set)\n",
    "hidden_size=len(char_set)\n",
    "num_classes=len(char_set)\n",
    "seq_length=10\n",
    "\n",
    "dataX=[]\n",
    "dataY=[]\n",
    "for i in range(len(sentence)-seq_length):\n",
    "    x_str=sentence[i:i+seq_length]\n",
    "    y_str=sentence[i+1:i+seq_length+1]\n",
    "    \n",
    "    x=[char_dic[c] for c in x_str]\n",
    "    y=[char_dic[c] for c in y_str]\n",
    "    \n",
    "    dataX.append(x)\n",
    "    dataY.append(y)\n",
    "    \n",
    "batch_size=len(dataX)\n",
    "\n",
    "X=tf.placeholder(tf.int32,[None,seq_length])\n",
    "Y=tf.placeholder(tf.int32,[None,seq_length])\n",
    "\n",
    "X_one_hot=tf.one_hot(X,num_classes)\n",
    "\n",
    "cell=tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size,state_is_tuple=True)\n",
    "cell=tf.contrib.rnn.MultiRNNCell([cell]*2,state_is_tuple=True)\n",
    "initial_state=cell.zero_state(batch_size,tf.float32)\n",
    "outputs,_=tf.nn.dynamic_rnn(cell,X_one_hot,initial_state=initial_state,dtype=tf.float32)\n",
    "\n",
    "X_for_softmax=tf.reshape(outputs,[-1,hidden_size])\n",
    "softmax_w=tf.get_variable(\"softmax_w\",[hidden_size,num_classes])\n",
    "softmax_b=tf.get_variable(\"softmax_b\",[num_classes])\n",
    "outputs=tf.matmul(X_for_softmax,softmax_w)+softmax_b\n",
    "outputs=tf.reshape(outputs,[batch_size,seq_length,num_classes])\n",
    "\n",
    "weights=tf.ones([batch_size,seq_length])\n",
    "seq_loss=tf.contrib.seq2seq.sequence_loss(logits=outputs,targets=Y,weights=weights)\n",
    "loss=tf.reduce_mean(seq_loss)\n",
    "train=tf.train.AdamOptimizer(0.1).minimize(loss)\n",
    "\n",
    "prediction=tf.argmax(outputs,2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(500):\n",
    "        l,_,results=sess.run([loss,train,outputs],feed_dict={X:dataX,Y:dataY})\n",
    "        for j,result in enumerate(results):\n",
    "            index=np.argmax(result,1)\n",
    "            print(i,j,''.join([char_set[t] for t in index]),l)\n",
    "    results=sess.run(outputs,feed_dict={X:dataX})\n",
    "    for j,result in enumerate(results):\n",
    "        index=np.argmax(result,1)\n",
    "        if j is 0:\n",
    "            print(''.join([char_set[t] for t in index]),end='')\n",
    "        else:\n",
    "            print(char_set[index[-1]],end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-afafe39ece20>:37: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "0 0.2709216\n",
      "1 0.14304486\n",
      "2 0.066729136\n",
      "3 0.031967755\n",
      "4 0.027456013\n",
      "5 0.038705554\n",
      "6 0.049588688\n",
      "7 0.051896032\n",
      "8 0.046285257\n",
      "9 0.036574196\n",
      "10 0.026318375\n",
      "11 0.017772926\n",
      "12 0.011903251\n",
      "13 0.00874076\n",
      "14 0.007776149\n",
      "15 0.00827312\n",
      "16 0.009480658\n",
      "17 0.010759722\n",
      "18 0.011647116\n",
      "19 0.011875723\n",
      "20 0.0113648055\n",
      "21 0.010190689\n",
      "22 0.00854526\n",
      "23 0.006688156\n",
      "24 0.0048970478\n",
      "25 0.0034197948\n",
      "26 0.0024325443\n",
      "27 0.0020089243\n",
      "28 0.002106649\n",
      "29 0.0025778874\n",
      "30 0.0032061203\n",
      "31 0.0037641558\n",
      "32 0.0040770937\n",
      "33 0.0040673576\n",
      "34 0.003764418\n",
      "35 0.0032782299\n",
      "36 0.0027516133\n",
      "37 0.0023123196\n",
      "38 0.0020404037\n",
      "39 0.0019567956\n",
      "40 0.0020306534\n",
      "41 0.002198385\n",
      "42 0.002386444\n",
      "43 0.0025313767\n",
      "44 0.0025931362\n",
      "45 0.0025601678\n",
      "46 0.002446908\n",
      "47 0.0022857205\n",
      "48 0.0021160431\n",
      "49 0.0019735838\n",
      "50 0.0018820134\n",
      "51 0.0018487724\n",
      "52 0.0018655432\n",
      "53 0.0019127519\n",
      "54 0.0019664983\n",
      "55 0.002005719\n",
      "56 0.0020175355\n",
      "57 0.0019994595\n",
      "58 0.0019582822\n",
      "59 0.00190647\n",
      "60 0.001857576\n",
      "61 0.001822162\n",
      "62 0.0018053292\n",
      "63 0.0018062455\n",
      "64 0.0018194172\n",
      "65 0.0018370261\n",
      "66 0.0018514883\n",
      "67 0.0018575036\n",
      "68 0.0018531301\n",
      "69 0.0018397592\n",
      "70 0.0018211731\n",
      "71 0.0018020784\n",
      "72 0.0017865987\n",
      "73 0.0017771571\n",
      "74 0.0017740142\n",
      "75 0.0017755228\n",
      "76 0.0017789233\n",
      "77 0.0017813743\n",
      "78 0.0017808602\n",
      "79 0.0017767053\n",
      "80 0.0017695779\n",
      "81 0.0017610574\n",
      "82 0.0017529695\n",
      "83 0.0017467435\n",
      "84 0.0017430035\n",
      "85 0.0017414904\n",
      "86 0.0017412907\n",
      "87 0.0017412447\n",
      "88 0.0017403679\n",
      "89 0.0017381521\n",
      "90 0.0017346505\n",
      "91 0.0017303647\n",
      "92 0.0017259963\n",
      "93 0.0017221654\n",
      "94 0.0017192077\n",
      "95 0.0017171018\n",
      "96 0.0017155338\n",
      "97 0.0017140592\n",
      "98 0.0017122839\n",
      "99 0.0017100028\n",
      "100 0.001707241\n",
      "101 0.0017042093\n",
      "102 0.0017011939\n",
      "103 0.0016984371\n",
      "104 0.0016960558\n",
      "105 0.0016940172\n",
      "106 0.0016921785\n",
      "107 0.0016903569\n",
      "108 0.0016884076\n",
      "109 0.0016862688\n",
      "110 0.0016839745\n",
      "111 0.0016816193\n",
      "112 0.0016793157\n",
      "113 0.0016771428\n",
      "114 0.0016751222\n",
      "115 0.0016732181\n",
      "116 0.0016713621\n",
      "117 0.0016694834\n",
      "118 0.0016675403\n",
      "119 0.0016655294\n",
      "120 0.0016634829\n",
      "121 0.001661447\n",
      "122 0.0016594633\n",
      "123 0.0016575479\n",
      "124 0.0016556943\n",
      "125 0.0016538762\n",
      "126 0.0016520629\n",
      "127 0.0016502336\n",
      "128 0.0016483823\n",
      "129 0.0016465207\n",
      "130 0.0016446677\n",
      "131 0.0016428389\n",
      "132 0.0016410451\n",
      "133 0.0016392812\n",
      "134 0.0016375382\n",
      "135 0.0016358019\n",
      "136 0.0016340633\n",
      "137 0.0016323203\n",
      "138 0.001630577\n",
      "139 0.0016288423\n",
      "140 0.0016271228\n",
      "141 0.0016254213\n",
      "142 0.0016237355\n",
      "143 0.001622061\n",
      "144 0.0016203917\n",
      "145 0.0016187236\n",
      "146 0.0016170571\n",
      "147 0.0016153941\n",
      "148 0.0016137382\n",
      "149 0.0016120921\n",
      "150 0.0016104562\n",
      "151 0.0016088288\n",
      "152 0.0016072078\n",
      "153 0.0016055902\n",
      "154 0.0016039753\n",
      "155 0.0016023639\n",
      "156 0.0016007561\n",
      "157 0.0015991542\n",
      "158 0.001597558\n",
      "159 0.0015959671\n",
      "160 0.0015943804\n",
      "161 0.0015927973\n",
      "162 0.001591216\n",
      "163 0.0015896362\n",
      "164 0.0015880592\n",
      "165 0.0015864846\n",
      "166 0.0015849129\n",
      "167 0.0015833442\n",
      "168 0.0015817779\n",
      "169 0.001580214\n",
      "170 0.0015786509\n",
      "171 0.0015770892\n",
      "172 0.0015755284\n",
      "173 0.0015739691\n",
      "174 0.0015724109\n",
      "175 0.0015708539\n",
      "176 0.0015692981\n",
      "177 0.0015677424\n",
      "178 0.0015661867\n",
      "179 0.0015646314\n",
      "180 0.001563075\n",
      "181 0.0015615192\n",
      "182 0.0015599633\n",
      "183 0.001558407\n",
      "184 0.00155685\n",
      "185 0.0015552926\n",
      "186 0.0015537342\n",
      "187 0.001552175\n",
      "188 0.0015506147\n",
      "189 0.001549053\n",
      "190 0.0015474902\n",
      "191 0.0015459264\n",
      "192 0.0015443607\n",
      "193 0.0015427935\n",
      "194 0.0015412245\n",
      "195 0.0015396537\n",
      "196 0.0015380811\n",
      "197 0.001536506\n",
      "198 0.0015349291\n",
      "199 0.0015333496\n",
      "200 0.0015317682\n",
      "201 0.0015301841\n",
      "202 0.0015285976\n",
      "203 0.001527009\n",
      "204 0.0015254172\n",
      "205 0.0015238228\n",
      "206 0.0015222254\n",
      "207 0.0015206251\n",
      "208 0.0015190226\n",
      "209 0.0015174161\n",
      "210 0.0015158069\n",
      "211 0.0015141946\n",
      "212 0.0015125786\n",
      "213 0.0015109597\n",
      "214 0.0015093373\n",
      "215 0.0015077115\n",
      "216 0.0015060827\n",
      "217 0.0015044499\n",
      "218 0.001502813\n",
      "219 0.0015011735\n",
      "220 0.0014995297\n",
      "221 0.0014978821\n",
      "222 0.0014962311\n",
      "223 0.001494576\n",
      "224 0.0014929171\n",
      "225 0.001491254\n",
      "226 0.0014895869\n",
      "227 0.0014879163\n",
      "228 0.0014862415\n",
      "229 0.0014845624\n",
      "230 0.001482879\n",
      "231 0.0014811916\n",
      "232 0.0014795002\n",
      "233 0.0014778046\n",
      "234 0.0014761047\n",
      "235 0.0014744003\n",
      "236 0.0014726915\n",
      "237 0.001470979\n",
      "238 0.0014692614\n",
      "239 0.0014675398\n",
      "240 0.0014658133\n",
      "241 0.0014640827\n",
      "242 0.0014623476\n",
      "243 0.0014606077\n",
      "244 0.001458864\n",
      "245 0.0014571152\n",
      "246 0.0014553618\n",
      "247 0.001453604\n",
      "248 0.0014518418\n",
      "249 0.0014500746\n",
      "250 0.001448303\n",
      "251 0.0014465266\n",
      "252 0.0014447462\n",
      "253 0.0014429605\n",
      "254 0.0014411707\n",
      "255 0.0014393757\n",
      "256 0.0014375764\n",
      "257 0.0014357722\n",
      "258 0.0014339633\n",
      "259 0.0014321498\n",
      "260 0.001430332\n",
      "261 0.0014285094\n",
      "262 0.001426682\n",
      "263 0.00142485\n",
      "264 0.0014230135\n",
      "265 0.0014211717\n",
      "266 0.0014193262\n",
      "267 0.0014174758\n",
      "268 0.0014156207\n",
      "269 0.0014137613\n",
      "270 0.001411897\n",
      "271 0.0014100284\n",
      "272 0.0014081552\n",
      "273 0.0014062773\n",
      "274 0.0014043952\n",
      "275 0.0014025088\n",
      "276 0.0014006176\n",
      "277 0.0013987224\n",
      "278 0.0013968225\n",
      "279 0.0013949187\n",
      "280 0.0013930104\n",
      "281 0.001391098\n",
      "282 0.0013891815\n",
      "283 0.0013872609\n",
      "284 0.0013853357\n",
      "285 0.001383407\n",
      "286 0.0013814742\n",
      "287 0.0013795376\n",
      "288 0.001377597\n",
      "289 0.0013756527\n",
      "290 0.0013737049\n",
      "291 0.0013717528\n",
      "292 0.0013697974\n",
      "293 0.0013678386\n",
      "294 0.0013658765\n",
      "295 0.0013639109\n",
      "296 0.0013619419\n",
      "297 0.0013599703\n",
      "298 0.0013579949\n",
      "299 0.0013560168\n",
      "300 0.001354036\n",
      "301 0.0013520522\n",
      "302 0.001350066\n",
      "303 0.001348077\n",
      "304 0.0013460852\n",
      "305 0.0013440921\n",
      "306 0.0013420958\n",
      "307 0.0013400979\n",
      "308 0.0013380981\n",
      "309 0.0013360961\n",
      "310 0.0013340927\n",
      "311 0.0013320876\n",
      "312 0.0013300816\n",
      "313 0.0013280741\n",
      "314 0.0013260652\n",
      "315 0.0013240558\n",
      "316 0.0013220457\n",
      "317 0.001320035\n",
      "318 0.0013180237\n",
      "319 0.0013160121\n",
      "320 0.0013140007\n",
      "321 0.0013119899\n",
      "322 0.001309979\n",
      "323 0.0013079685\n",
      "324 0.001305959\n",
      "325 0.0013039502\n",
      "326 0.0013019426\n",
      "327 0.0012999366\n",
      "328 0.0012979322\n",
      "329 0.0012959297\n",
      "330 0.0012939291\n",
      "331 0.0012919308\n",
      "332 0.0012899353\n",
      "333 0.001287942\n",
      "334 0.0012859519\n",
      "335 0.0012839653\n",
      "336 0.0012819822\n",
      "337 0.0012800022\n",
      "338 0.0012780268\n",
      "339 0.0012760552\n",
      "340 0.0012740885\n",
      "341 0.0012721267\n",
      "342 0.0012701695\n",
      "343 0.0012682177\n",
      "344 0.0012662713\n",
      "345 0.0012643307\n",
      "346 0.0012623968\n",
      "347 0.0012604683\n",
      "348 0.0012585468\n",
      "349 0.0012566324\n",
      "350 0.0012547248\n",
      "351 0.0012528246\n",
      "352 0.001250932\n",
      "353 0.0012490471\n",
      "354 0.0012471705\n",
      "355 0.0012453024\n",
      "356 0.0012434429\n",
      "357 0.0012415914\n",
      "358 0.00123975\n",
      "359 0.0012379176\n",
      "360 0.0012360947\n",
      "361 0.0012342815\n",
      "362 0.001232478\n",
      "363 0.0012306848\n",
      "364 0.0012289023\n",
      "365 0.0012271298\n",
      "366 0.0012253678\n",
      "367 0.0012236171\n",
      "368 0.0012218778\n",
      "369 0.0012201491\n",
      "370 0.0012184314\n",
      "371 0.0012167254\n",
      "372 0.0012150314\n",
      "373 0.0012133484\n",
      "374 0.0012116777\n",
      "375 0.0012100184\n",
      "376 0.0012083712\n",
      "377 0.0012067361\n",
      "378 0.0012051126\n",
      "379 0.0012035017\n",
      "380 0.0012019022\n",
      "381 0.0012003151\n",
      "382 0.0011987397\n",
      "383 0.0011971765\n",
      "384 0.0011956256\n",
      "385 0.0011940863\n",
      "386 0.001192559\n",
      "387 0.0011910436\n",
      "388 0.0011895401\n",
      "389 0.0011880477\n",
      "390 0.0011865672\n",
      "391 0.0011850985\n",
      "392 0.001183641\n",
      "393 0.0011821944\n",
      "394 0.0011807584\n",
      "395 0.0011793342\n",
      "396 0.0011779203\n",
      "397 0.0011765165\n",
      "398 0.0011751238\n",
      "399 0.0011737405\n",
      "400 0.0011723674\n",
      "401 0.0011710041\n",
      "402 0.0011696505\n",
      "403 0.001168306\n",
      "404 0.0011669704\n",
      "405 0.0011656439\n",
      "406 0.0011643257\n",
      "407 0.0011630162\n",
      "408 0.0011617146\n",
      "409 0.0011604213\n",
      "410 0.0011591348\n",
      "411 0.0011578563\n",
      "412 0.0011565846\n",
      "413 0.0011553197\n",
      "414 0.0011540616\n",
      "415 0.0011528092\n",
      "416 0.001151564\n",
      "417 0.0011503241\n",
      "418 0.0011490901\n",
      "419 0.0011478605\n",
      "420 0.0011466364\n",
      "421 0.0011454176\n",
      "422 0.001144203\n",
      "423 0.001142993\n",
      "424 0.0011417867\n",
      "425 0.001140585\n",
      "426 0.0011393866\n",
      "427 0.0011381912\n",
      "428 0.0011369997\n",
      "429 0.001135811\n",
      "430 0.0011346249\n",
      "431 0.0011334413\n",
      "432 0.0011322601\n",
      "433 0.0011310815\n",
      "434 0.0011299046\n",
      "435 0.0011287297\n",
      "436 0.0011275565\n",
      "437 0.0011263848\n",
      "438 0.001125214\n",
      "439 0.0011240452\n",
      "440 0.0011228768\n",
      "441 0.0011217095\n",
      "442 0.0011205431\n",
      "443 0.001119377\n",
      "444 0.0011182111\n",
      "445 0.0011170459\n",
      "446 0.001115881\n",
      "447 0.0011147158\n",
      "448 0.0011135513\n",
      "449 0.0011123864\n",
      "450 0.0011112209\n",
      "451 0.0011100553\n",
      "452 0.0011088891\n",
      "453 0.0011077229\n",
      "454 0.001106556\n",
      "455 0.0011053882\n",
      "456 0.0011042198\n",
      "457 0.0011030504\n",
      "458 0.0011018805\n",
      "459 0.0011007093\n",
      "460 0.0010995376\n",
      "461 0.0010983647\n",
      "462 0.0010971908\n",
      "463 0.0010960154\n",
      "464 0.0010948392\n",
      "465 0.0010936616\n",
      "466 0.0010924829\n",
      "467 0.0010913025\n",
      "468 0.0010901215\n",
      "469 0.0010889383\n",
      "470 0.0010877546\n",
      "471 0.001086569\n",
      "472 0.0010853822\n",
      "473 0.0010841939\n",
      "474 0.001083004\n",
      "475 0.001081813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "476 0.0010806203\n",
      "477 0.0010794261\n",
      "478 0.0010782303\n",
      "479 0.0010770332\n",
      "480 0.0010758345\n",
      "481 0.0010746341\n",
      "482 0.0010734326\n",
      "483 0.0010722292\n",
      "484 0.0010710246\n",
      "485 0.0010698182\n",
      "486 0.0010686103\n",
      "487 0.001067401\n",
      "488 0.0010661902\n",
      "489 0.0010649781\n",
      "490 0.001063764\n",
      "491 0.0010625491\n",
      "492 0.0010613322\n",
      "493 0.001060114\n",
      "494 0.001058895\n",
      "495 0.001057674\n",
      "496 0.0010564515\n",
      "497 0.0010552278\n",
      "498 0.0010540024\n",
      "499 0.0010527767\n",
      "500 0.0010515489\n",
      "501 0.0010503201\n",
      "502 0.00104909\n",
      "503 0.0010478586\n",
      "504 0.001046626\n",
      "505 0.0010453926\n",
      "506 0.0010441577\n",
      "507 0.0010429219\n",
      "508 0.0010416848\n",
      "509 0.001040447\n",
      "510 0.0010392081\n",
      "511 0.001037968\n",
      "512 0.0010367275\n",
      "513 0.0010354861\n",
      "514 0.0010342436\n",
      "515 0.0010330002\n",
      "516 0.0010317566\n",
      "517 0.0010305118\n",
      "518 0.0010292665\n",
      "519 0.001028021\n",
      "520 0.0010267743\n",
      "521 0.0010255281\n",
      "522 0.0010242806\n",
      "523 0.0010230328\n",
      "524 0.0010217852\n",
      "525 0.0010205371\n",
      "526 0.0010192887\n",
      "527 0.00101804\n",
      "528 0.0010167916\n",
      "529 0.001015543\n",
      "530 0.0010142945\n",
      "531 0.0010130461\n",
      "532 0.0010117978\n",
      "533 0.0010105502\n",
      "534 0.0010093028\n",
      "535 0.0010080555\n",
      "536 0.001006809\n",
      "537 0.001005563\n",
      "538 0.0010043175\n",
      "539 0.0010030732\n",
      "540 0.001001829\n",
      "541 0.0010005864\n",
      "542 0.0009993442\n",
      "543 0.0009981038\n",
      "544 0.0009968636\n",
      "545 0.0009956253\n",
      "546 0.0009943877\n",
      "547 0.0009931521\n",
      "548 0.0009919181\n",
      "549 0.0009906857\n",
      "550 0.0009894547\n",
      "551 0.0009882257\n",
      "552 0.0009869984\n",
      "553 0.0009857732\n",
      "554 0.0009845503\n",
      "555 0.0009833294\n",
      "556 0.0009821106\n",
      "557 0.0009808945\n",
      "558 0.0009796812\n",
      "559 0.0009784704\n",
      "560 0.0009772622\n",
      "561 0.00097605656\n",
      "562 0.0009748541\n",
      "563 0.0009736548\n",
      "564 0.00097245886\n",
      "565 0.00097126584\n",
      "566 0.00097007654\n",
      "567 0.00096889044\n",
      "568 0.0009677082\n",
      "569 0.0009665299\n",
      "570 0.00096535526\n",
      "571 0.0009641845\n",
      "572 0.0009630174\n",
      "573 0.00096185517\n",
      "574 0.00096069695\n",
      "575 0.00095954316\n",
      "576 0.00095839374\n",
      "577 0.00095724955\n",
      "578 0.0009561096\n",
      "579 0.0009549747\n",
      "580 0.00095384434\n",
      "581 0.00095271965\n",
      "582 0.0009515994\n",
      "583 0.0009504851\n",
      "584 0.0009493759\n",
      "585 0.0009482725\n",
      "586 0.00094717427\n",
      "587 0.00094608177\n",
      "588 0.00094499555\n",
      "589 0.0009439148\n",
      "590 0.00094284053\n",
      "591 0.0009417718\n",
      "592 0.0009407095\n",
      "593 0.00093965314\n",
      "594 0.00093860296\n",
      "595 0.0009375597\n",
      "596 0.0009365226\n",
      "597 0.0009354923\n",
      "598 0.0009344688\n",
      "599 0.0009334516\n",
      "600 0.00093244127\n",
      "601 0.0009314378\n",
      "602 0.0009304411\n",
      "603 0.00092945143\n",
      "604 0.00092846906\n",
      "605 0.0009274934\n",
      "606 0.00092652516\n",
      "607 0.00092556345\n",
      "608 0.0009246093\n",
      "609 0.00092366245\n",
      "610 0.0009227229\n",
      "611 0.00092179\n",
      "612 0.0009208651\n",
      "613 0.0009199472\n",
      "614 0.00091903645\n",
      "615 0.0009181332\n",
      "616 0.0009172373\n",
      "617 0.00091634894\n",
      "618 0.0009154678\n",
      "619 0.0009145941\n",
      "620 0.00091372745\n",
      "621 0.00091286836\n",
      "622 0.0009120166\n",
      "623 0.00091117166\n",
      "624 0.0009103342\n",
      "625 0.000909504\n",
      "626 0.00090868125\n",
      "627 0.0009078656\n",
      "628 0.0009070565\n",
      "629 0.00090625527\n",
      "630 0.00090546074\n",
      "631 0.0009046733\n",
      "632 0.0009038926\n",
      "633 0.00090311904\n",
      "634 0.00090235216\n",
      "635 0.0009015918\n",
      "636 0.0009008386\n",
      "637 0.00090009184\n",
      "638 0.0008993522\n",
      "639 0.00089861825\n",
      "640 0.0008978916\n",
      "641 0.0008971707\n",
      "642 0.0008964565\n",
      "643 0.0008957483\n",
      "644 0.00089504675\n",
      "645 0.0008943508\n",
      "646 0.00089366094\n",
      "647 0.00089297735\n",
      "648 0.00089229934\n",
      "649 0.00089162705\n",
      "650 0.00089096086\n",
      "651 0.0008903004\n",
      "652 0.00088964484\n",
      "653 0.0008889951\n",
      "654 0.000888351\n",
      "655 0.000887712\n",
      "656 0.0008870782\n",
      "657 0.0008864496\n",
      "658 0.00088582624\n",
      "659 0.0008852077\n",
      "660 0.0008845944\n",
      "661 0.00088398554\n",
      "662 0.00088338164\n",
      "663 0.00088278245\n",
      "664 0.0008821876\n",
      "665 0.0008815978\n",
      "666 0.0008810123\n",
      "667 0.0008804308\n",
      "668 0.0008798544\n",
      "669 0.00087928167\n",
      "670 0.0008787136\n",
      "671 0.0008781487\n",
      "672 0.0008775887\n",
      "673 0.00087703235\n",
      "674 0.00087647984\n",
      "675 0.00087593147\n",
      "676 0.00087538635\n",
      "677 0.00087484566\n",
      "678 0.0008743086\n",
      "679 0.0008737748\n",
      "680 0.00087324466\n",
      "681 0.0008727179\n",
      "682 0.00087219477\n",
      "683 0.0008716749\n",
      "684 0.0008711587\n",
      "685 0.000870645\n",
      "686 0.00087013503\n",
      "687 0.0008696285\n",
      "688 0.00086912443\n",
      "689 0.00086862384\n",
      "690 0.0008681266\n",
      "691 0.0008676315\n",
      "692 0.0008671401\n",
      "693 0.0008666512\n",
      "694 0.0008661649\n",
      "695 0.0008656822\n",
      "696 0.00086520164\n",
      "697 0.00086472434\n",
      "698 0.00086424913\n",
      "699 0.000863777\n",
      "700 0.0008633072\n",
      "701 0.0008628401\n",
      "702 0.00086237583\n",
      "703 0.0008619139\n",
      "704 0.0008614543\n",
      "705 0.00086099695\n",
      "706 0.0008605423\n",
      "707 0.0008600903\n",
      "708 0.00085964013\n",
      "709 0.00085919286\n",
      "710 0.00085874775\n",
      "711 0.00085830473\n",
      "712 0.0008578634\n",
      "713 0.00085742556\n",
      "714 0.0008569889\n",
      "715 0.00085655495\n",
      "716 0.00085612293\n",
      "717 0.0008556931\n",
      "718 0.0008552654\n",
      "719 0.00085483986\n",
      "720 0.0008544163\n",
      "721 0.0008539944\n",
      "722 0.000853575\n",
      "723 0.00085315737\n",
      "724 0.000852742\n",
      "725 0.00085232855\n",
      "726 0.0008519172\n",
      "727 0.000851507\n",
      "728 0.0008510994\n",
      "729 0.00085069344\n",
      "730 0.0008502896\n",
      "731 0.00084988785\n",
      "732 0.0008494872\n",
      "733 0.0008490891\n",
      "734 0.00084869214\n",
      "735 0.00084829744\n",
      "736 0.0008479041\n",
      "737 0.00084751326\n",
      "738 0.0008471241\n",
      "739 0.000846736\n",
      "740 0.0008463504\n",
      "741 0.0008459658\n",
      "742 0.0008455837\n",
      "743 0.0008452026\n",
      "744 0.00084482384\n",
      "745 0.00084444636\n",
      "746 0.0008440705\n",
      "747 0.0008436968\n",
      "748 0.00084332435\n",
      "749 0.00084295357\n",
      "750 0.0008425843\n",
      "751 0.00084221706\n",
      "752 0.00084185135\n",
      "753 0.0008414873\n",
      "754 0.0008411247\n",
      "755 0.0008407636\n",
      "756 0.0008404042\n",
      "757 0.00084004656\n",
      "758 0.0008396903\n",
      "759 0.0008393354\n",
      "760 0.00083898293\n",
      "761 0.0008386314\n",
      "762 0.00083828106\n",
      "763 0.00083793316\n",
      "764 0.0008375862\n",
      "765 0.0008372405\n",
      "766 0.0008368972\n",
      "767 0.0008365546\n",
      "768 0.0008362138\n",
      "769 0.00083587464\n",
      "770 0.00083553686\n",
      "771 0.0008352006\n",
      "772 0.00083486614\n",
      "773 0.00083453266\n",
      "774 0.00083420094\n",
      "775 0.0008338709\n",
      "776 0.00083354226\n",
      "777 0.0008332147\n",
      "778 0.00083288905\n",
      "779 0.0008325644\n",
      "780 0.0008322416\n",
      "781 0.0008319195\n",
      "782 0.0008315999\n",
      "783 0.0008312813\n",
      "784 0.0008309643\n",
      "785 0.0008306483\n",
      "786 0.00083033403\n",
      "787 0.0008300209\n",
      "788 0.0008297092\n",
      "789 0.0008293994\n",
      "790 0.0008290905\n",
      "791 0.00082878314\n",
      "792 0.0008284772\n",
      "793 0.0008281725\n",
      "794 0.00082786917\n",
      "795 0.0008275675\n",
      "796 0.0008272669\n",
      "797 0.00082696794\n",
      "798 0.0008266703\n",
      "799 0.00082637393\n",
      "800 0.0008260786\n",
      "801 0.00082578487\n",
      "802 0.00082549284\n",
      "803 0.00082520157\n",
      "804 0.0008249114\n",
      "805 0.00082462304\n",
      "806 0.00082433585\n",
      "807 0.00082405045\n",
      "808 0.0008237659\n",
      "809 0.00082348235\n",
      "810 0.00082320045\n",
      "811 0.00082292024\n",
      "812 0.0008226404\n",
      "813 0.00082236214\n",
      "814 0.00082208525\n",
      "815 0.00082180975\n",
      "816 0.0008215354\n",
      "817 0.00082126237\n",
      "818 0.0008209902\n",
      "819 0.00082071935\n",
      "820 0.00082044996\n",
      "821 0.0008201821\n",
      "822 0.00081991445\n",
      "823 0.00081964885\n",
      "824 0.00081938464\n",
      "825 0.0008191214\n",
      "826 0.0008188586\n",
      "827 0.00081859785\n",
      "828 0.00081833795\n",
      "829 0.000818079\n",
      "830 0.00081782154\n",
      "831 0.000817565\n",
      "832 0.0008173097\n",
      "833 0.0008170554\n",
      "834 0.0008168032\n",
      "835 0.0008165509\n",
      "836 0.0008163\n",
      "837 0.00081605045\n",
      "838 0.0008158019\n",
      "839 0.0008155545\n",
      "840 0.0008153082\n",
      "841 0.00081506284\n",
      "842 0.00081481907\n",
      "843 0.00081457576\n",
      "844 0.0008143339\n",
      "845 0.00081409264\n",
      "846 0.00081385294\n",
      "847 0.000813614\n",
      "848 0.00081337633\n",
      "849 0.00081313954\n",
      "850 0.0008129042\n",
      "851 0.00081266905\n",
      "852 0.0008124356\n",
      "853 0.0008122032\n",
      "854 0.0008119712\n",
      "855 0.0008117405\n",
      "856 0.00081151084\n",
      "857 0.0008112821\n",
      "858 0.0008110543\n",
      "859 0.0008108276\n",
      "860 0.00081060245\n",
      "861 0.0008103771\n",
      "862 0.00081015343\n",
      "863 0.00080993044\n",
      "864 0.0008097088\n",
      "865 0.0008094876\n",
      "866 0.00080926775\n",
      "867 0.00080904865\n",
      "868 0.00080883026\n",
      "869 0.00080861297\n",
      "870 0.00080839626\n",
      "871 0.0008081808\n",
      "872 0.00080796704\n",
      "873 0.0008077526\n",
      "874 0.00080753956\n",
      "875 0.00080732803\n",
      "876 0.00080711656\n",
      "877 0.0008069063\n",
      "878 0.00080669724\n",
      "879 0.00080648874\n",
      "880 0.0008062807\n",
      "881 0.0008060743\n",
      "882 0.00080586836\n",
      "883 0.0008056632\n",
      "884 0.0008054588\n",
      "885 0.0008052553\n",
      "886 0.00080505287\n",
      "887 0.0008048506\n",
      "888 0.00080464955\n",
      "889 0.00080444966\n",
      "890 0.0008042502\n",
      "891 0.00080405123\n",
      "892 0.0008038534\n",
      "893 0.0008036561\n",
      "894 0.0008034602\n",
      "895 0.0008032644\n",
      "896 0.00080306974\n",
      "897 0.00080287503\n",
      "898 0.0008026819\n",
      "899 0.00080248946\n",
      "900 0.0008022975\n",
      "901 0.0008021063\n",
      "902 0.00080191635\n",
      "903 0.0008017265\n",
      "904 0.0008015376\n",
      "905 0.00080134935\n",
      "906 0.0008011619\n",
      "907 0.00080097525\n",
      "908 0.000800789\n",
      "909 0.0008006035\n",
      "910 0.00080041913\n",
      "911 0.0008002352\n",
      "912 0.0008000515\n",
      "913 0.0007998692\n",
      "914 0.0007996868\n",
      "915 0.0007995057\n",
      "916 0.00079932535\n",
      "917 0.0007991455\n",
      "918 0.0007989659\n",
      "919 0.00079878693\n",
      "920 0.0007986091\n",
      "921 0.00079843204\n",
      "922 0.0007982554\n",
      "923 0.00079807953\n",
      "924 0.0007979041\n",
      "925 0.0007977298\n",
      "926 0.00079755706\n",
      "927 0.000797386\n",
      "928 0.00079721987\n",
      "929 0.00079706043\n",
      "930 0.0007969191\n",
      "931 0.00079681724\n",
      "932 0.00079680735\n",
      "933 0.00079701253\n",
      "934 0.0007977085\n",
      "935 0.0007994836\n",
      "936 0.0008033011\n",
      "937 0.0008100832\n",
      "938 0.00081813795\n",
      "939 0.0008222454\n",
      "940 0.0008151732\n",
      "941 0.00080198224\n",
      "942 0.00079492776\n",
      "943 0.0007993731\n",
      "944 0.0008069595\n",
      "945 0.000806719\n",
      "946 0.0007992188\n",
      "947 0.000794197\n",
      "948 0.00079693645\n",
      "949 0.00080146553\n",
      "950 0.0008003847\n",
      "951 0.0007954742\n",
      "952 0.0007934989\n",
      "953 0.0007960252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "954 0.00079806853\n",
      "955 0.000796208\n",
      "956 0.0007932762\n",
      "957 0.00079308305\n",
      "958 0.0007949047\n",
      "959 0.00079537183\n",
      "960 0.0007936309\n",
      "961 0.000792138\n",
      "962 0.0007925304\n",
      "963 0.0007935481\n",
      "964 0.0007933493\n",
      "965 0.0007920932\n",
      "966 0.00079136994\n",
      "967 0.0007917462\n",
      "968 0.0007922303\n",
      "969 0.0007919018\n",
      "970 0.0007910725\n",
      "971 0.0007906553\n",
      "972 0.0007908592\n",
      "973 0.0007910774\n",
      "974 0.0007908158\n",
      "975 0.0007902657\n",
      "976 0.0007899386\n",
      "977 0.0007899798\n",
      "978 0.00079007354\n",
      "979 0.00078991265\n",
      "980 0.0007895428\n",
      "981 0.0007892425\n",
      "982 0.0007891564\n",
      "983 0.0007891688\n",
      "984 0.00078908325\n",
      "985 0.0007888482\n",
      "986 0.0007885813\n",
      "987 0.0007884103\n",
      "988 0.00078834046\n",
      "989 0.00078827824\n",
      "990 0.00078814186\n",
      "991 0.0007879394\n",
      "992 0.00078773976\n",
      "993 0.00078759616\n",
      "994 0.0007875004\n",
      "995 0.00078740536\n",
      "996 0.00078727317\n",
      "997 0.00078710646\n",
      "998 0.0007869364\n",
      "999 0.0007867911\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#47 time series rnn\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "timesteps=seq_length=7\n",
    "#open,high,low,close,volumn\n",
    "data_dim=5\n",
    "#close\n",
    "output_dim=1\n",
    "xy=np.loadtxt('data-02-stock_daily.csv.txt',delimiter=',')\n",
    "#시간순으로 만들기위함\n",
    "xy=xy[::-1]\n",
    "xy=MinMaxScaler(xy)\n",
    "x=xy\n",
    "y=xy[:,[-1]]\n",
    "\n",
    "dataX=[]\n",
    "dataY=[]\n",
    "for i in range(len(y)-seq_length):\n",
    "    #이전 7일간의 데이터를 가지고 다음 날 코스닥시장 종가 예측\n",
    "    _x=x[i:i+seq_length]\n",
    "    _y=y[i+seq_length]\n",
    "    dataX.append(_x)\n",
    "    dataY.append(_y)\n",
    "    \n",
    "train_size=int(len(dataY)*0.7)\n",
    "test_size=len(dataY)-train_size\n",
    "trainX,testX=np.array(dataX[0:train_size]),np.array(dataX[train_size:len(dataX)])\n",
    "trainY,testY=np.array(dataY[0:train_size]),np.array(dataY[train_size:len(dataY)])\n",
    "\n",
    "X=tf.placeholder(tf.float32,[None,seq_length,data_dim])\n",
    "Y=tf.placeholder(tf.float32,[None,1])\n",
    "\n",
    "cell=tf.contrib.rnn.BasicLSTMCell(num_units=10,state_is_tuple=True)\n",
    "outputs,_states=tf.nn.dynamic_rnn(cell,X,dtype=tf.float32)\n",
    "Y_pred=tf.contrib.layers.fully_connected(outputs[:,-1],output_dim,activation_fn=None)\n",
    "\n",
    "loss=tf.reduce_mean(tf.square(Y_pred-Y))\n",
    "optimizer=tf.train.AdamOptimizer(0.01).minimize(loss)\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(1000):\n",
    "    _,l=sess.run([optimizer,loss],feed_dict={X:trainX,Y:trainY})\n",
    "    print(i,l)\n",
    "testPredict=sess.run(Y_pred,feed_dict={X:testX})\n",
    "\n",
    "plt.plot(testY)\n",
    "plt.plot(testPredict)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
