{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7\n",
    "x_data=[1,2,3]\n",
    "y_data=[1,2,3]\n",
    "\n",
    "W=tf.Variable(tf.random_normal([1]),name='weight')\n",
    "X=tf.placeholder(tf.float32)\n",
    "Y=tf.placeholder(tf.float32)\n",
    "\n",
    "hypothesis=W*X\n",
    "\n",
    "cost=tf.reduce_mean(tf.square(hypothesis-Y))\n",
    "\n",
    "learning_rate=0.1\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train=optimizer.minimize(cost)\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(21):\n",
    "    sess.run(train, feed_dict={X:x_data,Y:y_data})\n",
    "    print(step,sess.run(cost,feed_dict={X:x_data,Y:y_data}),sess.run(W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9\n",
    "x_data=[[72.,80.,75.],[93.,88.,93.],[89.,91.,90.],[96.,98.,100.],[73.,66.,70.]]\n",
    "y_data=[[152.],[185.],[180.],[196.],[142.]]\n",
    "\n",
    "W=tf.Variable(tf.random_normal([3,1]),name='weight')\n",
    "b=tf.Variable(tf.random_normal([1]),name='bias')\n",
    "X=tf.placeholder(tf.float32,shape=[None,3])\n",
    "Y=tf.placeholder(tf.float32,shape=[None,1])\n",
    "#지금은 5개이지만 데이터 추가가 용이하도록 None으로 설정\n",
    "hypothesis=tf.matmul(X,W)+b\n",
    "\n",
    "cost=tf.reduce_mean(tf.square(hypothesis-Y))\n",
    "\n",
    "learning_rate=1e-5\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train=optimizer.minimize(cost)\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(2001):\n",
    "    cost_val,hy_val,_=sess.run([cost,hypothesis,train], feed_dict={X:x_data,Y:y_data})\n",
    "    if step%10==0:\n",
    "        print(step,cost_val, hy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#13 당뇨병, 로지스틱 분류 이용(시그모이드, 바이너리)\n",
    "xy=np.loadtxt(\"data-03-diabetes.csv.txt\",delimiter=',',dtype=np.float32)\n",
    "x_data=xy[:,0:-1]\n",
    "y_data=xy[:,[-1]]\n",
    "\n",
    "X=tf.placeholder(tf.float32,shape=[None,8])\n",
    "Y=tf.placeholder(tf.float32,shape=[None,1])\n",
    "W=tf.Variable(tf.random_normal([8,1]))\n",
    "B=tf.Variable(tf.random_normal([1]))\n",
    "\n",
    "hypothesis=tf.sigmoid(tf.matmul(X,W)+B)\n",
    "cost=-tf.reduce_mean(Y*tf.log(hypothesis)+(1-Y)*tf.log(1-hypothesis))\n",
    "\n",
    "train=tf.train.GradientDescentOptimizer(0.1).minimize(cost)\n",
    "\n",
    "predict=tf.cast(hypothesis>0.5,dtype=tf.float32)\n",
    "accuracy=tf.reduce_mean(tf.cast(tf.equal(predict,Y),dtype=tf.float32))\n",
    "\n",
    "with tf.Session()as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    feed={X:x_data,Y:y_data}\n",
    "    for step in range(10001):\n",
    "        sess.run(train,feed_dict=feed)\n",
    "        if step%200==0:\n",
    "            print(step, sess.run(cost,feed_dict=feed))\n",
    "    h,c,a=sess.run([hypothesis,cost,accuracy],feed_dict=feed)\n",
    "    print(\"\\nhypothesis=\",h,\"\\ncost=\",c,\"\\naccuracy=\",a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#17 softmax 이용\n",
    "xy=np.loadtxt(\"data-04-zoo.csv.txt\",delimiter=',',dtype=np.float32)\n",
    "x_data=xy[:,0:-1]\n",
    "y_data=xy[:,[-1]]\n",
    "\n",
    "nb_classes=7\n",
    "\n",
    "X=tf.placeholder(tf.float32,shape=[None,16])\n",
    "Y=tf.placeholder(tf.int32,shape=[None,1])\n",
    "W=tf.Variable(tf.random_normal([16,nb_classes]))\n",
    "B=tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "Y_one_hot=tf.one_hot(Y,nb_classes)\n",
    "Y_one_hot=tf.reshape(Y_one_hot,[-1,nb_classes])\n",
    "\n",
    "logits=tf.matmul(X,W)+B\n",
    "hypothesis=tf.nn.softmax(logits)\n",
    "\n",
    "cost_i=tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=Y_one_hot)\n",
    "cost=tf.reduce_mean(cost_i)\n",
    "train=tf.train.GradientDescentOptimizer(0.1).minimize(cost)\n",
    "\n",
    "prediction=tf.argmax(hypothesis,1)\n",
    "correct_prediction=tf.equal(prediction,tf.argmax(Y_one_hot,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    feed={X:x_data,Y:y_data}\n",
    "    for step in range(2001):\n",
    "        sess.run(train,feed_dict=feed)\n",
    "        if step%200==0:\n",
    "            loss,acc=sess.run([cost,accuracy],feed_dict=feed)\n",
    "            print('step={:5}\\tloss={:.3f}\\taccuracy={:.2%}'.format(step,loss,acc))\n",
    "    pred=sess.run(prediction,feed_dict=feed)\n",
    "    for p,y in zip(pred,y_data.flatten()):\n",
    "        print(\"[{}] prediction={} Y={}\".format(p==int(y),p,int(y)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#20 normalization \n",
    "#[1,2,10000,3,4]->[0.001,0.002,0.5,0.003,0.004]\n",
    "#input 중 outlier를 다른 데이터와 균일하게 해야함\n",
    "#xy=minmaxscaler(xy) 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import input_data\n",
    "\n",
    "mnist=input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n",
    "\n",
    "nb_classes=10\n",
    "\n",
    "X=tf.placeholder(tf.float32,[None,784])\n",
    "Y=tf.placeholder(tf.float32,[None,10])\n",
    "W=tf.Variable(tf.random_normal([784,10]))\n",
    "B=tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "\n",
    "logits=tf.matmul(X,W)+B\n",
    "hypothesis=tf.nn.softmax(logits)\n",
    "\n",
    "cost_i=tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=Y)\n",
    "cost=tf.reduce_mean(cost_i)\n",
    "\n",
    "train=tf.train.GradientDescentOptimizer(0.1).minimize(cost)\n",
    "\n",
    "is_correct=tf.equal(tf.argmax(hypothesis,1),tf.argmax(Y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(is_correct,tf.float32))\n",
    "\n",
    "training_epochs=15\n",
    "batch_size=100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost=0\n",
    "        total_batch=int(mnist.train.num_examples/batch_size) \n",
    "        for i in range(total_batch):\n",
    "            batch_xs,batch_ys=mnist.train.next_batch(batch_size)\n",
    "            c,_=sess.run([cost,train],feed_dict={X:batch_xs,Y:batch_ys})\n",
    "            avg_cost+=c/total_batch\n",
    "        print('Epoch:','%04d'%(epoch+1),'cost=','{:.9f}'.format(avg_cost))\n",
    "    print(\"Accuracy: \",accuracy.eval(session=sess,feed_dict={X:mnist.test.images,Y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#24 tf 함수실습\n",
    "sess=tf.Session()\n",
    "x=[1,2,3]\n",
    "y=[4,5,6]\n",
    "z=[7,8,9]\n",
    "tf.stack([x,y,z],axis=-1).eval(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#28 xor해결하기\n",
    "#2개의 깊이, 2개의 너비\n",
    "x_data=np.array([[0,0],[0,1],[1,0],[1,1]],dtype=np.float32)\n",
    "y_data=np.array([[0],[1],[1],[0]],dtype=np.float32)\n",
    "\n",
    "X=tf.placeholder(tf.float32,[4,2])\n",
    "Y=tf.placeholder(tf.float32,[4,1])\n",
    "\n",
    "#2,2에서 두번째 2가 해당 층의 너비\n",
    "#layer의 개수가 모형의 깊이\n",
    "W1=tf.Variable(tf.random_normal([2,2]))\n",
    "B1=tf.Variable(tf.random_normal([2]))\n",
    "layer1=tf.sigmoid(tf.matmul(X,W1)+B1)\n",
    "\n",
    "W2=tf.Variable(tf.random_normal([2,1]))\n",
    "B2=tf.Variable(tf.random_normal([1]))\n",
    "hypothesis=tf.sigmoid(tf.matmul(layer1,W2)+B2)\n",
    "\n",
    "cost=-tf.reduce_mean(Y*tf.log(hypothesis)+(1-Y)*tf.log(1-hypothesis))\n",
    "train=tf.train.GradientDescentOptimizer(0.1).minimize(cost)\n",
    "\n",
    "predicted=tf.cast(hypothesis>0.5,dtype=tf.float32)\n",
    "accuracy=tf.reduce_mean(tf.cast(tf.equal(predicted,Y),tf.float32))\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        sess.run(train,feed_dict={X:x_data,Y:y_data})\n",
    "        if step%1000==0:\n",
    "            print(step,sess.run(cost,feed_dict={X:x_data,Y:y_data}),sess.run([W1,W2])) \n",
    "    h,c,a=sess.run([hypothesis,cost,accuracy],feed_dict={X:x_data,Y:y_data})\n",
    "    print('\\nhypothesis=',h,'\\ncorrect=',c,'\\naccuracy=',a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#34 정확도 높이기 mnist\n",
    "#경사하강->89.53 / 아담->91.88 / 초기값 문제 해결&l.r 0.01로 변경\n",
    "#깊이 너비 확장(3,256)->96.33 / 깊이 너비 확장(5,512)->97.25\n",
    "#더 나아졌지만 강의대로 dropout 사용->96.03?\n",
    "import input_data\n",
    "\n",
    "mnist=input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n",
    "\n",
    "nb_classes=10\n",
    "\n",
    "X=tf.placeholder(tf.float32,[None,784])\n",
    "Y=tf.placeholder(tf.float32,[None,10])\n",
    "keep_prob=tf.placeholder(tf.float32)\n",
    "\n",
    "W1=tf.get_variable(\"W1\",shape=[784,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "W2=tf.get_variable(\"W2\",shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "W3=tf.get_variable(\"W3\",shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "W4=tf.get_variable(\"W4\",shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "W5=tf.get_variable(\"W5\",shape=[512,10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "B1=tf.Variable(tf.random_normal([512]))\n",
    "B2=tf.Variable(tf.random_normal([512]))\n",
    "B3=tf.Variable(tf.random_normal([512]))\n",
    "B4=tf.Variable(tf.random_normal([512]))\n",
    "B5=tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "L1=tf.nn.relu(tf.matmul(X,W1)+B1)\n",
    "L1=tf.nn.dropout(L1,keep_prob=keep_prob)\n",
    "L2=tf.nn.relu(tf.matmul(L1,W2)+B2)\n",
    "L2=tf.nn.dropout(L2,keep_prob=keep_prob)\n",
    "L3=tf.nn.relu(tf.matmul(L2,W3)+B3)\n",
    "L4=tf.nn.relu(tf.matmul(L3,W4)+B4)\n",
    "\n",
    "logits=tf.matmul(L4,W5)+B5\n",
    "hypothesis=tf.nn.softmax(logits)\n",
    "\n",
    "cost_i=tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=Y)\n",
    "cost=tf.reduce_mean(cost_i)\n",
    "\n",
    "train=tf.train.AdamOptimizer(0.01).minimize(cost)\n",
    "\n",
    "is_correct=tf.equal(tf.argmax(hypothesis,1),tf.argmax(Y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(is_correct,tf.float32))\n",
    "\n",
    "training_epochs=15\n",
    "batch_size=100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost=0\n",
    "        total_batch=int(mnist.train.num_examples/batch_size) \n",
    "        for i in range(total_batch):\n",
    "            batch_xs,batch_ys=mnist.train.next_batch(batch_size)\n",
    "            c,_=sess.run([cost,train],feed_dict={X:batch_xs,Y:batch_ys,keep_prob:0.7})\n",
    "            avg_cost+=c/total_batch\n",
    "        print('Epoch:','%04d'%(epoch+1),'cost=','{:.9f}'.format(avg_cost))\n",
    "    print(\"Accuracy: \",accuracy.eval(session=sess,feed_dict={X:mnist.test.images,Y:mnist.test.labels,keep_prob:1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#34 정확도 높이기 iris 혼자실습\n",
    "#iris-setosa 1 / iris-versicolor 2 / iris-virginica 3\n",
    "#정확도::경사하강->97.3 / 아담->98.6 / 깊이와 너비 증가->1.0\n",
    "#but this program does'nt seperate test and train data\n",
    "xy=np.loadtxt(\"iris.csv.txt\",delimiter=',',dtype=np.string_)\n",
    "for i in range(len(xy)):\n",
    "    for j in range(len(xy[i])):\n",
    "        if xy[i][j]==b'Iris-setosa':\n",
    "            xy[i][j]='0'\n",
    "        elif xy[i][j]==b'Iris-versicolor':\n",
    "            xy[i][j]='1'\n",
    "        elif xy[i][j]==b'Iris-virginica':\n",
    "            xy[i][j]='2'\n",
    "xy1=xy.astype(np.float32)\n",
    "x_data=xy1[:,0:-1]\n",
    "y_data=xy1[:,[-1]]\n",
    "X=tf.placeholder(tf.float32,shape=[None,4])\n",
    "Y=tf.placeholder(tf.int32,shape=[None,1])\n",
    "W1=tf.Variable(tf.random_normal([4,100]))\n",
    "W2=tf.Variable(tf.random_normal([100,100]))\n",
    "W3=tf.Variable(tf.random_normal([100,3]))\n",
    "B1=tf.Variable(tf.random_normal([100]))\n",
    "B2=tf.Variable(tf.random_normal([100]))\n",
    "B3=tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "Y_one_hot=tf.one_hot(Y,3)\n",
    "Y_one_hot=tf.reshape(Y_one_hot,[-1,3])\n",
    "\n",
    "layer1=tf.nn.relu(tf.matmul(X,W1)+B1)\n",
    "layer2=tf.nn.relu(tf.matmul(layer1,W2)+B2)\n",
    "logits=tf.matmul(layer2,W3)+B3\n",
    "hypothesis=tf.nn.softmax(tf.matmul(layer2,W3)+B3)\n",
    "cost_i=tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=Y_one_hot)\n",
    "cost=tf.reduce_mean(cost_i)\n",
    "train=tf.train.AdamOptimizer(0.1).minimize(cost)\n",
    "\n",
    "prediction=tf.argmax(hypothesis,1)\n",
    "correct_prediction=tf.equal(prediction,tf.argmax(Y_one_hot,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    feed={X:x_data,Y:y_data}\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(5000):\n",
    "        sess.run(train,feed_dict=feed)\n",
    "        if step%500==0:\n",
    "            loss,acc=sess.run([cost,accuracy],feed_dict=feed)\n",
    "            print('step=',step,'\\tloss=',loss,'\\taccuracy=',acc)\n",
    "    pred=sess.run(prediction,feed_dict=feed)\n",
    "    for p,y in zip(pred,y_data.flatten()):\n",
    "        print('[{}] Prediction: {} True Y: {}'.format(p==int(y),p,int(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#39 mnist cnn적용 97.98\n",
    "import input_data\n",
    "\n",
    "mnist=input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n",
    "\n",
    "X=tf.placeholder(tf.float32,[None,784])\n",
    "X_img=tf.reshape(X,[-1,28,28,1])\n",
    "Y=tf.placeholder(tf.float32,[None,10])\n",
    "\n",
    "W1=tf.Variable(tf.random_normal([3,3,1,32]),tf.float32)\n",
    "L1=tf.nn.conv2d(X_img,W1,strides=[1,1,1,1],padding='SAME')\n",
    "L1=tf.nn.relu(L1)\n",
    "#maxpool strides=2 => 14,14,1\n",
    "L1=tf.nn.max_pool(L1,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "\n",
    "W2=tf.Variable(tf.random_normal([3,3,32,64]),tf.float32)\n",
    "L2=tf.nn.conv2d(L1,W2,strides=[1,1,1,1],padding='SAME')\n",
    "L2=tf.nn.relu(L2)\n",
    "L2=tf.nn.max_pool(L2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "L2=tf.reshape(L2,[-1,7*7*64])\n",
    "\n",
    "W3=tf.get_variable(\"W3\",shape=[7*7*64,10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "B=tf.Variable(tf.random_uniform([10]))\n",
    "\n",
    "logits=tf.matmul(L2,W3)+B\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=Y))\n",
    "optimizer=tf.train.AdamOptimizer(0.01).minimize(cost)\n",
    "\n",
    "training_epoch=15\n",
    "batch_size=100\n",
    "\n",
    "correct_prediction=tf.equal(tf.argmax(logits,1),tf.argmax(Y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(training_epoch):\n",
    "        avg_cost=0\n",
    "        total_batch=int(mnist.train.num_examples/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_xs,batch_ys=mnist.train.next_batch(batch_size)\n",
    "            c,_=sess.run([cost,optimizer],feed_dict={X:batch_xs,Y:batch_ys})\n",
    "            avg_cost+=c/total_batch\n",
    "        print(\"epoch: %04d\" %(epoch+1),\"cost: {:.9f}\".format(avg_cost))\n",
    "    print(\"Accuracy:\",sess.run(accuracy,feed_dict={X:mnist.test.images,Y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#40 mnist cnn 적용2 class로 정리\n",
    "#깊이 너비 증가(cnn깊이 2->3,행렬계산깊이1->2 너비625)->99.28%\n",
    "#앙상블 포함->\n",
    "class Model():\n",
    "    def __init__(self,sess,name):\n",
    "        self.sess=sess\n",
    "        self.name=name\n",
    "        self._build_net()\n",
    "    \n",
    "    def _build_net(self):\n",
    "        with tf.variable_scope(self.name):\n",
    "            self.X=tf.placeholder(tf.float32,[None,784])\n",
    "            X_img=tf.reshape(self.X,[-1,28,28,1])\n",
    "            self.Y=tf.placeholder(tf.float32,[None,10])\n",
    "            \n",
    "            self.keep_prob=tf.placeholder(tf.float32)\n",
    "\n",
    "            W1=tf.Variable(tf.random_normal([3,3,1,32],stddev=0.01))\n",
    "            L1=tf.nn.conv2d(X_img,W1,strides=[1,1,1,1],padding='SAME')\n",
    "            L1=tf.nn.relu(L1)\n",
    "            #maxpool strides=2 => 14,14,1\n",
    "            L1=tf.nn.max_pool(L1,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "            L1=tf.nn.dropout(L1,keep_prob=self.keep_prob)\n",
    "\n",
    "            W2=tf.Variable(tf.random_normal([3,3,32,64],stddev=0.01))\n",
    "            L2=tf.nn.conv2d(L1,W2,strides=[1,1,1,1],padding='SAME')\n",
    "            L2=tf.nn.relu(L2)\n",
    "            L2=tf.nn.max_pool(L2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "            L2=tf.nn.dropout(L2,keep_prob=self.keep_prob)\n",
    "            \n",
    "            W3=tf.Variable(tf.random_normal([3,3,64,128],stddev=0.01))\n",
    "            L3=tf.nn.conv2d(L2,W3,strides=[1,1,1,1],padding='SAME')\n",
    "            L3=tf.nn.relu(L3)\n",
    "            L3=tf.nn.max_pool(L3,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "            L3=tf.nn.dropout(L3,keep_prob=self.keep_prob)\n",
    "            L3=tf.reshape(L3,[-1,4*4*128])\n",
    "\n",
    "            W4=tf.get_variable(\"W4\",shape=[4*4*128,625],initializer=tf.contrib.layers.xavier_initializer())\n",
    "            B1=tf.Variable(tf.random_uniform([625]))\n",
    "            L4=tf.nn.relu(tf.matmul(L3,W4)+B1)\n",
    "            L4=tf.nn.dropout(L4,keep_prob=self.keep_prob)\n",
    "            \n",
    "            W5=tf.get_variable(\"W5\",shape=[625,10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "            B2=tf.Variable(tf.random_normal([10]))\n",
    "            \n",
    "            self.logits=tf.matmul(L4,W5)+B2\n",
    "            self.cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.logits,labels=self.Y))\n",
    "            self.optimizer=tf.train.AdamOptimizer(0.001).minimize(self.cost)\n",
    "\n",
    "            correct_prediction=tf.equal(tf.argmax(self.logits,1),tf.argmax(self.Y,1))\n",
    "            self.accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "            \n",
    "    def predict(self,x_test,keep_prob=1.0):\n",
    "        return self.sess.run(self.logits,feed_dict={self.X:x_test,self.keep_prob:keep_prob})\n",
    "    \n",
    "    def get_accuracy(self,x_test,y_test,keep_prob=1.0):\n",
    "        return self.sess.run(self.accuracy,feed_dict={self.X:x_test,self.Y:y_test,self.keep_prob:keep_prob})\n",
    "    def train(self,x_data,y_data,keep_prob=0.7):\n",
    "        return self.sess.run([self.cost,self.optimizer],feed_dict={self.X:x_data,self.Y:y_data,self.keep_prob:keep_prob})\n",
    "\n",
    "    \n",
    "import input_data\n",
    "\n",
    "mnist=input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n",
    "\n",
    "sess=tf.Session()\n",
    "m=[]\n",
    "for i in range(7):\n",
    "    m.append(Model(sess,\"model\"+str(i)))\n",
    "\n",
    "training_epoch=15\n",
    "batch_size=100\n",
    "prediction=np.zeros(len(mnist.test.labels)*10).reshape(len(mnist.test.labels),10)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(training_epoch):\n",
    "    avg_cost_list=np.zeros(len(m))\n",
    "    total_batch=int(mnist.train.num_examples/batch_size)\n",
    "    for i in range(total_batch):\n",
    "        batch_xs,batch_ys=mnist.train.next_batch(batch_size)\n",
    "        for m_idx,mm in enumerate(m):\n",
    "            c,_=mm.train(batch_xs,batch_ys)\n",
    "            avg_cost_list[m_idx]+=c/total_batch\n",
    "    print(\"epoch: %04d\" %(epoch+1),\"cost: \",avg_cost_list)\n",
    "for m_idx,mm in enumerate(m):\n",
    "    print(\"Accuracy:\",mm.get_accuracy(mnist.test.images,mnist.test.labels))\n",
    "    p=mm.predict(mnist.test.images)\n",
    "    prediction+=p\n",
    "ensemble_correct_prediction=tf.equal(tf.argmax(prediction,1),tf.argmax(mnist.test.labels,1))\n",
    "ensemble_accuracy=tf.reduce_mean(tf.cast(ensemble_correct_prediction,tf.float32))\n",
    "print('ensemble accuracy:',sess.run(ensemble_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cifar10 실습\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "train_x_list=[]\n",
    "train_y_list=[]\n",
    "for i in range(1,6):\n",
    "    a=unpickle(\"cifar-10-batches-py/data_batch_\"+str(i))\n",
    "    train_x_list.append(np.array(a[b'data']))\n",
    "    train_y_list.append(np.array(a[b'labels']).reshape(-1,1))\n",
    "test=unpickle(\"cifar-10-batches-py/test_batch\")\n",
    "test_x=np.array(test[b'data'])\n",
    "#test_x=np.array(train_x_list).flatten('C').reshape(-1,3072).shape\n",
    "test_y=np.array(test[b'labels']).reshape(-1,1)\n",
    "\n",
    "keep_prob=tf.placeholder(tf.float32)\n",
    "\n",
    "X=tf.placeholder(tf.float32,[None,3072])\n",
    "X_img=tf.reshape(X,[-1,32,32,3])\n",
    "Y=tf.placeholder(tf.int32,[None,1])\n",
    "Y_one_hot=tf.one_hot(Y,10)\n",
    "Y_one_hot=tf.reshape(Y_one_hot,[-1,10])\n",
    "W1=tf.Variable(tf.random_normal([3,3,3,32],stddev=0.01))\n",
    "L1=tf.nn.conv2d(X_img,W1,strides=[1,1,1,1],padding='SAME')\n",
    "L1=tf.nn.relu(L1)\n",
    "L1=tf.nn.max_pool(L1,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "L1=tf.nn.dropout(L1,keep_prob=keep_prob)\n",
    "L1=tf.reshape(L1,[-1,16*16*32])\n",
    "W2=tf.get_variable(\"W2\",shape=[16*16*32,10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "B=tf.Variable(tf.random_normal([1]))\n",
    "L2=tf.matmul(L1,W2)+B\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=L2,labels=Y_one_hot))\n",
    "optimizer=tf.train.AdamOptimizer(0.00001).minimize(cost)\n",
    "\n",
    "is_correct=tf.equal(tf.argmax(L1,1),tf.argmax(Y_one_hot,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(is_correct,tf.float32))\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(15):\n",
    "        avg_cost=0\n",
    "        for m,n in zip(train_x_list,train_y_list):\n",
    "            c,_=sess.run([cost,optimizer],feed_dict={X:m,Y:n,keep_prob:0.7})\n",
    "            avg_cost+=c/5\n",
    "        print(\"cost: \",avg_cost)\n",
    "    a=sess.run(accuracy,feed_dict={X:test_x,Y:test_y,keep_prob:1})\n",
    "    print('accuracy=',a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cifar10 출처: http://solarisailab.com/archives/2325\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "CIFAR-10 Convolutional Neural Networks(CNN) Example\n",
    "next_batch function is copied from edo's answer\n",
    "https://stackoverflow.com/questions/40994583/how-to-implement-tensorflows-next-batch-for-own-data\n",
    "Author : solaris33\n",
    "Project URL : http://solarisailab.com/archives/2325\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# CIFAR-10 데이터를 다운로드 받기 위한 keras의 helper 함수인 load_data 함수를 임포트합니다.\n",
    "from tensorflow.keras.datasets.cifar10 import load_data\n",
    "\n",
    "# 다음 배치를 읽어오기 위한 next_batch 유틸리티 함수를 정의합니다.\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    `num` 개수 만큼의 랜덤한 샘플들과 레이블들을 리턴합니다.\n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n",
    "\n",
    "# CNN 모델을 정의합니다. \n",
    "def build_CNN_classifier(x):\n",
    "  # 입력 이미지\n",
    "    x_image = x\n",
    "\n",
    "    # 첫번째 convolutional layer - 하나의 grayscale 이미지를 64개의 특징들(feature)으로 맵핑(maping)합니다.\n",
    "    W_conv1 = tf.Variable(tf.truncated_normal(shape=[5, 5, 3, 64], stddev=5e-2))\n",
    "    b_conv1 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "    h_conv1 = tf.nn.relu(tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1)\n",
    "\n",
    "    # 첫번째 Pooling layer\n",
    "    h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    # 두번째 convolutional layer - 32개의 특징들(feature)을 64개의 특징들(feature)로 맵핑(maping)합니다.\n",
    "    W_conv2 = tf.Variable(tf.truncated_normal(shape=[5, 5, 64, 64], stddev=5e-2))\n",
    "    b_conv2 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "    h_conv2 = tf.nn.relu(tf.nn.conv2d(h_pool1, W_conv2, strides=[1, 1, 1, 1], padding='SAME') + b_conv2)\n",
    "\n",
    "    # 두번째 pooling layer.\n",
    "    h_pool2 = tf.nn.max_pool(h_conv2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    # 세번째 convolutional layer\n",
    "    W_conv3 = tf.Variable(tf.truncated_normal(shape=[3, 3, 64, 128], stddev=5e-2))\n",
    "    b_conv3 = tf.Variable(tf.constant(0.1, shape=[128]))\n",
    "    h_conv3 = tf.nn.relu(tf.nn.conv2d(h_pool2, W_conv3, strides=[1, 1, 1, 1], padding='SAME') + b_conv3)\n",
    "\n",
    "    # 네번째 convolutional layer\n",
    "    W_conv4 = tf.Variable(tf.truncated_normal(shape=[3, 3, 128, 128], stddev=5e-2))\n",
    "    b_conv4 = tf.Variable(tf.constant(0.1, shape=[128])) \n",
    "    h_conv4 = tf.nn.relu(tf.nn.conv2d(h_conv3, W_conv4, strides=[1, 1, 1, 1], padding='SAME') + b_conv4)\n",
    "\n",
    "    # 다섯번째 convolutional layer\n",
    "    W_conv5 = tf.Variable(tf.truncated_normal(shape=[3, 3, 128, 128], stddev=5e-2))\n",
    "    b_conv5 = tf.Variable(tf.constant(0.1, shape=[128]))\n",
    "    h_conv5 = tf.nn.relu(tf.nn.conv2d(h_conv4, W_conv5, strides=[1, 1, 1, 1], padding='SAME') + b_conv5)\n",
    "\n",
    "    # Fully Connected Layer 1 - 2번의 downsampling 이후에, 우리의 32x32 이미지는 8x8x128 특징맵(feature map)이 됩니다.\n",
    "    # 이를 384개의 특징들로 맵핑(maping)합니다.\n",
    "    W_fc1 = tf.Variable(tf.truncated_normal(shape=[8 * 8 * 128, 384], stddev=5e-2))\n",
    "    b_fc1 = tf.Variable(tf.constant(0.1, shape=[384]))\n",
    "\n",
    "    h_conv5_flat = tf.reshape(h_conv5, [-1, 8*8*128])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_conv5_flat, W_fc1) + b_fc1)\n",
    "\n",
    "    # Dropout - 모델의 복잡도를 컨트롤합니다. 특징들의 co-adaptation을 방지합니다.\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob) \n",
    "\n",
    "    # Fully Connected Layer 2 - 384개의 특징들(feature)을 10개의 클래스-airplane, automobile, bird...-로 맵핑(maping)합니다.\n",
    "    W_fc2 = tf.Variable(tf.truncated_normal(shape=[384, 10], stddev=5e-2))\n",
    "    b_fc2 = tf.Variable(tf.constant(0.1, shape=[10]))\n",
    "    logits = tf.matmul(h_fc1_drop,W_fc2) + b_fc2\n",
    "    y_pred = tf.nn.softmax(logits)\n",
    "\n",
    "    return y_pred, logits\n",
    "\n",
    "# 인풋 아웃풋 데이터, 드롭아웃 확률을 입력받기위한 플레이스홀더를 정의합니다.\n",
    "x = tf.placeholder(tf.float32, shape=[None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# CIFAR-10 데이터를 다운로드하고 데이터를 불러옵니다.\n",
    "(x_train, y_train), (x_test, y_test) = load_data()\n",
    "# scalar 형태의 레이블(0~9)을 One-hot Encoding 형태로 변환합니다.\n",
    "y_train_one_hot = tf.squeeze(tf.one_hot(y_train, 10),axis=1)\n",
    "y_test_one_hot = tf.squeeze(tf.one_hot(y_test, 10),axis=1)\n",
    "\n",
    "# Convolutional Neural Networks(CNN) 그래프를 생성합니다.\n",
    "y_pred, logits = build_CNN_classifier(x)\n",
    "\n",
    "# Cross Entropy를 비용함수(loss function)으로 정의하고, RMSPropOptimizer를 이용해서 비용 함수를 최소화합니다.\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "train_step = tf.train.RMSPropOptimizer(1e-3).minimize(loss)\n",
    "\n",
    "# 정확도를 계산하는 연산을 추가합니다.\n",
    "correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# 세션을 열어 실제 학습을 진행합니다.\n",
    "with tf.Session() as sess:\n",
    "  # 모든 변수들을 초기화한다. \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "  \n",
    "    # 10000 Step만큼 최적화를 수행합니다.\n",
    "    for i in range(10000):\n",
    "        batch = next_batch(128, x_train, y_train_one_hot.eval())\n",
    "\n",
    "    # 100 Step마다 training 데이터셋에 대한 정확도와 loss를 출력합니다.\n",
    "        if i % 100 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={x: batch[0], y: batch[1], keep_prob: 1.0})\n",
    "            loss_print = loss.eval(feed_dict={x: batch[0], y: batch[1], keep_prob: 1.0})\n",
    "\n",
    "            print(\"반복(Epoch): %d, 트레이닝 데이터 정확도: %f, 손실 함수(loss): %f\" % (i, train_accuracy, loss_print))\n",
    "    # 20% 확률의 Dropout을 이용해서 학습을 진행합니다.\n",
    "        sess.run(train_step, feed_dict={x: batch[0], y: batch[1], keep_prob: 0.8})\n",
    "\n",
    "  # 학습이 끝나면 테스트 데이터(10000개)에 대한 정확도를 출력합니다.  \n",
    "    test_accuracy = 0.0  \n",
    "    for i in range(10):\n",
    "        test_batch = next_batch(1000, x_test, y_test_one_hot.eval())\n",
    "        test_accuracy = test_accuracy + accuracy.eval(feed_dict={x: test_batch[0], y: test_batch[1], keep_prob: 1.0})\n",
    "    test_accuracy = test_accuracy / 10;\n",
    "    print(\"테스트 데이터 정확도: %f\" % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#43 RNN basic\n",
    "input_dim=5\n",
    "hidden_size=5\n",
    "sequence_length=6\n",
    "batch_size=1\n",
    "\n",
    "idx2char=['h','i','e','l','o']\n",
    "x_data=[[0,1,0,2,3,3]]\n",
    "x_one_hot=[[[1,0,0,0,0],[0,1,0,0,0],[1,0,0,0,0],[0,0,1,0,0],[0,0,0,1,0],[0,0,0,1,0]]]\n",
    "y_data=[[1,0,2,3,3,4]]\n",
    "\n",
    "X=tf.placeholder(tf.float32,[None,sequence_length,input_dim])\n",
    "Y=tf.placeholder(tf.int32,[None,sequence_length])\n",
    "\n",
    "cell=tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size,state_is_tuple=True)\n",
    "initial_state=cell.zero_state(batch_size,tf.float32)\n",
    "#단순화를 위해 rnn에서 나온 output을 바로 sequence_loss에 사용(잘못된 것임)\n",
    "outputs,_state=tf.nn.dynamic_rnn(cell,X,initial_state=initial_state,dtype=tf.float32)\n",
    "weights=tf.ones([batch_size,sequence_length])\n",
    "\n",
    "sequence_loss=tf.contrib.seq2seq.sequence_loss(logits=outputs,targets=Y,weights=weights)\n",
    "loss=tf.reduce_mean(sequence_loss)\n",
    "train=tf.train.AdamOptimizer(0.1).minimize(loss)\n",
    "\n",
    "prediction=tf.argmax(outputs,axis=2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(2000):\n",
    "        l,_=sess.run([loss,train],feed_dict={X:x_one_hot,Y:y_data})\n",
    "        result=sess.run(prediction,feed_dict={X:x_one_hot})\n",
    "        print(i,\"loss:\",l,\"prediction: \",result,\"true Y:\",y_data)\n",
    "        result_str=[idx2char[c] for c in np.squeeze(result)]\n",
    "        print(\"\\tPrediction str: \",''.join(result_str))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#44 rnn long sequence\n",
    "sample='if you want you'\n",
    "\n",
    "idx2char=list(set(sample))\n",
    "char2idx={c: i for i,c in enumerate(idx2char)}\n",
    "sample_idx=[char2idx[c] for c in sample]\n",
    "x_data=[sample_idx[:-1]]\n",
    "y_data=[sample_idx[1:]]\n",
    "\n",
    "sequence_length=len(sample)-1\n",
    "num_classes=len(idx2char)\n",
    "dic_size=len(char2idx)\n",
    "rnn_hidden_size=len(char2idx)\n",
    "batch_size=1\n",
    "\n",
    "X=tf.placeholder(tf.int32,[None,sequence_length])\n",
    "Y=tf.placeholder(tf.int32,[None, sequence_length])\n",
    "\n",
    "x_one_hot=tf.one_hot(X,num_classes)\n",
    "\n",
    "cell=tf.contrib.rnn.BasicLSTMCell(num_units=rnn_hidden_size,state_is_tuple=True)\n",
    "initial_state=cell.zero_state(batch_size,tf.float32)\n",
    "outputs,_state=tf.nn.dynamic_rnn(cell,x_one_hot,initial_state=initial_state,dtype=tf.float32)\n",
    "\n",
    "weights=tf.ones([batch_size,sequence_length])\n",
    "sequence_loss=tf.contrib.seq2seq.sequence_loss(logits=outputs,targets=Y,weights=weights)\n",
    "loss=tf.reduce_mean(sequence_loss)\n",
    "train=tf.train.AdamOptimizer(0.1).minimize(loss)\n",
    "\n",
    "prediction=tf.argmax(outputs,axis=2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(2000):\n",
    "        l,_=sess.run([loss,train],feed_dict={X:x_data,Y:y_data})\n",
    "        result=sess.run(prediction,feed_dict={X:x_data})\n",
    "        result_str=[idx2char[c] for c in np.squeeze(result)]\n",
    "        print(i,\"loss:\",l,\"prediction:\",''.join(result_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#45 rnn longlong sequence\n",
    "sentence=(\"if you want to build a ship, don't drum up people together to \"\n",
    "        \"collect wood and don't assign them tasks and work, but rether \"\n",
    "        \"teach them to long for the endless immensity of the sea.\")\n",
    "char_set=list(set(sentence))\n",
    "char_dic={w:i for i,w in enumerate(char_set)}\n",
    "\n",
    "data_dim=len(char_set)\n",
    "hidden_size=len(char_set)\n",
    "num_classes=len(char_set)\n",
    "seq_length=10\n",
    "\n",
    "dataX=[]\n",
    "dataY=[]\n",
    "for i in range(len(sentence)-seq_length):\n",
    "    x_str=sentence[i:i+seq_length]\n",
    "    y_str=sentence[i+1:i+seq_length+1]\n",
    "    \n",
    "    x=[char_dic[c] for c in x_str]\n",
    "    y=[char_dic[c] for c in y_str]\n",
    "    \n",
    "    dataX.append(x)\n",
    "    dataY.append(y)\n",
    "    \n",
    "batch_size=len(dataX)\n",
    "\n",
    "X=tf.placeholder(tf.int32,[None,seq_length])\n",
    "Y=tf.placeholder(tf.int32,[None,seq_length])\n",
    "\n",
    "X_one_hot=tf.one_hot(X,num_classes)\n",
    "\n",
    "cell=tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size,state_is_tuple=True)\n",
    "cell=tf.contrib.rnn.MultiRNNCell([cell]*2,state_is_tuple=True)\n",
    "initial_state=cell.zero_state(batch_size,tf.float32)\n",
    "outputs,_=tf.nn.dynamic_rnn(cell,X_one_hot,initial_state=initial_state,dtype=tf.float32)\n",
    "\n",
    "X_for_softmax=tf.reshape(outputs,[-1,hidden_size])\n",
    "softmax_w=tf.get_variable(\"softmax_w\",[hidden_size,num_classes])\n",
    "softmax_b=tf.get_variable(\"softmax_b\",[num_classes])\n",
    "outputs=tf.matmul(X_for_softmax,softmax_w)+softmax_b\n",
    "outputs=tf.reshape(outputs,[batch_size,seq_length,num_classes])\n",
    "\n",
    "weights=tf.ones([batch_size,seq_length])\n",
    "seq_loss=tf.contrib.seq2seq.sequence_loss(logits=outputs,targets=Y,weights=weights)\n",
    "loss=tf.reduce_mean(seq_loss)\n",
    "train=tf.train.AdamOptimizer(0.1).minimize(loss)\n",
    "\n",
    "prediction=tf.argmax(outputs,2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(500):\n",
    "        l,_,results=sess.run([loss,train,outputs],feed_dict={X:dataX,Y:dataY})\n",
    "        for j,result in enumerate(results):\n",
    "            index=np.argmax(result,1)\n",
    "            print(i,j,''.join([char_set[t] for t in index]),l)\n",
    "    results=sess.run(outputs,feed_dict={X:dataX})\n",
    "    for j,result in enumerate(results):\n",
    "        index=np.argmax(result,1)\n",
    "        if j is 0:\n",
    "            print(''.join([char_set[t] for t in index]),end='')\n",
    "        else:\n",
    "            print(char_set[index[-1]],end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-afafe39ece20>:37: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "0 0.014389117\n",
      "1 0.007681905\n",
      "2 0.003726041\n",
      "3 0.003043809\n",
      "4 0.0032517973\n",
      "5 0.0043229884\n",
      "6 0.004376528\n",
      "7 0.0039873873\n",
      "8 0.0033956883\n",
      "9 0.002662215\n",
      "10 0.0022739966\n",
      "11 0.002126639\n",
      "12 0.0020665436\n",
      "13 0.002172877\n",
      "14 0.002333539\n",
      "15 0.0023804079\n",
      "16 0.0023556962\n",
      "17 0.002305278\n",
      "18 0.0021790322\n",
      "19 0.0019985705\n",
      "20 0.001855673\n",
      "21 0.0017666422\n",
      "22 0.0016960575\n",
      "23 0.0016682756\n",
      "24 0.0016999645\n",
      "25 0.0017342665\n",
      "26 0.0017410787\n",
      "27 0.0017404943\n",
      "28 0.0017181071\n",
      "29 0.0016587886\n",
      "30 0.0015974094\n",
      "31 0.0015523656\n",
      "32 0.0015111793\n",
      "33 0.0014833902\n",
      "34 0.0014802722\n",
      "35 0.0014831986\n",
      "36 0.0014812017\n",
      "37 0.0014808569\n",
      "38 0.0014739278\n",
      "39 0.0014524012\n",
      "40 0.0014278644\n",
      "41 0.0014047321\n",
      "42 0.0013787972\n",
      "43 0.0013588986\n",
      "44 0.0013485348\n",
      "45 0.0013396661\n",
      "46 0.0013341431\n",
      "47 0.0013309763\n",
      "48 0.0013221662\n",
      "49 0.0013111798\n",
      "50 0.0012986542\n",
      "51 0.0012822296\n",
      "52 0.001268422\n",
      "53 0.0012568625\n",
      "54 0.0012468689\n",
      "55 0.0012410217\n",
      "56 0.0012347425\n",
      "57 0.0012285248\n",
      "58 0.0012218797\n",
      "59 0.0012126688\n",
      "60 0.0012038798\n",
      "61 0.001194194\n",
      "62 0.0011857635\n",
      "63 0.001178837\n",
      "64 0.0011726158\n",
      "65 0.00116786\n",
      "66 0.0011622896\n",
      "67 0.0011569532\n",
      "68 0.0011503542\n",
      "69 0.0011439136\n",
      "70 0.0011372225\n",
      "71 0.0011313254\n",
      "72 0.0011259663\n",
      "73 0.001121234\n",
      "74 0.0011166455\n",
      "75 0.0011119405\n",
      "76 0.0011069472\n",
      "77 0.0011017333\n",
      "78 0.0010965122\n",
      "79 0.0010915487\n",
      "80 0.0010868798\n",
      "81 0.0010825697\n",
      "82 0.0010782945\n",
      "83 0.0010740864\n",
      "84 0.0010696158\n",
      "85 0.0010651909\n",
      "86 0.0010606566\n",
      "87 0.0010563898\n",
      "88 0.001052204\n",
      "89 0.0010482395\n",
      "90 0.0010442962\n",
      "91 0.0010403361\n",
      "92 0.0010363603\n",
      "93 0.0010323314\n",
      "94 0.0010284191\n",
      "95 0.0010245517\n",
      "96 0.0010208342\n",
      "97 0.0010171721\n",
      "98 0.0010135318\n",
      "99 0.0010099161\n",
      "100 0.0010062755\n",
      "101 0.0010027008\n",
      "102 0.0009991741\n",
      "103 0.0009957253\n",
      "104 0.0009923581\n",
      "105 0.0009890122\n",
      "106 0.0009857094\n",
      "107 0.0009824253\n",
      "108 0.000979171\n",
      "109 0.00097598205\n",
      "110 0.00097284326\n",
      "111 0.0009697648\n",
      "112 0.0009667371\n",
      "113 0.0009637371\n",
      "114 0.000960777\n",
      "115 0.0009578575\n",
      "116 0.00095498096\n",
      "117 0.0009521626\n",
      "118 0.0009493947\n",
      "119 0.0009466679\n",
      "120 0.0009439854\n",
      "121 0.00094134104\n",
      "122 0.0009387363\n",
      "123 0.0009361802\n",
      "124 0.000933673\n",
      "125 0.00093121076\n",
      "126 0.0009287929\n",
      "127 0.00092641695\n",
      "128 0.0009240805\n",
      "129 0.0009217859\n",
      "130 0.00091953715\n",
      "131 0.0009173324\n",
      "132 0.00091516954\n",
      "133 0.00091304735\n",
      "134 0.0009109651\n",
      "135 0.0009089215\n",
      "136 0.0009069171\n",
      "137 0.00090495235\n",
      "138 0.0009030271\n",
      "139 0.0009011394\n",
      "140 0.00089928694\n",
      "141 0.00089746877\n",
      "142 0.0008956855\n",
      "143 0.00089393614\n",
      "144 0.00089222105\n",
      "145 0.0008905376\n",
      "146 0.0008888849\n",
      "147 0.0008872616\n",
      "148 0.00088566693\n",
      "149 0.0008841\n",
      "150 0.0008825616\n",
      "151 0.00088104897\n",
      "152 0.0008795618\n",
      "153 0.00087809854\n",
      "154 0.00087665854\n",
      "155 0.00087524124\n",
      "156 0.0008738456\n",
      "157 0.0008724706\n",
      "158 0.0008711156\n",
      "159 0.0008697803\n",
      "160 0.00086846366\n",
      "161 0.0008671698\n",
      "162 0.0008659109\n",
      "163 0.0008647553\n",
      "164 0.0008640161\n",
      "165 0.0008652882\n",
      "166 0.0008764843\n",
      "167 0.0009284462\n",
      "168 0.0010724512\n",
      "169 0.0011255518\n",
      "170 0.0009178162\n",
      "171 0.00090061256\n",
      "172 0.0010183018\n",
      "173 0.00087435846\n",
      "174 0.0009231755\n",
      "175 0.00093262317\n",
      "176 0.00085960556\n",
      "177 0.000934516\n",
      "178 0.0008567963\n",
      "179 0.0009050258\n",
      "180 0.00087130733\n",
      "181 0.0008760177\n",
      "182 0.0008806099\n",
      "183 0.00085868203\n",
      "184 0.0008814052\n",
      "185 0.00085073215\n",
      "186 0.0008769062\n",
      "187 0.0008481974\n",
      "188 0.00087007985\n",
      "189 0.00084804796\n",
      "190 0.00086279755\n",
      "191 0.0008486478\n",
      "192 0.00085611426\n",
      "193 0.00084923545\n",
      "194 0.00085041515\n",
      "195 0.0008492639\n",
      "196 0.000845833\n",
      "197 0.00084861263\n",
      "198 0.0008424462\n",
      "199 0.0008472425\n",
      "200 0.0008401092\n",
      "201 0.0008452015\n",
      "202 0.00083865534\n",
      "203 0.0008426535\n",
      "204 0.0008378239\n",
      "205 0.0008398386\n",
      "206 0.0008372928\n",
      "207 0.0008370783\n",
      "208 0.0008367086\n",
      "209 0.0008347003\n",
      "210 0.0008357353\n",
      "211 0.0008329128\n",
      "212 0.00083424273\n",
      "213 0.0008317416\n",
      "214 0.00083232764\n",
      "215 0.00083094276\n",
      "216 0.0008303224\n",
      "217 0.0008301121\n",
      "218 0.0008285995\n",
      "219 0.00082893425\n",
      "220 0.00082736666\n",
      "221 0.0008273621\n",
      "222 0.00082646613\n",
      "223 0.00082570495\n",
      "224 0.0008254989\n",
      "225 0.0008243299\n",
      "226 0.0008242068\n",
      "227 0.00082331325\n",
      "228 0.00082271884\n",
      "229 0.00082234305\n",
      "230 0.0008214123\n",
      "231 0.0008211114\n",
      "232 0.0008203934\n",
      "233 0.00081974594\n",
      "234 0.00081936\n",
      "235 0.0008185742\n",
      "236 0.0008181096\n",
      "237 0.00081757264\n",
      "238 0.0008168672\n",
      "239 0.0008164384\n",
      "240 0.0008158267\n",
      "241 0.00081520504\n",
      "242 0.00081475434\n",
      "243 0.0008141317\n",
      "244 0.00081355433\n",
      "245 0.00081307866\n",
      "246 0.0008124721\n",
      "247 0.00081190845\n",
      "248 0.00081141875\n",
      "249 0.00081083534\n",
      "250 0.00081026973\n",
      "251 0.00080977066\n",
      "252 0.00080921315\n",
      "253 0.0008086422\n",
      "254 0.0008081288\n",
      "255 0.00080759753\n",
      "256 0.00080703106\n",
      "257 0.00080649427\n",
      "258 0.0008059775\n",
      "259 0.0008054328\n",
      "260 0.00080487883\n",
      "261 0.0008043507\n",
      "262 0.0008038284\n",
      "263 0.0008032861\n",
      "264 0.0008027399\n",
      "265 0.00080220826\n",
      "266 0.0008016847\n",
      "267 0.0008011533\n",
      "268 0.00080061285\n",
      "269 0.00080007553\n",
      "270 0.00079954707\n",
      "271 0.0007990225\n",
      "272 0.0007984965\n",
      "273 0.0007979655\n",
      "274 0.000797433\n",
      "275 0.00079690263\n",
      "276 0.0007963761\n",
      "277 0.0007958535\n",
      "278 0.00079533353\n",
      "279 0.00079481583\n",
      "280 0.0007943001\n",
      "281 0.0007937892\n",
      "282 0.0007932832\n",
      "283 0.0007927911\n",
      "284 0.0007923241\n",
      "285 0.00079192047\n",
      "286 0.00079167\n",
      "287 0.0007918423\n",
      "288 0.00079321966\n",
      "289 0.00079818605\n",
      "290 0.0008143597\n",
      "291 0.00086243526\n",
      "292 0.0009905839\n",
      "293 0.0011733124\n",
      "294 0.0011647517\n",
      "295 0.00084416586\n",
      "296 0.0008736968\n",
      "297 0.0010140038\n",
      "298 0.0008084271\n",
      "299 0.0008931867\n",
      "300 0.0008919359\n",
      "301 0.0008005168\n",
      "302 0.00090198487\n",
      "303 0.00079450855\n",
      "304 0.0008660775\n",
      "305 0.0008136691\n",
      "306 0.0008302787\n",
      "307 0.0008262379\n",
      "308 0.0008079921\n",
      "309 0.00082999707\n",
      "310 0.0007974716\n",
      "311 0.00082756\n",
      "312 0.00079259113\n",
      "313 0.00082178856\n",
      "314 0.00079111964\n",
      "315 0.0008155139\n",
      "316 0.00079141266\n",
      "317 0.000809028\n",
      "318 0.0007922528\n",
      "319 0.00080300024\n",
      "320 0.00079316966\n",
      "321 0.00079766865\n",
      "322 0.00079380034\n",
      "323 0.0007932887\n",
      "324 0.0007940622\n",
      "325 0.00078987045\n",
      "326 0.0007936225\n",
      "327 0.0007872988\n",
      "328 0.00079250406\n",
      "329 0.00078575604\n",
      "330 0.0007906951\n",
      "331 0.0007851031\n",
      "332 0.0007883352\n",
      "333 0.00078501785\n",
      "334 0.0007857151\n",
      "335 0.00078508427\n",
      "336 0.0007834419\n",
      "337 0.00078476366\n",
      "338 0.00078191987\n",
      "339 0.0007837427\n",
      "340 0.0007812806\n",
      "341 0.00078205194\n",
      "342 0.00078117254\n",
      "343 0.0007803242\n",
      "344 0.0007808758\n",
      "345 0.0007791949\n",
      "346 0.00077990844\n",
      "347 0.00077880424\n",
      "348 0.000778492\n",
      "349 0.0007785473\n",
      "350 0.00077742705\n",
      "351 0.0007777513\n",
      "352 0.0007770042\n",
      "353 0.0007765975\n",
      "354 0.00077663135\n",
      "355 0.0007757986\n",
      "356 0.0007757881\n",
      "357 0.0007754401\n",
      "358 0.0007748616\n",
      "359 0.0007748709\n",
      "360 0.000774361\n",
      "361 0.000774002\n",
      "362 0.00077390764\n",
      "363 0.00077339745\n",
      "364 0.00077314745\n",
      "365 0.00077296596\n",
      "366 0.00077250705\n",
      "367 0.0007722882\n",
      "368 0.00077206374\n",
      "369 0.00077165704\n",
      "370 0.00077143766\n",
      "371 0.0007712036\n",
      "372 0.00077083736\n",
      "373 0.00077060505\n",
      "374 0.00077037583\n",
      "375 0.00077004253\n",
      "376 0.0007697943\n",
      "377 0.0007695744\n",
      "378 0.0007692707\n",
      "379 0.0007690073\n",
      "380 0.00076879153\n",
      "381 0.00076851784\n",
      "382 0.0007682461\n",
      "383 0.00076802436\n",
      "384 0.0007677795\n",
      "385 0.00076750974\n",
      "386 0.00076727447\n",
      "387 0.00076704845\n",
      "388 0.0007667944\n",
      "389 0.00076654716\n",
      "390 0.0007663236\n",
      "391 0.000766091\n",
      "392 0.0007658447\n",
      "393 0.0007656123\n",
      "394 0.00076539075\n",
      "395 0.000765159\n",
      "396 0.000764923\n",
      "397 0.0007646981\n",
      "398 0.0007644783\n",
      "399 0.0007642519\n",
      "400 0.0007640235\n",
      "401 0.0007638029\n",
      "402 0.000763587\n",
      "403 0.0007633677\n",
      "404 0.0007631461\n",
      "405 0.0007629292\n",
      "406 0.000762717\n",
      "407 0.0007625037\n",
      "408 0.00076228945\n",
      "409 0.00076207635\n",
      "410 0.0007618669\n",
      "411 0.00076166\n",
      "412 0.00076145254\n",
      "413 0.00076124474\n",
      "414 0.0007610386\n",
      "415 0.0007608352\n",
      "416 0.0007606338\n",
      "417 0.0007604326\n",
      "418 0.0007602319\n",
      "419 0.0007600322\n",
      "420 0.00075983437\n",
      "421 0.00075963855\n",
      "422 0.0007594435\n",
      "423 0.0007592495\n",
      "424 0.00075905654\n",
      "425 0.0007588642\n",
      "426 0.0007586737\n",
      "427 0.00075848476\n",
      "428 0.0007582975\n",
      "429 0.00075811084\n",
      "430 0.00075792515\n",
      "431 0.0007577405\n",
      "432 0.00075755717\n",
      "433 0.00075737556\n",
      "434 0.00075719465\n",
      "435 0.0007570149\n",
      "436 0.000756837\n",
      "437 0.0007566602\n",
      "438 0.0007564842\n",
      "439 0.0007563095\n",
      "440 0.0007561363\n",
      "441 0.00075596425\n",
      "442 0.00075579307\n",
      "443 0.0007556232\n",
      "444 0.0007554548\n",
      "445 0.0007552873\n",
      "446 0.0007551209\n",
      "447 0.0007549562\n",
      "448 0.00075479306\n",
      "449 0.000754631\n",
      "450 0.00075447035\n",
      "451 0.0007543119\n",
      "452 0.00075415586\n",
      "453 0.0007540042\n",
      "454 0.00075385877\n",
      "455 0.0007537259\n",
      "456 0.0007536181\n",
      "457 0.00075356406\n",
      "458 0.00075362704\n",
      "459 0.0007539572\n",
      "460 0.00075489876\n",
      "461 0.00075731263\n",
      "462 0.0007631525\n",
      "463 0.00077744754\n",
      "464 0.00080972974\n",
      "465 0.0008806986\n",
      "466 0.0009876611\n",
      "467 0.0010817822\n",
      "468 0.0009718324\n",
      "469 0.0007790166\n",
      "470 0.00080166315\n",
      "471 0.0009044465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472 0.0008120604\n",
      "473 0.0007654172\n",
      "474 0.0008515318\n",
      "475 0.00079737516\n",
      "476 0.00076739374\n",
      "477 0.000825114\n",
      "478 0.0007721166\n",
      "479 0.0007774092\n",
      "480 0.0008022004\n",
      "481 0.00075832696\n",
      "482 0.0007857558\n",
      "483 0.00077764294\n",
      "484 0.00075787987\n",
      "485 0.00078303553\n",
      "486 0.00076123275\n",
      "487 0.00076249975\n",
      "488 0.0007739586\n",
      "489 0.00075417064\n",
      "490 0.0007645706\n",
      "491 0.00076502503\n",
      "492 0.0007528903\n",
      "493 0.0007639832\n",
      "494 0.0007589613\n",
      "495 0.00075296714\n",
      "496 0.0007615725\n",
      "497 0.0007555829\n",
      "498 0.0007529417\n",
      "499 0.00075892505\n",
      "500 0.00075357745\n",
      "501 0.0007523585\n",
      "502 0.0007565008\n",
      "503 0.000752283\n",
      "504 0.0007517296\n",
      "505 0.0007546054\n",
      "506 0.0007513669\n",
      "507 0.0007509904\n",
      "508 0.0007531167\n",
      "509 0.0007507776\n",
      "510 0.00075031753\n",
      "511 0.00075191044\n",
      "512 0.0007502716\n",
      "513 0.00074966927\n",
      "514 0.0007508904\n",
      "515 0.0007498307\n",
      "516 0.0007490968\n",
      "517 0.0007499785\n",
      "518 0.0007493901\n",
      "519 0.00074858806\n",
      "520 0.00074915995\n",
      "521 0.00074897194\n",
      "522 0.00074818486\n",
      "523 0.0007484194\n",
      "524 0.00074851466\n",
      "525 0.00074788317\n",
      "526 0.0007478114\n",
      "527 0.0007480204\n",
      "528 0.0007476303\n",
      "529 0.00074733875\n",
      "530 0.00074749335\n",
      "531 0.0007473637\n",
      "532 0.0007470094\n",
      "533 0.0007469877\n",
      "534 0.00074701424\n",
      "535 0.00074675906\n",
      "536 0.00074657437\n",
      "537 0.0007465969\n",
      "538 0.0007464967\n",
      "539 0.0007462699\n",
      "540 0.0007461831\n",
      "541 0.00074616453\n",
      "542 0.0007460191\n",
      "543 0.00074585\n",
      "544 0.00074579084\n",
      "545 0.00074573286\n",
      "546 0.0007455903\n",
      "547 0.00074545987\n",
      "548 0.0007454\n",
      "549 0.0007453245\n",
      "550 0.00074519555\n",
      "551 0.0007450835\n",
      "552 0.000745016\n",
      "553 0.00074493577\n",
      "554 0.0007448214\n",
      "555 0.00074471696\n",
      "556 0.0007446418\n",
      "557 0.00074456236\n",
      "558 0.00074446\n",
      "559 0.00074435933\n",
      "560 0.0007442781\n",
      "561 0.0007442\n",
      "562 0.00074410747\n",
      "563 0.00074401073\n",
      "564 0.00074392493\n",
      "565 0.0007438467\n",
      "566 0.0007437613\n",
      "567 0.0007436701\n",
      "568 0.00074358145\n",
      "569 0.00074350054\n",
      "570 0.00074342004\n",
      "571 0.00074333395\n",
      "572 0.000743246\n",
      "573 0.0007431624\n",
      "574 0.00074308197\n",
      "575 0.000743001\n",
      "576 0.00074291666\n",
      "577 0.00074283243\n",
      "578 0.00074275065\n",
      "579 0.0007426706\n",
      "580 0.0007425899\n",
      "581 0.00074250787\n",
      "582 0.0007424253\n",
      "583 0.0007423445\n",
      "584 0.0007422649\n",
      "585 0.00074218545\n",
      "586 0.00074210484\n",
      "587 0.0007420245\n",
      "588 0.0007419441\n",
      "589 0.00074186485\n",
      "590 0.00074178615\n",
      "591 0.00074170687\n",
      "592 0.00074162736\n",
      "593 0.00074154825\n",
      "594 0.0007414693\n",
      "595 0.000741391\n",
      "596 0.0007413126\n",
      "597 0.0007412341\n",
      "598 0.0007411555\n",
      "599 0.00074107735\n",
      "600 0.0007409995\n",
      "601 0.0007409214\n",
      "602 0.0007408435\n",
      "603 0.00074076606\n",
      "604 0.00074068835\n",
      "605 0.0007406104\n",
      "606 0.0007405329\n",
      "607 0.0007404555\n",
      "608 0.0007403779\n",
      "609 0.00074030075\n",
      "610 0.00074022356\n",
      "611 0.0007401463\n",
      "612 0.00074006914\n",
      "613 0.00073999213\n",
      "614 0.00073991495\n",
      "615 0.00073983794\n",
      "616 0.00073976093\n",
      "617 0.0007396842\n",
      "618 0.00073960726\n",
      "619 0.0007395306\n",
      "620 0.0007394533\n",
      "621 0.00073937664\n",
      "622 0.0007392998\n",
      "623 0.000739223\n",
      "624 0.00073914614\n",
      "625 0.0007390693\n",
      "626 0.00073899265\n",
      "627 0.00073891605\n",
      "628 0.0007388395\n",
      "629 0.00073876255\n",
      "630 0.00073868554\n",
      "631 0.0007386089\n",
      "632 0.0007385322\n",
      "633 0.00073845545\n",
      "634 0.00073837856\n",
      "635 0.00073830184\n",
      "636 0.00073822495\n",
      "637 0.00073814817\n",
      "638 0.0007380713\n",
      "639 0.0007379944\n",
      "640 0.0007379172\n",
      "641 0.0007378402\n",
      "642 0.00073776353\n",
      "643 0.0007376865\n",
      "644 0.00073760946\n",
      "645 0.00073753233\n",
      "646 0.00073745503\n",
      "647 0.0007373781\n",
      "648 0.00073730096\n",
      "649 0.00073722383\n",
      "650 0.0007371466\n",
      "651 0.00073706935\n",
      "652 0.0007369917\n",
      "653 0.0007369145\n",
      "654 0.0007368369\n",
      "655 0.0007367597\n",
      "656 0.00073668256\n",
      "657 0.0007366054\n",
      "658 0.000736529\n",
      "659 0.00073645276\n",
      "660 0.0007363785\n",
      "661 0.0007363058\n",
      "662 0.00073623826\n",
      "663 0.000736179\n",
      "664 0.00073613564\n",
      "665 0.00073612196\n",
      "666 0.00073616713\n",
      "667 0.0007363246\n",
      "668 0.00073670933\n",
      "669 0.00073753606\n",
      "670 0.0007392777\n",
      "671 0.00074279256\n",
      "672 0.0007500538\n",
      "673 0.0007640802\n",
      "674 0.0007919334\n",
      "675 0.000837462\n",
      "676 0.0009079004\n",
      "677 0.00095382426\n",
      "678 0.00093366706\n",
      "679 0.000809941\n",
      "680 0.0007378055\n",
      "681 0.0007942248\n",
      "682 0.000834405\n",
      "683 0.000774168\n",
      "684 0.00074131245\n",
      "685 0.000789462\n",
      "686 0.00078518\n",
      "687 0.0007400184\n",
      "688 0.0007628038\n",
      "689 0.00077694317\n",
      "690 0.00074196554\n",
      "691 0.0007490211\n",
      "692 0.00076726463\n",
      "693 0.0007441902\n",
      "694 0.0007402047\n",
      "695 0.0007577742\n",
      "696 0.00074657914\n",
      "697 0.00073586975\n",
      "698 0.00074839307\n",
      "699 0.0007474339\n",
      "700 0.0007355225\n",
      "701 0.000740464\n",
      "702 0.0007456955\n",
      "703 0.00073742494\n",
      "704 0.00073546934\n",
      "705 0.00074166345\n",
      "706 0.0007391084\n",
      "707 0.00073401356\n",
      "708 0.00073741045\n",
      "709 0.000739088\n",
      "710 0.00073467946\n",
      "711 0.00073448726\n",
      "712 0.0007374367\n",
      "713 0.00073569146\n",
      "714 0.0007333602\n",
      "715 0.00073503825\n",
      "716 0.0007356903\n",
      "717 0.00073352933\n",
      "718 0.0007332133\n",
      "719 0.00073457696\n",
      "720 0.00073392503\n",
      "721 0.000732553\n",
      "722 0.00073306257\n",
      "723 0.0007336747\n",
      "724 0.00073275226\n",
      "725 0.00073212525\n",
      "726 0.0007326866\n",
      "727 0.00073278527\n",
      "728 0.0007320298\n",
      "729 0.00073180365\n",
      "730 0.0007321899\n",
      "731 0.0007320678\n",
      "732 0.00073152565\n",
      "733 0.0007314542\n",
      "734 0.0007316753\n",
      "735 0.0007314923\n",
      "736 0.0007311164\n",
      "737 0.0007310854\n",
      "738 0.0007311947\n",
      "739 0.0007310229\n",
      "740 0.0007307499\n",
      "741 0.00073070993\n",
      "742 0.00073075335\n",
      "743 0.0007306128\n",
      "744 0.00073040003\n",
      "745 0.00073033286\n",
      "746 0.0007303371\n",
      "747 0.0007302322\n",
      "748 0.0007300601\n",
      "749 0.00072996865\n",
      "750 0.000729945\n",
      "751 0.00072986836\n",
      "752 0.0007297294\n",
      "753 0.0007296209\n",
      "754 0.0007295701\n",
      "755 0.0007295092\n",
      "756 0.0007294003\n",
      "757 0.00072928634\n",
      "758 0.0007292104\n",
      "759 0.00072914985\n",
      "760 0.0007290639\n",
      "761 0.0007289576\n",
      "762 0.00072886475\n",
      "763 0.0007287933\n",
      "764 0.00072871975\n",
      "765 0.0007286276\n",
      "766 0.0007285302\n",
      "767 0.00072844635\n",
      "768 0.0007283712\n",
      "769 0.00072829006\n",
      "770 0.0007281986\n",
      "771 0.0007281076\n",
      "772 0.0007280246\n",
      "773 0.0007279459\n",
      "774 0.000727862\n",
      "775 0.00072777306\n",
      "776 0.0007276843\n",
      "777 0.000727601\n",
      "778 0.00072751945\n",
      "779 0.00072743476\n",
      "780 0.0007273472\n",
      "781 0.00072725967\n",
      "782 0.0007271749\n",
      "783 0.0007270916\n",
      "784 0.00072700676\n",
      "785 0.0007269198\n",
      "786 0.00072683214\n",
      "787 0.00072674634\n",
      "788 0.00072666164\n",
      "789 0.00072657614\n",
      "790 0.0007264895\n",
      "791 0.00072640256\n",
      "792 0.00072631566\n",
      "793 0.00072622934\n",
      "794 0.0007261431\n",
      "795 0.0007260566\n",
      "796 0.0007259691\n",
      "797 0.00072588195\n",
      "798 0.00072579493\n",
      "799 0.00072570797\n",
      "800 0.0007256207\n",
      "801 0.0007255332\n",
      "802 0.0007254453\n",
      "803 0.0007253573\n",
      "804 0.00072526967\n",
      "805 0.0007251822\n",
      "806 0.0007250937\n",
      "807 0.0007250061\n",
      "808 0.0007249175\n",
      "809 0.0007248292\n",
      "810 0.0007247405\n",
      "811 0.00072465185\n",
      "812 0.0007245636\n",
      "813 0.0007244746\n",
      "814 0.00072438596\n",
      "815 0.00072429684\n",
      "816 0.00072420767\n",
      "817 0.0007241181\n",
      "818 0.0007240291\n",
      "819 0.0007239394\n",
      "820 0.00072385\n",
      "821 0.0007237604\n",
      "822 0.0007236706\n",
      "823 0.00072358066\n",
      "824 0.0007234908\n",
      "825 0.0007234005\n",
      "826 0.00072331046\n",
      "827 0.0007232206\n",
      "828 0.00072313024\n",
      "829 0.00072303956\n",
      "830 0.000722949\n",
      "831 0.00072285853\n",
      "832 0.00072276755\n",
      "833 0.00072267686\n",
      "834 0.0007225861\n",
      "835 0.0007224954\n",
      "836 0.0007224039\n",
      "837 0.0007223131\n",
      "838 0.00072222174\n",
      "839 0.0007221304\n",
      "840 0.00072203943\n",
      "841 0.0007219479\n",
      "842 0.00072185637\n",
      "843 0.00072176475\n",
      "844 0.00072167336\n",
      "845 0.00072158134\n",
      "846 0.00072148984\n",
      "847 0.0007213977\n",
      "848 0.0007213063\n",
      "849 0.00072121416\n",
      "850 0.0007211223\n",
      "851 0.0007210302\n",
      "852 0.00072093785\n",
      "853 0.0007208458\n",
      "854 0.00072075357\n",
      "855 0.0007206616\n",
      "856 0.0007205691\n",
      "857 0.0007204766\n",
      "858 0.0007203843\n",
      "859 0.0007202917\n",
      "860 0.00072019955\n",
      "861 0.0007201072\n",
      "862 0.0007200148\n",
      "863 0.00071992207\n",
      "864 0.00071982906\n",
      "865 0.00071973674\n",
      "866 0.00071964454\n",
      "867 0.0007195516\n",
      "868 0.0007194589\n",
      "869 0.00071936625\n",
      "870 0.00071927335\n",
      "871 0.00071918085\n",
      "872 0.0007190883\n",
      "873 0.0007189954\n",
      "874 0.0007189024\n",
      "875 0.0007188097\n",
      "876 0.00071871694\n",
      "877 0.00071862456\n",
      "878 0.00071853166\n",
      "879 0.0007184388\n",
      "880 0.000718346\n",
      "881 0.00071825343\n",
      "882 0.00071816077\n",
      "883 0.000718068\n",
      "884 0.0007179752\n",
      "885 0.00071788265\n",
      "886 0.00071779\n",
      "887 0.00071769743\n",
      "888 0.00071760477\n",
      "889 0.000717512\n",
      "890 0.0007174198\n",
      "891 0.00071732723\n",
      "892 0.0007172351\n",
      "893 0.0007171426\n",
      "894 0.0007170504\n",
      "895 0.00071695825\n",
      "896 0.0007168667\n",
      "897 0.0007167759\n",
      "898 0.0007166857\n",
      "899 0.00071659783\n",
      "900 0.0007165141\n",
      "901 0.0007164385\n",
      "902 0.00071637915\n",
      "903 0.0007163531\n",
      "904 0.00071639474\n",
      "905 0.0007165775\n",
      "906 0.0007170517\n",
      "907 0.00071815174\n",
      "908 0.0007205408\n",
      "909 0.0007257741\n",
      "910 0.00073663704\n",
      "911 0.0007599379\n",
      "912 0.0008034584\n",
      "913 0.00088501826\n",
      "914 0.0009760713\n",
      "915 0.0010377988\n",
      "916 0.00091412076\n",
      "917 0.00074557256\n",
      "918 0.00074843125\n",
      "919 0.0008502229\n",
      "920 0.0008101455\n",
      "921 0.0007223436\n",
      "922 0.00078174646\n",
      "923 0.00079921744\n",
      "924 0.00072517514\n",
      "925 0.0007565913\n",
      "926 0.0007796704\n",
      "927 0.0007241735\n",
      "928 0.0007423622\n",
      "929 0.00076556974\n",
      "930 0.00072423246\n",
      "931 0.0007315277\n",
      "932 0.000753262\n",
      "933 0.0007242073\n",
      "934 0.0007236556\n",
      "935 0.00074267545\n",
      "936 0.000723971\n",
      "937 0.000718326\n",
      "938 0.0007337718\n",
      "939 0.0007235235\n",
      "940 0.0007157938\n",
      "941 0.00072721526\n",
      "942 0.00072288647\n",
      "943 0.0007147989\n",
      "944 0.00072219066\n",
      "945 0.0007220251\n",
      "946 0.00071459834\n",
      "947 0.00071806903\n",
      "948 0.0007204727\n",
      "949 0.0007148987\n",
      "950 0.000715159\n",
      "951 0.0007184989\n",
      "952 0.00071542803\n",
      "953 0.0007135329\n",
      "954 0.0007162734\n",
      "955 0.0007155926\n",
      "956 0.0007130013\n",
      "957 0.00071427063\n",
      "958 0.0007150792\n",
      "959 0.0007130166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "960 0.000712875\n",
      "961 0.0007141079\n",
      "962 0.0007131147\n",
      "963 0.0007121792\n",
      "964 0.0007130433\n",
      "965 0.0007129911\n",
      "966 0.00071195717\n",
      "967 0.0007121481\n",
      "968 0.00071256934\n",
      "969 0.0007119039\n",
      "970 0.0007115409\n",
      "971 0.0007119388\n",
      "972 0.0007117848\n",
      "973 0.000711247\n",
      "974 0.00071131706\n",
      "975 0.00071148336\n",
      "976 0.000711133\n",
      "977 0.0007108863\n",
      "978 0.00071102945\n",
      "979 0.00071097736\n",
      "980 0.00071067084\n",
      "981 0.0007105936\n",
      "982 0.0007106675\n",
      "983 0.00071052334\n",
      "984 0.0007103101\n",
      "985 0.0007102915\n",
      "986 0.0007102926\n",
      "987 0.00071013527\n",
      "988 0.0007099928\n",
      "989 0.0007099774\n",
      "990 0.00070993137\n",
      "991 0.0007097887\n",
      "992 0.000709684\n",
      "993 0.0007096554\n",
      "994 0.0007095869\n",
      "995 0.0007094638\n",
      "996 0.0007093784\n",
      "997 0.00070933526\n",
      "998 0.0007092603\n",
      "999 0.0007091541\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#47 time series rnn\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "timesteps=seq_length=7\n",
    "#open,high,low,close,volumn\n",
    "data_dim=5\n",
    "#close\n",
    "output_dim=1\n",
    "xy=np.loadtxt('data-02-stock_daily.csv.txt',delimiter=',')\n",
    "#시간순으로 만들기위함\n",
    "xy=xy[::-1]\n",
    "xy=MinMaxScaler(xy)\n",
    "x=xy\n",
    "y=xy[:,[-1]]\n",
    "\n",
    "dataX=[]\n",
    "dataY=[]\n",
    "for i in range(len(y)-seq_length):\n",
    "    #이전 7일간의 데이터를 가지고 다음 날 코스닥시장 종가 예측\n",
    "    _x=x[i:i+seq_length]\n",
    "    _y=y[i+seq_length]\n",
    "    dataX.append(_x)\n",
    "    dataY.append(_y)\n",
    "    \n",
    "train_size=int(len(dataY)*0.7)\n",
    "test_size=len(dataY)-train_size\n",
    "trainX,testX=np.array(dataX[0:train_size]),np.array(dataX[train_size:len(dataX)])\n",
    "trainY,testY=np.array(dataY[0:train_size]),np.array(dataY[train_size:len(dataY)])\n",
    "\n",
    "X=tf.placeholder(tf.float32,[None,seq_length,data_dim])\n",
    "Y=tf.placeholder(tf.float32,[None,1])\n",
    "\n",
    "cell=tf.contrib.rnn.BasicLSTMCell(num_units=10,state_is_tuple=True)\n",
    "outputs,_states=tf.nn.dynamic_rnn(cell,X,dtype=tf.float32)\n",
    "Y_pred=tf.contrib.layers.fully_connected(outputs[:,-1],output_dim,activation_fn=None)\n",
    "\n",
    "loss=tf.reduce_mean(tf.square(Y_pred-Y))\n",
    "optimizer=tf.train.AdamOptimizer(0.01).minimize(loss)\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(1000):\n",
    "    _,l=sess.run([optimizer,loss],feed_dict={X:trainX,Y:trainY})\n",
    "    print(i,l)\n",
    "testPredict=sess.run(Y_pred,feed_dict={X:testX})\n",
    "\n",
    "plt.plot(testY)\n",
    "plt.plot(testPredict)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
