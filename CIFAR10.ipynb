{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#데이터 다운로드\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "(train_x_list, train_y_list), (test_x, test_y) = cifar10.load_data()\n",
    "\n",
    "class Model():\n",
    "    def __init__(self,sess,name):\n",
    "        self.sess=sess\n",
    "        self.name=name\n",
    "        self._build_net()\n",
    "    \n",
    "\n",
    "    def _build_net(self):\n",
    "        with tf.variable_scope(self.name):\n",
    "            self.X=tf.placeholder(tf.float32,[None,32,32,3])\n",
    "            self.Y=tf.placeholder(tf.int32,[None,1])\n",
    "            Y_one_hot=tf.one_hot(self.Y,10)\n",
    "            Y_one_hot=tf.reshape(Y_one_hot,[-1,10])\n",
    "            self.keep_prob=tf.placeholder(tf.float32)\n",
    "            self.training=tf.placeholder(tf.bool)\n",
    "\n",
    "            W1=tf.get_variable(\"W1\",shape=[3,3,3,32],initializer=tf.contrib.layers.xavier_initializer())\n",
    "            L1=tf.nn.conv2d(self.X,W1,strides=[1,1,1,1],padding='SAME')\n",
    "            L1=tf.nn.relu(tf.layers.batch_normalization(L1, training=self.training))\n",
    "            L1=tf.nn.max_pool(L1,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "            L1=tf.nn.dropout(L1,keep_prob=self.keep_prob)\n",
    "\n",
    "            W2=tf.get_variable(\"W2\",shape=[3,3,32,64],initializer=tf.contrib.layers.xavier_initializer())\n",
    "            L2=tf.nn.conv2d(L1,W2,strides=[1,1,1,1],padding='SAME')\n",
    "            L2=tf.nn.relu(tf.layers.batch_normalization(L2, training=self.training))\n",
    "            L2=tf.nn.max_pool(L2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "            L2=tf.nn.dropout(L2,keep_prob=self.keep_prob)\n",
    "            \n",
    "            W3=tf.get_variable(\"W3\",shape=[3,3,64,128],initializer=tf.contrib.layers.xavier_initializer())\n",
    "            L3=tf.nn.conv2d(L2,W3,strides=[1,1,1,1],padding='SAME')\n",
    "            L3=tf.nn.relu(tf.layers.batch_normalization(L3, training=self.training))\n",
    "            L3=tf.nn.max_pool(L3,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "            L3=tf.nn.dropout(L3,keep_prob=self.keep_prob)\n",
    "            \n",
    "            W4=tf.get_variable(\"W4\",shape=[3,3,128,256],initializer=tf.contrib.layers.xavier_initializer())\n",
    "            L4=tf.nn.conv2d(L3,W4,strides=[1,1,1,1],padding='SAME')\n",
    "            L4=tf.nn.relu(tf.layers.batch_normalization(L4, training=self.training))\n",
    "            L4=tf.nn.dropout(L4,keep_prob=self.keep_prob)\n",
    "            L4=tf.reshape(L4,[-1,4*4*256])\n",
    "\n",
    "            W5=tf.get_variable(\"W5\",shape=[4*4*256,625],initializer=tf.contrib.layers.xavier_initializer())\n",
    "            B1=tf.Variable(tf.random_uniform([625]))\n",
    "            L5=tf.nn.relu(tf.matmul(L4,W5)+B1)\n",
    "            L5=tf.nn.dropout(L5,keep_prob=self.keep_prob)\n",
    "            \n",
    "            W6=tf.get_variable(\"W6\",shape=[625,10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "            B2=tf.Variable(tf.random_normal([10]))\n",
    "            \n",
    "            self.logits=tf.matmul(L5,W6)+B2\n",
    "            self.cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.logits,labels=Y_one_hot))\n",
    "            self.optimizer=tf.train.AdamOptimizer(0.01).minimize(self.cost)\n",
    "\n",
    "            correct_prediction=tf.equal(tf.argmax(self.logits,1),tf.argmax(Y_one_hot,1))\n",
    "            self.accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "            \n",
    "    def predict(self,x_test,keep_prob=1.0,training=False):\n",
    "        return self.sess.run(self.logits,feed_dict={self.X:x_test,self.keep_prob:keep_prob,self.training:training})\n",
    "    \n",
    "    def get_accuracy(self,x_test,y_test,keep_prob=1.0,training=False):\n",
    "        return self.sess.run(self.accuracy,feed_dict={self.X:x_test,self.Y:y_test,self.keep_prob:keep_prob,self.training:training})\n",
    "    def train(self,x_data,y_data,keep_prob=0.7,training=True):\n",
    "        return self.sess.run([self.cost,self.optimizer],feed_dict={self.X:x_data,self.Y:y_data,self.keep_prob:keep_prob,self.training:True})\n",
    "\n",
    "sess=tf.Session()\n",
    "m=[]\n",
    "for i in range(2):\n",
    "    m.append(Model(sess,\"model\"+str(i)))\n",
    "\n",
    "training_epoch=1000\n",
    "batch_size=100\n",
    "prediction=np.zeros(len(test_y)*10).reshape(len(test_y),10)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(training_epoch):\n",
    "    avg_cost_list=np.zeros(len(m))\n",
    "    total_batch=500\n",
    "    idx=0\n",
    "    for i in range(total_batch):\n",
    "        batch_xs,batch_ys=train_x_list[idx:idx+batch_size,:],train_y_list[idx:idx+batch_size,:]\n",
    "        idx+=100\n",
    "        for m_idx,mm in enumerate(m):\n",
    "            c,_=mm.train(batch_xs,batch_ys)\n",
    "            avg_cost_list[m_idx]+=c/total_batch\n",
    "    print(\"epoch: %04d\" %(epoch+1),\"cost: \",avg_cost_list)\n",
    "for m_idx,mm in enumerate(m):\n",
    "    print(\"Index: \",m_idx,\" Accuracy:\",mm.get_accuracy(test_x,test_y))\n",
    "    p=mm.predict(test_x)\n",
    "    prediction+=p\n",
    "ensemble_correct_prediction=tf.equal(tf.argmax(prediction,1),test_y)\n",
    "ensemble_accuracy=tf.reduce_mean(tf.cast(ensemble_correct_prediction,tf.float32))\n",
    "print('ensemble accuracy:',sess.run(ensemble_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
