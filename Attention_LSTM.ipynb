{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Attention_LSTM.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOkjBRpggJqAAn+HJnt0tW5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"tDJzLJQAuZb1","colab_type":"code","outputId":"6fa06c18-b648-4f4b-fa1f-4a4361acc302","executionInfo":{"status":"ok","timestamp":1579254768727,"user_tz":-540,"elapsed":2847,"user":{"displayName":"eodud dj","photoUrl":"","userId":"05787899662039541278"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive',force_remount=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"j4sn8JWOyb5V","colab_type":"code","outputId":"18ab9602-db0b-44d1-d8c1-caa99e2b8c2e","executionInfo":{"status":"ok","timestamp":1579254772664,"user_tz":-540,"elapsed":6736,"user":{"displayName":"eodud dj","photoUrl":"","userId":"05787899662039541278"}},"colab":{"base_uri":"https://localhost:8080/","height":316}},"source":["pip install konlpy"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: konlpy in /usr/local/lib/python3.6/dist-packages (0.5.2)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n","Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (3.8.0)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.17.5)\n","Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (0.7.1)\n","Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from konlpy) (0.4.3)\n","Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.6.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.12.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n","Requirement already satisfied: PySocks>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.7.1)\n","Requirement already satisfied: requests>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.21.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (2019.11.28)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (2.8)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kXod5RV9sGww","colab_type":"code","outputId":"e30b8a78-c3af-4b13-c35d-b6cbcc59d840","executionInfo":{"status":"ok","timestamp":1579254773790,"user_tz":-540,"elapsed":7815,"user":{"displayName":"eodud dj","photoUrl":"","userId":"05787899662039541278"}},"colab":{"base_uri":"https://localhost:8080/","height":80}},"source":["import random\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from konlpy.tag import Okt\n","tf.enable_eager_execution()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"yKCjaLg8t5Rx","colab_type":"code","colab":{}},"source":["epochs = 200\n","num_words = 2000"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-jlOGP-IsvYz","colab_type":"code","colab":{}},"source":["class Encoder(tf.keras.Model):\n","\n","  def __init__(self):\n","    super(Encoder, self).__init__()\n","    self.embed = layers.Embedding(num_words, 64)\n","    self.lstm = layers.LSTM(512, return_sequences=True, return_state=True)\n","\n","  # H contain all information of input (return_sequences)\n","  def __call__(self, x, training=False, mask=None):\n","    x = self.embed(x)\n","    H, h, c = self.lstm(x)\n","    return H, h, c"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7nxj0O0JucbD","colab_type":"code","colab":{}},"source":["class Decoder(tf.keras.Model):\n","\n","  def __init__(self):\n","    super(Decoder, self).__init__()\n","    self.embed = layers.Embedding(num_words, 64)\n","    self.lstm = layers.LSTM(512, return_sequences=True, return_state=True)\n","    self.att = layers.Attention()\n","    self.dense = layers.Dense(num_words, activation='softmax')\n","\n","  # S_ has information of inputs until t-1 time\n","  def __call__(self, inputs, training=False, mask=False):\n","    x, s0, c0, H = inputs\n","    x = self.embed(x)\n","    S, h, c = self.lstm(x, initial_state=[s0, c0])\n","    S_ = tf.concat([s0[:,tf.newaxis,:], S[:,:-1,:]], axis=1)\n","    # Attention()([query, key, value])\n","    A = self.att([S_,H])\n","    y = tf.concat([S,A], axis=-1)\n","    return self.dense(y), h, c"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ic3ndPsJvvck","colab_type":"code","colab":{}},"source":["class Seq2seq(tf.keras.Model):\n","\n","  def __init__(self, sos, eos):\n","    super(Seq2seq, self).__init__()\n","    self.enc = Encoder()\n","    self.dec = Decoder()\n","    self.sos = sos\n","    self.eos = eos\n","\n","  def __call__(self, inputs, training=False,mask=None):\n","    if training is True:\n","      x, y = inputs\n","      H, h, c = self.enc(x)\n","      y, _, _ = self.dec((y,h,c,H))\n","      return y\n","    else:\n","      x = inputs\n","      H, h, c = self.enc(x)\n","\n","      y = tf.convert_to_tensor(self.sos)\n","      y = tf.reshape(y,(1,1))\n","      seq = tf.TensorArray(tf.int32, 64)\n","\n","      for idx in tf.range(64):\n","        y, h, c = self.dec([y, h, c, H])\n","        y = tf.cast(tf.argmax(y, axis=-1), dtype=tf.int32)\n","        y = tf.reshape(y,(1,1))\n","        seq = seq.write(idx, y)\n","        if y==self.eos:\n","          break\n","      return tf.reshape(seq.stack(), (1,64))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4j4r8u4Cygxg","colab_type":"code","colab":{}},"source":["@tf.function\n","def train_step(model, inputs, labels, loss_object, optimizer, train_loss, train_accuracy):\n","  # labels = [sos, ..., eos], shifted = [sos, ...], output = [..., eos]\n","  output_labels = labels[:,1:]\n","  shifted_labels = labels[:,:-1]\n","  with tf.GradientTape() as tape:\n","    predictions = model([inputs, shifted_labels], training=True)\n","    loss =loss_object(output_labels, predictions)\n","  gradients = tape.gradient(loss, model.trainable_variables)\n","  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","  train_loss(loss)\n","  train_accuracy(output_labels, predictions)\n","\n","@tf.function\n","def test_step(model, inputs):\n","  return model(inputs, training=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HJOcIZOks8KT","colab_type":"code","outputId":"724da746-3486-48ba-e761-47484ba4bdfd","executionInfo":{"status":"ok","timestamp":1579255349854,"user_tz":-540,"elapsed":2329,"user":{"displayName":"eodud dj","photoUrl":"","userId":"05787899662039541278"}},"colab":{"base_uri":"https://localhost:8080/","height":176}},"source":["file_path = 'gdrive/My Drive/dataset/chatbot_data.csv'\n","okt = Okt()\n","\n","#open file and split by morpheme\n","with open(file_path, 'r') as file:\n","  lines = file.readlines()\n","  seq = [' '.join(okt.morphs(line)) for line in lines]\n","print(seq[0])\n","\n","# seperate question, answer   answers have to contain sos(\\t), eos(\\n)\n","questions = seq[::2]\n","answers = ['\\t' + lines for lines in seq[1::2]]\n","print(answers[0])\n","\n","num_samples = len(questions)\n","\n","# seperate train, test data (4:1)\n","perm = list(range(num_samples))\n","random.seed(0)\n","random.shuffle(perm)\n","\n","train_q = []\n","train_a = []\n","test_q = []\n","test_a = []\n","\n","for idx,qna in enumerate(zip(questions, answers)):\n","  q, a = qna\n","  if perm[idx] > num_samples//5:\n","    train_q.append(q)\n","    train_a.append(a)\n","  else:\n","    test_q.append(q)\n","    test_a.append(a)\n","\n","#tokenize data\n","tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~')\n","\n","tokenizer.fit_on_texts(train_q + train_a)\n","\n","train_q_seq = tokenizer.texts_to_sequences(train_q)\n","train_a_seq = tokenizer.texts_to_sequences(train_a)\n","test_q_seq = tokenizer.texts_to_sequences(test_q)\n","test_a_seq = tokenizer.texts_to_sequences(test_a)\n","print(train_q_seq)\n","\n","# padding to match same length model input size\n","x_train = pad_sequences(train_q_seq, 64)\n","y_train = pad_sequences(train_a_seq, 65, padding='post')\n","x_test = pad_sequences(test_q_seq, 64)\n","y_test = pad_sequences(test_a_seq, 65, padding='post')\n","print(x_train[0])\n","\n","train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32).prefetch(1024)\n","test_df = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(1).prefetch(1024)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["아이스 아메리카노 하나요 \n","\n","\t테이크아웃 하실 건가 요 ? \n","\n","[[8, 5, 86, 1], [122, 189, 4, 26, 25, 1], [122, 230, 60, 304, 446, 7, 305, 30, 1], [15, 87, 33, 123, 36, 95, 19, 22, 1], [447, 231, 96, 7, 190, 448, 1], [42, 5, 16, 10, 8, 306, 16, 307, 1], [11, 159, 9, 449, 38, 97, 3, 1], [189, 191, 30, 1], [8, 189, 450, 110, 7, 141, 19, 22, 1], [46, 34, 97, 451, 308, 142, 24, 1], [452, 57, 35, 39, 95, 19, 68, 1], [232, 17, 160, 7, 22, 1], [33, 69, 111, 453, 47, 23, 98, 74, 1], [309, 16, 124, 161, 8, 5, 12, 1], [57, 162, 1], [11, 310, 15, 454, 61, 36, 25, 1], [192, 193, 6, 98, 74, 1], [455, 88, 26, 75, 1], [311, 194, 1], [47, 7, 99, 27, 75, 1], [456, 46, 163, 312, 1], [112, 51, 22, 1], [112, 51, 7, 125, 164, 23, 313, 457, 164, 314, 15, 313, 1], [112, 51, 458, 100, 6, 98, 23, 68, 1], [57, 312, 1], [8, 5, 48, 15, 12, 1], [126, 459, 33, 7, 233, 1], [101, 460, 461, 195, 7, 462, 19, 143, 1], [122, 6, 315, 42, 463, 40, 47, 3, 1], [125, 308, 100, 9, 22, 1], [20, 77, 30, 1], [189, 48, 78, 5, 58, 10, 3, 1], [234, 316, 12, 1], [46, 40, 60, 102, 43, 74, 1], [464, 144, 37, 9, 27, 235, 1], [5, 48, 3, 1], [11, 126, 7, 236, 19, 22, 1], [126, 61, 22, 1], [165, 465, 9, 21, 3, 1], [317, 196, 30, 1], [5, 48, 26, 95, 19, 143, 1], [79, 80, 35, 48, 26, 25, 1], [70, 49, 97, 95, 19, 22, 1], [47, 26, 25, 1], [11, 237, 161, 47, 4, 3, 1], [197, 11, 111, 318, 15, 466, 467, 35, 25, 1], [197, 11, 238, 23, 468, 469, 47, 166, 12, 1], [239, 51, 21, 3, 1], [20, 77, 30, 1], [52, 240, 81, 9, 97, 3, 1], [26, 470, 127, 12, 1], [8, 5, 40, 42, 306, 4, 3, 1], [241, 26, 75, 1], [471, 46, 23, 472, 1], [125, 319, 47, 9, 145, 86, 1], [167, 7, 128, 141, 19, 22, 1], [473, 69, 168, 36, 75, 1], [16, 474, 475, 48, 476, 477, 86, 1], [146, 320, 321, 7, 190, 478, 1], [190, 479, 19, 22, 1], [169, 5, 21, 40, 146, 320, 321, 480, 242, 21, 3, 1], [8, 4, 3, 1], [5, 3, 1], [191, 3, 1], [481, 322, 6, 43, 74, 1], [234, 323, 1], [147, 9, 30, 1], [482, 103, 9, 148, 128, 141, 19, 22, 1], [52, 20, 483, 43, 27, 59, 1], [70, 49, 127, 3, 1], [104, 69, 129, 10, 36, 75, 1], [62, 16, 10, 3, 1], [243, 111, 484, 20, 4, 3, 1], [198, 9, 21, 244, 1], [198, 149, 17, 53, 130, 22, 1], [485, 149, 35, 245, 1], [8, 5, 16, 10, 3, 1], [54, 104, 69, 486, 1], [11, 33, 487, 105, 488, 1], [54, 170, 324, 1], [46, 20, 77, 75, 1], [8, 5, 20, 77, 24, 1], [5, 16, 10, 3, 1], [8, 5, 12, 1], [46, 16, 10, 63, 27, 30, 1], [131, 28, 9, 21, 3, 1], [11, 131, 28, 42, 41, 4, 3, 1], [11, 62, 79, 80, 30, 1], [11, 62, 79, 80, 35, 3, 1], [11, 199, 1], [62, 79, 80, 489, 1], [89, 7, 200, 16, 10, 43, 74, 1], [89, 7, 200, 20, 77, 30, 1], [201, 37, 6, 98, 23, 111, 325, 1], [89, 7, 200, 40, 326, 201, 37, 3, 1], [147, 490, 1], [8, 96, 3, 1], [8, 96, 16, 307, 1], [8, 96, 491, 246, 327, 328, 4, 12, 1], [492, 493, 202, 329, 1], [5, 16, 10, 3, 1], [5, 171, 20, 12, 1], [11, 232, 17, 160, 74, 1], [192, 193, 6, 98, 82, 1], [247, 150, 30, 1], [42, 28, 16, 10, 3, 1], [15, 146, 9, 330, 3, 1], [494, 495, 49, 90, 172, 203, 88, 9, 21, 71, 49, 30, 1], [87, 9, 113, 88, 7, 36, 75, 1], [15, 87, 35, 331, 95, 19, 248, 203, 88, 6, 332, 59, 1], [52, 333, 4, 334, 62, 58, 10, 3, 1], [62, 171, 4, 3, 1], [54, 496, 32, 82, 1], [47, 167, 9, 141, 19, 22, 1], [122, 42, 5, 16, 10, 90, 8, 28, 16, 10, 3, 1], [15, 76, 9, 16, 144, 3, 1], [11, 114, 335, 204, 316, 12, 1], [11, 101, 249, 15, 497, 1], [150, 16, 10, 3, 1], [11, 104, 69, 336, 32, 82, 1], [54, 5, 337, 41, 4, 3, 1], [33, 70, 9, 49, 30, 1], [79, 80, 47, 9, 145, 86, 1], [76, 9, 16, 144, 3, 1], [205, 498, 1], [247, 250, 4, 3, 1], [499, 37, 97, 3, 1], [29, 39, 25, 1], [338, 106, 23, 251, 12, 1], [338, 106, 16, 10, 3, 1], [339, 124, 173, 4, 245, 1], [43, 59, 1], [76, 22, 1], [8, 5, 58, 10, 3, 1], [252, 100, 6, 53, 130, 22, 1], [239, 6, 206, 1], [253, 15, 500, 42, 174, 46, 254, 142, 24, 1], [112, 51, 6, 51, 64, 340, 341, 123, 501, 91, 12, 1], [8, 5, 16, 10, 26, 25, 1], [165, 37, 6, 151, 206, 1], [207, 9, 3, 1], [208, 342, 9, 141, 19, 152, 44, 1], [42, 5, 16, 10, 17, 43, 59, 1], [104, 202, 114, 209, 132, 86, 1], [502, 60, 107, 255, 132, 503, 504, 83, 30, 1], [8, 5, 58, 10, 3, 1], [8, 5, 16, 10, 505, 103, 256, 1], [192, 30, 1], [232, 15, 160, 82, 1], [47, 506, 174, 46, 6, 206, 1], [241, 7, 99, 27, 244, 1], [11, 39, 6, 507, 508, 196, 4, 25, 1], [11, 509, 153, 257, 25, 1], [258, 6, 114, 209, 132, 86, 1], [241, 7, 210, 133, 256, 1], [5, 87, 36, 95, 19, 22, 1], [5, 87, 36, 24, 1], [343, 344, 510, 15, 115, 511, 1], [11, 87, 33, 68, 1], [237, 161, 47, 6, 251, 12, 1], [134, 3, 1], [54, 175, 3, 1], [29, 4, 25, 1], [512, 6, 160, 22, 1], [345, 40, 62, 3, 1], [11, 345, 9, 513, 19, 346, 1], [514, 9, 515, 3, 1], [347, 26, 75, 1], [8, 5, 516, 38, 3, 1], [28, 517, 38, 26, 25, 1], [52, 16, 348, 518, 7, 46, 259, 519, 260, 12, 1], [70, 40, 57, 49, 24, 1], [47, 34, 144, 37, 113, 4, 145, 194, 1], [113, 100, 23, 53, 130, 346, 1], [5, 207, 113, 211, 17, 92, 349, 1], [5, 64, 134, 4, 261, 75, 1], [169, 5, 207, 113, 7, 134, 4, 261, 102, 26, 350, 1], [8, 5, 30, 1], [210, 520, 521, 351, 30, 1], [81, 17, 43, 74, 1], [198, 154, 81, 522, 3, 1], [52, 5, 40, 131, 28, 3, 1], [11, 33, 31, 1], [523, 70, 39, 30, 1], [125, 524, 29, 4, 525, 526, 527, 22, 1], [11, 528, 8, 5, 48, 3, 1], [116, 135, 262, 125, 168, 110, 7, 236, 19, 143, 1], [529, 352, 530, 168, 110, 7, 236, 19, 6, 531, 1], [5, 21, 3, 1], [54, 8, 4, 3, 1], [136, 28, 48, 3, 1], [5, 16, 10, 3, 1], [8, 4, 3, 1], [54, 103, 17, 532, 1], [54, 212, 213, 15, 12, 1], [33, 29, 4, 39, 24, 1], [5, 58, 10, 78, 76, 16, 38, 3, 1], [11, 263, 24, 1], [170, 9, 533, 176, 1], [5, 42, 32, 171, 20, 4, 3, 1], [264, 37, 9, 3, 1], [153, 4, 25, 1], [353, 83, 534, 1], [89, 354, 22, 1], [52, 89, 252, 48, 3, 1], [535, 7, 536, 537, 538, 28, 23, 237, 17, 539, 1], [540, 88, 155, 541, 355, 130, 251, 12, 1], [52, 542, 28, 101, 10, 78, 543, 356, 357, 101, 38, 3, 1], [5, 48, 3, 1], [8, 5, 4, 3, 1], [136, 81, 9, 21, 27, 24, 1], [106, 4, 544, 329, 1], [43, 82, 1], [5, 16, 10, 78, 131, 28, 16, 10, 3, 1], [54, 5, 6, 8, 4, 3, 1], [33, 57, 15, 12, 1], [11, 42, 41, 4, 3, 1], [11, 33, 123, 116, 135, 32, 82, 1], [125, 265, 27, 30, 1], [545, 165, 322, 7, 201, 177, 546, 19, 6, 206, 1], [8, 131, 28, 9, 16, 10, 3, 1], [28, 16, 10, 7, 43, 59, 1], [547, 17, 548, 1], [169, 159, 21, 40, 28, 16, 10, 3, 1], [125, 49, 57, 35, 47, 137, 95, 19, 22, 1], [169, 47, 6, 57, 35, 159, 17, 29, 4, 137, 24, 1], [8, 5, 7, 210, 148, 549, 550, 19, 22, 1], [11, 240, 150, 84, 38, 63, 27, 4, 3, 1], [54, 129, 15, 12, 1], [54, 87, 35, 39, 25, 1], [266, 358, 1], [266, 48, 5, 48, 267, 43, 59, 1], [20, 77, 30, 1], [551, 51, 9, 358, 1], [111, 355, 37, 23, 98, 82, 1], [16, 144, 7, 43, 176, 1], [552, 40, 5, 3, 1], [171, 20, 4, 3, 1], [29, 4, 39, 25, 1], [122, 5, 7, 99, 27, 102, 3, 1], [11, 553, 99, 35, 256, 1], [122, 554, 196, 4, 39, 25, 1], [197, 122, 15, 264, 9, 21, 3, 1], [11, 58, 38, 330, 555, 1], [238, 23, 47, 64, 556, 557, 46, 254, 142, 24, 1], [29, 4, 39, 25, 1], [79, 80, 46, 26, 268, 12, 1], [52, 79, 80, 28, 48, 24, 1], [156, 35, 327, 328, 77, 24, 1], [11, 359, 360, 4, 137, 25, 1], [558, 106, 8, 9, 22, 1], [81, 361, 37, 9, 22, 1], [52, 136, 81, 21, 3, 1], [559, 560, 36, 561, 1], [131, 28, 562, 268, 12, 1], [8, 166, 210, 362, 563, 19, 178, 1], [57, 269, 38, 60, 304, 36, 268, 12, 1], [148, 71, 564, 352, 262, 168, 110, 270, 565, 12, 1], [8, 4, 3, 1], [54, 175, 39, 24, 1], [271, 35, 25, 1], [5, 16, 10, 3, 1], [15, 146, 6, 98, 82, 1], [15, 214, 9, 21, 566, 43, 74, 1], [11, 33, 170, 9, 24, 1], [8, 5, 21, 42, 89, 567, 21, 3, 1], [133, 568, 1], [169, 569, 570, 571, 3, 1], [212, 213, 35, 24, 1], [8, 179, 28, 7, 363, 99, 21, 27, 102, 3, 1], [151, 179, 37, 6, 364, 365, 32, 176, 1], [52, 179, 28, 7, 115, 572, 37, 21, 142, 235, 1], [11, 104, 69, 116, 366, 1], [573, 51, 96, 21, 3, 1], [574, 29, 4, 137, 25, 1], [11, 575, 105, 367, 24, 1], [363, 16, 10, 90, 134, 58, 10, 3, 1], [368, 40, 246, 20, 4, 576, 577, 578, 1], [61, 29, 31, 1], [266, 7, 164, 64, 63, 133, 579, 580, 581, 19, 22, 1], [43, 59, 1], [8, 4, 261, 95, 582, 27, 369, 15, 22, 1], [175, 42, 41, 4, 157, 242, 42, 5, 9, 48, 63, 141, 19, 22, 1], [11, 583, 584, 135, 19, 585, 586, 1], [112, 51, 16, 10, 43, 74, 1], [20, 77, 30, 1], [37, 6, 98, 23, 111, 325, 1], [136, 37, 16, 144, 3, 1], [271, 205, 24, 1], [62, 211, 17, 43, 59, 1], [156, 172, 70, 370, 19, 22, 1], [70, 36, 90, 49, 587, 7, 75, 1], [70, 36, 362, 49, 71, 25, 1], [156, 29, 588, 271, 35, 137, 25, 1], [8, 5, 3, 1], [170, 9, 25, 1], [147, 15, 12, 1], [49, 9, 24, 1], [8, 5, 21, 40, 371, 28, 3, 1], [371, 28, 6, 263, 3, 1], [129, 10, 35, 3, 1], [272, 59, 1], [27, 24, 1], [5, 589, 590, 15, 172, 174, 41, 4, 591, 19, 143, 1], [592, 57, 36, 102, 171, 20, 4, 24, 1], [27, 369, 17, 43, 59, 1], [15, 29, 4, 24, 1], [8, 5, 84, 10, 78, 8, 62, 84, 10, 43, 59, 1], [156, 61, 30, 1], [87, 172, 370, 19, 22, 1], [33, 69, 593, 594, 595, 372, 35, 596, 86, 1], [33, 353, 30, 1], [597, 598, 86, 1], [11, 8, 12, 1], [11, 116, 44, 1], [33, 29, 4, 39, 24, 1], [253, 83, 599, 46, 6, 98, 23, 143, 1], [373, 600, 53, 130, 68, 1], [601, 155, 7, 53, 130, 63, 115, 273, 1], [89, 373, 34, 3, 1], [51, 100, 9, 8, 75, 1], [52, 602, 603, 8, 4, 21, 3, 1], [11, 324, 1], [106, 374, 16, 10, 3, 1], [274, 129, 15, 12, 1], [42, 41, 4, 3, 1], [8, 5, 21, 40, 134, 21, 3, 1], [134, 6, 42, 32, 4, 3, 1], [604, 605, 113, 61, 30, 1], [606, 607, 21, 40, 8, 5, 21, 102, 3, 1], [317, 196, 30, 1], [608, 6, 609, 166, 12, 1], [11, 610, 7, 5, 64, 611, 612, 1], [53, 124, 59, 1], [375, 613, 17, 614, 166, 12, 1], [52, 47, 34, 615, 37, 9, 142, 127, 3, 1], [62, 3, 1], [54, 129, 15, 12, 1], [11, 29, 33, 12, 1], [275, 21, 616, 3, 1], [5, 84, 10, 3, 1], [131, 28, 64, 333, 4, 334, 376, 1], [101, 10, 26, 267, 43, 176, 1], [374, 28, 84, 10, 3, 1], [76, 21, 27, 24, 1], [5, 58, 10, 26, 25, 1], [165, 37, 617, 26, 25, 1], [215, 175, 116, 618, 1], [8, 5, 16, 10, 78, 159, 21, 25, 1], [54, 147, 15, 12, 1], [11, 245, 1], [230, 162, 1], [33, 253, 83, 619, 130, 22, 1], [276, 191, 26, 75, 1], [52, 216, 191, 21, 40, 276, 21, 25, 1], [11, 33, 620, 621, 61, 349, 1], [11, 33, 29, 31, 1], [622, 28, 8, 9, 30, 1], [258, 192, 193, 623, 19, 143, 1], [57, 114, 38, 64, 377, 272, 4, 624, 64, 336, 19, 68, 1], [258, 114, 209, 132, 127, 12, 1], [625, 15, 114, 209, 59, 1], [277, 96, 3, 1], [265, 7, 277, 378, 78, 379, 380, 231, 626, 1], [277, 378, 78, 379, 380, 627, 1], [8, 5, 40, 112, 51, 278, 32, 40, 76, 21, 3, 1], [52, 368, 41, 4, 25, 1], [11, 116, 366, 1], [5, 8, 4, 86, 1], [628, 20, 4, 3, 1], [146, 9, 3, 1], [274, 175, 3, 1], [61, 29, 6, 55, 1], [5, 86, 1], [275, 8, 4, 3, 1], [275, 21, 376, 1], [29, 39, 25, 1], [5, 48, 629, 1], [15, 630, 7, 278, 32, 631, 1], [5, 101, 10, 3, 1], [21, 6, 42, 32, 21, 6, 278, 32, 4, 3, 1], [148, 632, 135, 32, 74, 1], [33, 57, 230, 162, 1], [310, 633, 634, 9, 84, 10, 71, 244, 1], [8, 5, 58, 10, 78, 179, 37, 16, 38, 3, 1], [54, 116, 135, 32, 82, 1], [58, 38, 3, 1], [43, 83, 381, 135, 262, 212, 213, 10, 7, 157, 72, 83, 260, 12, 1], [382, 7, 3, 1], [5, 99, 27, 4, 3, 1], [11, 33, 126, 7, 233, 1], [103, 17, 160, 22, 1], [635, 159, 58, 636, 147, 24, 1], [217, 637, 638, 1], [383, 24, 1], [104, 69, 116, 135, 32, 82, 1], [46, 6, 126, 7, 233, 1], [11, 199, 1], [384, 7, 195, 22, 1]]\n","[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  8  5 86  1]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3pA_SQnVu2Tw","colab_type":"code","colab":{}},"source":["model = Seq2seq(sos=tokenizer.word_index['\\t'], eos=tokenizer.word_index['\\n'])\n","\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n","optimizer = tf.keras.optimizers.Adam()\n","\n","train_loss = tf.keras.metrics.Mean(name='train_loss')\n","train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dl3DcoqF0Fsr","colab_type":"code","outputId":"477bf6c5-1eb1-4c10-d00b-f41e1dfd203e","executionInfo":{"status":"ok","timestamp":1579256015682,"user_tz":-540,"elapsed":662296,"user":{"displayName":"eodud dj","photoUrl":"","userId":"05787899662039541278"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["for epoch in range(epochs):\n","  for seqs, labels in train_ds:\n","    train_step(model, seqs, labels, loss_object, optimizer, train_loss, train_accuracy)\n","  template = 'Epoch {}, Loss: {}, Accuracy: {}'\n","  print(template.format(epoch + 1,\n","                        train_loss.result(),\n","                        train_accuracy.result() * 100))\n","\n","  train_loss.reset_states()\n","  train_accuracy.reset_states()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","Epoch 1, Loss: 2.9809491634368896, Accuracy: 84.55122375488281\n","Epoch 2, Loss: 0.5946705341339111, Accuracy: 91.89771270751953\n","Epoch 3, Loss: 0.5005366206169128, Accuracy: 91.99952697753906\n","Epoch 4, Loss: 0.4821603000164032, Accuracy: 92.0974349975586\n","Epoch 5, Loss: 0.4737153649330139, Accuracy: 92.15225219726562\n","Epoch 6, Loss: 0.4668738543987274, Accuracy: 92.14442443847656\n","Epoch 7, Loss: 0.4595836102962494, Accuracy: 92.1483383178711\n","Epoch 8, Loss: 0.4502396881580353, Accuracy: 92.1953353881836\n","Epoch 9, Loss: 0.43705856800079346, Accuracy: 92.23841094970703\n","Epoch 10, Loss: 0.42030662298202515, Accuracy: 92.3245620727539\n","Epoch 11, Loss: 0.4078884422779083, Accuracy: 92.55560302734375\n","Epoch 12, Loss: 0.4016413986682892, Accuracy: 92.70832824707031\n","Epoch 13, Loss: 0.38907620310783386, Accuracy: 92.97462463378906\n","Epoch 14, Loss: 0.37991511821746826, Accuracy: 93.19001007080078\n","Epoch 15, Loss: 0.3736167550086975, Accuracy: 93.20567321777344\n","Epoch 16, Loss: 0.3703174293041229, Accuracy: 93.11168670654297\n","Epoch 17, Loss: 0.3725452721118927, Accuracy: 93.09602355957031\n","Epoch 18, Loss: 0.36026495695114136, Accuracy: 93.27616119384766\n","Epoch 19, Loss: 0.3484964370727539, Accuracy: 93.60511016845703\n","Epoch 20, Loss: 0.3410773277282715, Accuracy: 93.66384887695312\n","Epoch 21, Loss: 0.3355444669723511, Accuracy: 93.76174926757812\n","Epoch 22, Loss: 0.3300728499889374, Accuracy: 93.80091094970703\n","Epoch 23, Loss: 0.32482627034187317, Accuracy: 93.85181427001953\n","Epoch 24, Loss: 0.3196248710155487, Accuracy: 93.86356353759766\n","Epoch 25, Loss: 0.31453242897987366, Accuracy: 93.90272521972656\n","Epoch 26, Loss: 0.30959436297416687, Accuracy: 93.98104858398438\n","Epoch 27, Loss: 0.3048340380191803, Accuracy: 94.0515365600586\n","Epoch 28, Loss: 0.3002433776855469, Accuracy: 94.1024398803711\n","Epoch 29, Loss: 0.29581883549690247, Accuracy: 94.15726470947266\n","Epoch 30, Loss: 0.29166436195373535, Accuracy: 94.18467712402344\n","Epoch 31, Loss: 0.28774502873420715, Accuracy: 94.19251251220703\n","Epoch 32, Loss: 0.2843494117259979, Accuracy: 94.2003402709961\n","Epoch 33, Loss: 0.2825359106063843, Accuracy: 94.21208953857422\n","Epoch 34, Loss: 0.28263941407203674, Accuracy: 94.18467712402344\n","Epoch 35, Loss: 0.28747832775115967, Accuracy: 94.15335083007812\n","Epoch 36, Loss: 0.2816756069660187, Accuracy: 94.2864990234375\n","Epoch 37, Loss: 0.2730996012687683, Accuracy: 94.36481475830078\n","Epoch 38, Loss: 0.26418668031692505, Accuracy: 94.40397644042969\n","Epoch 39, Loss: 0.25760021805763245, Accuracy: 94.45488739013672\n","Epoch 40, Loss: 0.2532902956008911, Accuracy: 94.50971221923828\n","Epoch 41, Loss: 0.24905723333358765, Accuracy: 94.55278778076172\n","Epoch 42, Loss: 0.24478627741336823, Accuracy: 94.5684585571289\n","Epoch 43, Loss: 0.24070389568805695, Accuracy: 94.62327575683594\n","Epoch 44, Loss: 0.23672805726528168, Accuracy: 94.67027282714844\n","Epoch 45, Loss: 0.23294129967689514, Accuracy: 94.72118377685547\n","Epoch 46, Loss: 0.22973810136318207, Accuracy: 94.73684692382812\n","Epoch 47, Loss: 0.2274581640958786, Accuracy: 94.76817321777344\n","Epoch 48, Loss: 0.22688518464565277, Accuracy: 94.79167175292969\n","Epoch 49, Loss: 0.2255536913871765, Accuracy: 94.83866119384766\n","Epoch 50, Loss: 0.22357971966266632, Accuracy: 94.80733489990234\n","Epoch 51, Loss: 0.22018831968307495, Accuracy: 94.84257507324219\n","Epoch 52, Loss: 0.21538887917995453, Accuracy: 94.94047546386719\n","Epoch 53, Loss: 0.2096443772315979, Accuracy: 95.06970977783203\n","Epoch 54, Loss: 0.20087596774101257, Accuracy: 95.1558609008789\n","Epoch 55, Loss: 0.19571031630039215, Accuracy: 95.39082336425781\n","Epoch 56, Loss: 0.19014543294906616, Accuracy: 95.49263763427734\n","Epoch 57, Loss: 0.18485617637634277, Accuracy: 95.58662414550781\n","Epoch 58, Loss: 0.17961335182189941, Accuracy: 95.7432632446289\n","Epoch 59, Loss: 0.1750781387090683, Accuracy: 95.8411636352539\n","Epoch 60, Loss: 0.17058271169662476, Accuracy: 95.91948699951172\n","Epoch 61, Loss: 0.16535250842571259, Accuracy: 96.10746002197266\n","Epoch 62, Loss: 0.1601410210132599, Accuracy: 96.24452209472656\n","Epoch 63, Loss: 0.15529672801494598, Accuracy: 96.33458709716797\n","Epoch 64, Loss: 0.15055155754089355, Accuracy: 96.43248748779297\n","Epoch 65, Loss: 0.14510473608970642, Accuracy: 96.64787292480469\n","Epoch 66, Loss: 0.14017587900161743, Accuracy: 96.7849349975586\n","Epoch 67, Loss: 0.13557317852973938, Accuracy: 96.92591094970703\n","Epoch 68, Loss: 0.13055258989334106, Accuracy: 97.06297302246094\n","Epoch 69, Loss: 0.1277695596218109, Accuracy: 97.03947448730469\n","Epoch 70, Loss: 0.12478472292423248, Accuracy: 97.04730224609375\n","Epoch 71, Loss: 0.11951467394828796, Accuracy: 97.26268768310547\n","Epoch 72, Loss: 0.11468298733234406, Accuracy: 97.3723373413086\n","Epoch 73, Loss: 0.10814428329467773, Accuracy: 97.61121368408203\n","Epoch 74, Loss: 0.10304972529411316, Accuracy: 97.71694946289062\n","Epoch 75, Loss: 0.09817269444465637, Accuracy: 97.94408416748047\n","Epoch 76, Loss: 0.0939473956823349, Accuracy: 98.05372619628906\n","Epoch 77, Loss: 0.08982086926698685, Accuracy: 98.13204956054688\n","Epoch 78, Loss: 0.08556529879570007, Accuracy: 98.19078826904297\n","Epoch 79, Loss: 0.0814603865146637, Accuracy: 98.35526275634766\n","Epoch 80, Loss: 0.0775308683514595, Accuracy: 98.41008758544922\n","Epoch 81, Loss: 0.07324628531932831, Accuracy: 98.57847595214844\n","Epoch 82, Loss: 0.06936066597700119, Accuracy: 98.67637634277344\n","Epoch 83, Loss: 0.06513562798500061, Accuracy: 98.79777526855469\n","Epoch 84, Loss: 0.06168637052178383, Accuracy: 98.89567565917969\n","Epoch 85, Loss: 0.058520473539829254, Accuracy: 98.9739990234375\n","Epoch 86, Loss: 0.055417731404304504, Accuracy: 99.09931182861328\n","Epoch 87, Loss: 0.052765004336833954, Accuracy: 99.16587829589844\n","Epoch 88, Loss: 0.052569951862096786, Accuracy: 99.15021514892578\n","Epoch 89, Loss: 0.051440004259347916, Accuracy: 99.15805053710938\n","Epoch 90, Loss: 0.04963058605790138, Accuracy: 99.21678924560547\n","Epoch 91, Loss: 0.04691893234848976, Accuracy: 99.24812316894531\n","Epoch 92, Loss: 0.04318380355834961, Accuracy: 99.34210968017578\n","Epoch 93, Loss: 0.03991415351629257, Accuracy: 99.44001007080078\n","Epoch 94, Loss: 0.03668441250920296, Accuracy: 99.510498046875\n","Epoch 95, Loss: 0.03366988152265549, Accuracy: 99.6005630493164\n","Epoch 96, Loss: 0.031206604093313217, Accuracy: 99.6475601196289\n","Epoch 97, Loss: 0.029163649305701256, Accuracy: 99.66322326660156\n","Epoch 98, Loss: 0.02766454964876175, Accuracy: 99.6984634399414\n","Epoch 99, Loss: 0.025789346545934677, Accuracy: 99.72979736328125\n","Epoch 100, Loss: 0.024242054671049118, Accuracy: 99.74153900146484\n","Epoch 101, Loss: 0.022675523534417152, Accuracy: 99.77678680419922\n","Epoch 102, Loss: 0.021307174116373062, Accuracy: 99.78853607177734\n","Epoch 103, Loss: 0.01998007670044899, Accuracy: 99.82377624511719\n","Epoch 104, Loss: 0.019009647890925407, Accuracy: 99.83943939208984\n","Epoch 105, Loss: 0.018295779824256897, Accuracy: 99.85118865966797\n","Epoch 106, Loss: 0.017396073788404465, Accuracy: 99.8629379272461\n","Epoch 107, Loss: 0.01672840304672718, Accuracy: 99.87468719482422\n","Epoch 108, Loss: 0.015488558448851109, Accuracy: 99.902099609375\n","Epoch 109, Loss: 0.014613669365644455, Accuracy: 99.90601348876953\n","Epoch 110, Loss: 0.013890533708035946, Accuracy: 99.92559814453125\n","Epoch 111, Loss: 0.013231008313596249, Accuracy: 99.92951202392578\n","Epoch 112, Loss: 0.012959874235093594, Accuracy: 99.93342590332031\n","Epoch 113, Loss: 0.01276931818574667, Accuracy: 99.9412612915039\n","Epoch 114, Loss: 0.012283938005566597, Accuracy: 99.94517517089844\n","Epoch 115, Loss: 0.011873498558998108, Accuracy: 99.92951202392578\n","Epoch 116, Loss: 0.011498273350298405, Accuracy: 99.93734741210938\n","Epoch 117, Loss: 0.01070234552025795, Accuracy: 99.93734741210938\n","Epoch 118, Loss: 0.009820853359997272, Accuracy: 99.95692443847656\n","Epoch 119, Loss: 0.0090930862352252, Accuracy: 99.94908905029297\n","Epoch 120, Loss: 0.008503169752657413, Accuracy: 99.95301055908203\n","Epoch 121, Loss: 0.008035845123231411, Accuracy: 99.95301055908203\n","Epoch 122, Loss: 0.007628701161593199, Accuracy: 99.95692443847656\n","Epoch 123, Loss: 0.0073186978697776794, Accuracy: 99.95692443847656\n","Epoch 124, Loss: 0.007031098939478397, Accuracy: 99.95692443847656\n","Epoch 125, Loss: 0.006748897489160299, Accuracy: 99.95692443847656\n","Epoch 126, Loss: 0.006475427187979221, Accuracy: 99.95692443847656\n","Epoch 127, Loss: 0.006249716039747, Accuracy: 99.95692443847656\n","Epoch 128, Loss: 0.006039741449058056, Accuracy: 99.95692443847656\n","Epoch 129, Loss: 0.005829323548823595, Accuracy: 99.95692443847656\n","Epoch 130, Loss: 0.005604307632893324, Accuracy: 99.95692443847656\n","Epoch 131, Loss: 0.005395978689193726, Accuracy: 99.9608383178711\n","Epoch 132, Loss: 0.005236468743532896, Accuracy: 99.9608383178711\n","Epoch 133, Loss: 0.005084536504000425, Accuracy: 99.9608383178711\n","Epoch 134, Loss: 0.004973202012479305, Accuracy: 99.95692443847656\n","Epoch 135, Loss: 0.005355402827262878, Accuracy: 99.95692443847656\n","Epoch 136, Loss: 0.006595036946237087, Accuracy: 99.93734741210938\n","Epoch 137, Loss: 0.005484770517796278, Accuracy: 99.96867370605469\n","Epoch 138, Loss: 0.004994054790586233, Accuracy: 99.9608383178711\n","Epoch 139, Loss: 0.004659339319914579, Accuracy: 99.9608383178711\n","Epoch 140, Loss: 0.004347649868577719, Accuracy: 99.9608383178711\n","Epoch 141, Loss: 0.004130105022341013, Accuracy: 99.9608383178711\n","Epoch 142, Loss: 0.00397325586527586, Accuracy: 99.9608383178711\n","Epoch 143, Loss: 0.0038453401066362858, Accuracy: 99.9608383178711\n","Epoch 144, Loss: 0.0037341120187193155, Accuracy: 99.9608383178711\n","Epoch 145, Loss: 0.003619581926614046, Accuracy: 99.9608383178711\n","Epoch 146, Loss: 0.003524747211486101, Accuracy: 99.9608383178711\n","Epoch 147, Loss: 0.003431411227211356, Accuracy: 99.9608383178711\n","Epoch 148, Loss: 0.0033531237859278917, Accuracy: 99.96475219726562\n","Epoch 149, Loss: 0.003269422799348831, Accuracy: 99.96475219726562\n","Epoch 150, Loss: 0.003194477641955018, Accuracy: 99.96475219726562\n","Epoch 151, Loss: 0.0031186575070023537, Accuracy: 99.96475219726562\n","Epoch 152, Loss: 0.0030524416361004114, Accuracy: 99.96475219726562\n","Epoch 153, Loss: 0.002988977823406458, Accuracy: 99.96475219726562\n","Epoch 154, Loss: 0.002928016474470496, Accuracy: 99.96475219726562\n","Epoch 155, Loss: 0.0028734563384205103, Accuracy: 99.96475219726562\n","Epoch 156, Loss: 0.002814067294821143, Accuracy: 99.96475219726562\n","Epoch 157, Loss: 0.0027532284148037434, Accuracy: 99.96475219726562\n","Epoch 158, Loss: 0.002693155314773321, Accuracy: 99.96475219726562\n","Epoch 159, Loss: 0.0026566574815660715, Accuracy: 99.96475219726562\n","Epoch 160, Loss: 0.002607169095426798, Accuracy: 99.96475219726562\n","Epoch 161, Loss: 0.002553460653871298, Accuracy: 99.96475219726562\n","Epoch 162, Loss: 0.0025109562557190657, Accuracy: 99.96475219726562\n","Epoch 163, Loss: 0.0024675303138792515, Accuracy: 99.96475219726562\n","Epoch 164, Loss: 0.0024307644926011562, Accuracy: 99.96475219726562\n","Epoch 165, Loss: 0.0023770108819007874, Accuracy: 99.96475219726562\n","Epoch 166, Loss: 0.0023503375705331564, Accuracy: 99.96475219726562\n","Epoch 167, Loss: 0.0023014978505671024, Accuracy: 99.96475219726562\n","Epoch 168, Loss: 0.00226945080794394, Accuracy: 99.96475219726562\n","Epoch 169, Loss: 0.0022280814591795206, Accuracy: 99.96475219726562\n","Epoch 170, Loss: 0.0022000966127961874, Accuracy: 99.96475219726562\n","Epoch 171, Loss: 0.0021714901085942984, Accuracy: 99.96475219726562\n","Epoch 172, Loss: 0.002148356521502137, Accuracy: 99.96475219726562\n","Epoch 173, Loss: 0.002108578337356448, Accuracy: 99.96475219726562\n","Epoch 174, Loss: 0.002083414699882269, Accuracy: 99.96475219726562\n","Epoch 175, Loss: 0.0020553325302898884, Accuracy: 99.96475219726562\n","Epoch 176, Loss: 0.002039809711277485, Accuracy: 99.96475219726562\n","Epoch 177, Loss: 0.002021380700170994, Accuracy: 99.96475219726562\n","Epoch 178, Loss: 0.001984777394682169, Accuracy: 99.96475219726562\n","Epoch 179, Loss: 0.0019456344889476895, Accuracy: 99.96475219726562\n","Epoch 180, Loss: 0.0019102387595921755, Accuracy: 99.96475219726562\n","Epoch 181, Loss: 0.0018809196772053838, Accuracy: 99.96475219726562\n","Epoch 182, Loss: 0.001857658033259213, Accuracy: 99.96475219726562\n","Epoch 183, Loss: 0.001826721359975636, Accuracy: 99.96475219726562\n","Epoch 184, Loss: 0.0018082530004903674, Accuracy: 99.96475219726562\n","Epoch 185, Loss: 0.0017940292600542307, Accuracy: 99.96475219726562\n","Epoch 186, Loss: 0.0017647863132879138, Accuracy: 99.96475219726562\n","Epoch 187, Loss: 0.0017434462206438184, Accuracy: 99.96475219726562\n","Epoch 188, Loss: 0.0017162836156785488, Accuracy: 99.96475219726562\n","Epoch 189, Loss: 0.0017039327649399638, Accuracy: 99.96475219726562\n","Epoch 190, Loss: 0.0016844546189531684, Accuracy: 99.96475219726562\n","Epoch 191, Loss: 0.0016691407654434443, Accuracy: 99.96475219726562\n","Epoch 192, Loss: 0.0016465074149891734, Accuracy: 99.96475219726562\n","Epoch 193, Loss: 0.0016275488305836916, Accuracy: 99.96475219726562\n","Epoch 194, Loss: 0.001609607832506299, Accuracy: 99.96475219726562\n","Epoch 195, Loss: 0.0015913302777335048, Accuracy: 99.96475219726562\n","Epoch 196, Loss: 0.0015739465598016977, Accuracy: 99.96475219726562\n","Epoch 197, Loss: 0.0015616387827321887, Accuracy: 99.96475219726562\n","Epoch 198, Loss: 0.0015512711834162474, Accuracy: 99.96475219726562\n","Epoch 199, Loss: 0.0015471961814910173, Accuracy: 99.96475219726562\n","Epoch 200, Loss: 0.0015223156660795212, Accuracy: 99.96475219726562\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MRGH09Ly5uYt","colab_type":"code","outputId":"941bbe89-817c-4108-e1e2-0841e712c8cf","executionInfo":{"status":"error","timestamp":1579258536871,"user_tz":-540,"elapsed":1289,"user":{"displayName":"eodud dj","photoUrl":"","userId":"05787899662039541278"}},"colab":{"base_uri":"https://localhost:8080/","height":238}},"source":["for test_seq, test_labels in test_ds:\n","  prediction = test_step(model, test_seq)\n","  test_text = tokenizer.sequences_to_texts(test_seq.numpy())\n","  gt_text = tokenizer.sequences_to_texts(test_labels.numpy())\n","  texts = tokenizer.sequences_to_texts(prediction.numpy())\n","  print('_')\n","  print('q: ', test_text)\n","  print('a: ', gt_text)\n","  print('p: ', texts)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-41-2d9c3f6659cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtest_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mtest_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequences_to_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mgt_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequences_to_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequences_to_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'test_ds' is not defined"]}]},{"cell_type":"code","metadata":{"id":"pi1a20V1JN1N","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}